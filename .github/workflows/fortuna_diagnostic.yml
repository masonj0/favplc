name: "ğŸ”¬ Fortuna Targeted v2"
on:
  workflow_dispatch:
    inputs:
      test_date:
        description: 'Date to test results fetching (YYYY-MM-DD)'
        required: false
        default: ''
        type: string

jobs:
  targeted:
    runs-on: ubuntu-22.04
    timeout-minutes: 20
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-python@v5
        with:
          python-version: '3.11'
      - name: Install dependencies
        run: |
          python3 -m pip install --upgrade pip
          python3 -m pip install -r requirements.txt
          python3 -m browserforge update
          playwright install --with-deps chromium

      - name: Restore Master Database
        uses: actions/cache/restore@v4
        with:
          path: fortuna.db
          key: fortuna-db-v3-master-
          restore-keys: fortuna-db-v3-master-

      - name: "ğŸ”¬ Run Targeted Diagnostic"
        env:
          PYTHONPATH: .
          TEST_DATE: "${{ github.event.inputs.test_date }}"
        run: |
          python3 << 'PYEOF'
          import sqlite3, os, sys, asyncio, inspect, json, re
          from datetime import datetime, timedelta
          from pathlib import Path
          from zoneinfo import ZoneInfo

          sys.path.insert(0, '.')
          import fortuna
          import fortuna_analytics

          EASTERN = ZoneInfo("America/New_York")
          S = os.environ.get("GITHUB_STEP_SUMMARY", "/dev/stdout")
          buf = []
          def emit(line=""):
              buf.append(line)
          def flush():
              with open(S, "a") as f:
                  f.write("\n".join(buf) + "\n")
              buf.clear()

          emit("# ğŸ”¬ Targeted Diagnostic v2\n")

          # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
          # Q1: What's audited vs stuck â€” the critical split
          # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
          emit("## Q1: Audited vs Stuck Breakdown\n")

          db_path = "fortuna.db"
          if not os.path.exists(db_path):
              emit("âš ï¸ `fortuna.db` not found. Creating empty one for structure test.")
              # Minimal init to avoid crash
              conn = sqlite3.connect(db_path)
              conn.close()

          conn = sqlite3.connect(db_path)
          conn.row_factory = sqlite3.Row

          # Check if tips table exists
          cursor = conn.execute("SELECT name FROM sqlite_master WHERE type='table' AND name='tips'")
          if not cursor.fetchone():
              emit("âŒ `tips` table does not exist in the database.")
          else:
              emit("### âœ… Audited tips (what works)")
              emit("```")
              for row in conn.execute("""
                  SELECT venue, discipline, verdict, COUNT(*) as n,
                         GROUP_CONCAT(DISTINCT race_id) as sample_ids
                  FROM tips WHERE audit_completed = 1
                  GROUP BY venue, discipline, verdict ORDER BY n DESC
              """):
                  disc = {'H':'Harness','T':'Thorough','G':'Greyhound'}.get(
                      row['discipline'], row['discipline'] or '?')
                  ids = row['sample_ids'][:80] if row['sample_ids'] else ''
                  emit(f"  {row['venue']:25s} [{disc:10s}] {row['verdict']:20s} {row['n']:3d}  ids: {ids}")
              emit("```\n")

              emit("### âŒ Stuck tips (what's broken)")
              emit("```")
              for row in conn.execute("""
                  SELECT venue, discipline, COUNT(*) as n,
                         MIN(start_time) as earliest, MAX(start_time) as latest,
                         GROUP_CONCAT(DISTINCT SUBSTR(race_id, 1, CASE WHEN INSTR(race_id, '_') > 0 THEN INSTR(race_id, '_')-1 ELSE LENGTH(race_id) END)) as prefixes
                  FROM tips WHERE audit_completed = 0
                  GROUP BY venue, discipline ORDER BY n DESC
              """):
                  disc = {'H':'Harness','T':'Thorough','G':'Greyhound'}.get(
                      row['discipline'], row['discipline'] or '?')
                  emit(f"  {row['venue']:25s} [{disc:10s}] {row['n']:3d} tips  {row['earliest'][:16]} â†’ {row['latest'][:16]}  prefixes: {row['prefixes']}")
              emit("```\n")

          # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
          # Q2: Adapter registry â€” what exists and how it's classified
          # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
          emit("## Q2: Adapter Registry & Quality Split\n")

          all_results_classes = fortuna_analytics.get_results_adapter_classes()
          emit(f"**Total results adapter classes:** {len(all_results_classes)}\n")

          # Check SOLID_RESULTS_ADAPTERS
          solid = getattr(fortuna, 'SOLID_RESULTS_ADAPTERS', None)
          emit(f"**`fortuna.SOLID_RESULTS_ADAPTERS`:** `{solid}`\n")

          # Check other adapter lists
          for attr_name in ['USA_RESULTS_ADAPTERS', 'INT_RESULTS_ADAPTERS',
                            'SOLID_ADAPTERS', 'LOUSY_ADAPTERS']:
              val = getattr(fortuna, attr_name, None)
              if val is not None:
                  emit(f"**`fortuna.{attr_name}`:** `{val}`\n")

          emit("### All registered results adapters")
          emit("```")
          for cls in all_results_classes:
              name = cls.SOURCE_NAME
              is_solid = name in (solid or [])
              base_url = getattr(cls, 'BASE_URL', '?')
              engine = 'unknown'
              try:
                  inst = cls()
                  strat = inst._configure_fetch_strategy()
                  engine = str(strat.primary_engine)
              except:
                  pass
              quality_label = "SOLID" if is_solid else "LOUSY"
              emit(f"  {name:40s} [{quality_label:5s}]  {base_url}")
              emit(f"    {'':40s}  engine: {engine}")
          emit("```\n")

          # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
          # Q3: Actually test adapters with get_races() â€” the RIGHT method
          # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
          emit("## Q3: Live Adapter Test (using get_races)\n")

          test_date_str = os.environ.get('TEST_DATE', '')
          if test_date_str:
              yesterday = test_date_str
          else:
              yesterday = (datetime.now(EASTERN) - timedelta(days=1)).strftime('%Y-%m-%d')
          emit(f"Testing date: **{yesterday}**\n")

          async def test_one(cls):
              name = cls.SOURCE_NAME
              try:
                  adapter = cls()
                  start = datetime.now()

                  # Step 1: Test full get_races (Interface for all V3 adapters)
                  start2 = datetime.now()
                  races = await adapter.get_races(yesterday)
                  race_time = (datetime.now() - start2).total_seconds()

                  if not races:
                      # If no races, try to see if it's a discovery-link failure or parse failure
                      # for PageFetchingResultsAdapter subclasses
                      if hasattr(adapter, "_discover_result_links"):
                          links = await adapter._discover_result_links(yesterday)
                          if not links:
                              emit(f"- âš ï¸ `{name}` â†’ **0 links discovered** in {race_time:.1f}s")
                          else:
                              emit(f"- âš ï¸ `{name}` â†’ **0 races parsed** from {len(links)} links in {race_time:.1f}s")
                      else:
                          emit(f"- âš ï¸ `{name}` â†’ **0 races fetched** in {race_time:.1f}s")
                  else:
                      venues = sorted(set(r.venue for r in races))
                      emit(f"- âœ… `{name}` â†’ **{len(races)} races** parsed in {race_time:.1f}s")
                      emit(f"  Venues: {', '.join(venues[:8])}")
                      # Show canonical keys
                      for r in races[:3]:
                          emit(f"  Key: `{r.canonical_key}` runners: {len(r.runners)}")

                  try:
                      await adapter.close()
                  except:
                      pass

              except Exception as e:
                  import traceback
                  emit(f"- âŒ `{name}` â†’ **{type(e).__name__}:** `{str(e)[:120]}`")
                  emit(f"  ```\n  {traceback.format_exc()[:500]}\n  ```")

          async def run_all():
              for cls in all_results_classes:
                  await test_one(cls)
                  emit("")
              # Cleanup
              try:
                  await fortuna.GlobalResourceManager.cleanup()
              except:
                  pass

          asyncio.run(run_all())

          # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
          # Q4: Canonical key comparison â€” do tips match results?
          # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
          emit("## Q4: Key Format Comparison\n")

          # Check if tips table exists again before Q4
          cursor = conn.execute("SELECT name FROM sqlite_master WHERE type='table' AND name='tips'")
          if cursor.fetchone():
              emit("### Tip canonical keys (computed from tip data)")
              emit("```")
              for row in conn.execute("""
                  SELECT race_id, venue, race_number, start_time, discipline
                  FROM tips WHERE audit_completed = 0
                  LIMIT 10
              """):
                  tip = dict(row)
                  key = fortuna_analytics.AuditorEngine._tip_canonical_key(tip)
                  emit(f"  race_id:  {tip['race_id']}")
                  emit(f"  tip_key:  {key}")
                  emit(f"  venue: {tip['venue']} â†’ canonical: {fortuna.get_canonical_venue(tip['venue'])}")
                  emit(f"  start_time: {tip['start_time']}")
                  emit(f"  ---")
              emit("```\n")

          # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
          # Q5: Network test for the ACTUAL adapter domains
          # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
          emit("## Q5: Network Test (actual adapter domains)\n")
          import urllib.request
          emit("```")
          domains = sorted(set(
              getattr(cls, 'BASE_URL', '') for cls in all_results_classes
          ))
          # Specific check for subdomains
          domains.append("https://greyhounds.attheraces.com")
          domains = sorted(list(set(domains)))

          for url in domains:
              if not url:
                  continue
              try:
                  req = urllib.request.Request(url, method="HEAD")
                  req.add_header("User-Agent", "Mozilla/5.0")
                  resp = urllib.request.urlopen(req, timeout=8)
                  emit(f"  âœ… {url:50s} â†’ {resp.status}")
              except Exception as e:
                  emit(f"  âŒ {url:50s} â†’ {type(e).__name__}: {str(e)[:60]}")
          emit("```\n")

          # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
          # Q5.5: Rendered Browser Ping (OVERKILL)
          # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
          emit("## Q5.5: Rendered Browser Ping (Playwright)\n")

          async def browser_ping(domains):
              from playwright.async_api import async_playwright
              async with async_playwright() as p:
                  browser = await p.chromium.launch(headless=True, args=["--disable-gpu"])
                  # Curated subset for stealth rendering test
                  test_urls = [
                      "https://www.racingpost.com/results",
                      "https://www.equibase.com/static/chart/summary/index.html",
                      "https://www.attheraces.com/results"
                  ]
                  # Add other discovered domains
                  for d in domains:
                      if d and d not in test_urls: test_urls.append(d)

                  context = await browser.new_context(
                      user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/125.0.0.0 Safari/537.36",
                      viewport={'width': 1280, 'height': 720}
                  )

                  block_sigs = ["cloudflare", "incapsula", "just a moment", "pardon our interruption", "attention required"]

                  for url in test_urls[:8]: # Cap at 8
                      try:
                          page = await context.new_page()
                          start = datetime.now()
                          resp = await page.goto(url, wait_until="networkidle", timeout=20000)
                          elapsed = (datetime.now() - start).total_seconds()

                          content = await page.content()
                          content_len = len(content)
                          title = await page.title()
                          status = resp.status if resp else "no_resp"

                          blocked = any(sig in content.lower() or sig in title.lower() for sig in block_sigs)

                          if blocked:
                              emit(f"- âŒ `{url}` â†’ **BOT BLOCKED** (Title: {title}) Status: {status}")
                          else:
                              emit(f"- âœ… `{url}` â†’ Title: {title[:40]}... | Status: {status} ({content_len} chars) in {elapsed:.1f}s")
                          await page.close()
                      except Exception as e:
                          emit(f"- âŒ `{url}` â†’ Failed: `{str(e)[:60]}`")
                  await browser.close()

          try:
              asyncio.run(browser_ping(domains))
          except Exception as e:
              emit(f"âš ï¸ Playwright ping failed: {e}")
          emit("")

          # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
          # Q6: Check the most recent workflow runs' harvest JSON
          # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
          emit("## Q6: Last Harvest Summary\n")

          for fname in ['results_harvest.json', 'discovery_harvest.json']:
              p = Path(fname)
              if p.exists():
                  emit(f"### `{fname}`")
                  emit("```json")
                  emit(p.read_text()[:2000])
                  emit("```\n")
              else:
                  emit(f"âš ï¸ `{fname}` not found\n")

          # Check harvest_logs table
          cursor = conn.execute("SELECT name FROM sqlite_master WHERE type='table' AND name='harvest_logs'")
          if cursor.fetchone():
              emit("### Last 15 harvest log entries")
              emit("```")
              for row in conn.execute("""
                  SELECT timestamp, region, adapter_name, race_count, max_odds
                  FROM harvest_logs ORDER BY id DESC LIMIT 15
              """):
                  emit(f"  {row['timestamp'][:19]}  {row['region'] or 'GLOBAL':6s}  {row['adapter_name']:40s}  races={row['race_count']:3d}  maxOdds={row['max_odds'] or 0:.1f}")
              emit("```\n")

          # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
          # Q7: DB Shadow Comparison (Fresh vs Cached)
          # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
          emit("## Q7: DB Shadow Comparison\n")

          # Try to produce a fresh snapshot if consolidation script exists
          if os.path.exists("scripts/consolidate_parallel_runs.py"):
              # Only run consolidation if we see other regional DBs
              if any(f.endswith('.db') and f != 'fortuna.db' for f in os.listdir('.')):
                  import subprocess
                  subprocess.run(["python3", "scripts/consolidate_parallel_runs.py", "--output-db", "fortuna_rebuilt_snapshot.db"])

          # Look for other DB files
          db_files = [f for f in os.listdir('.') if f.endswith('.db') and f != 'fortuna.db']
          if not db_files:
              emit("â„¹ï¸ No shadow database files found to compare against.\n")
          else:
              import subprocess
              for dbf in db_files:
                  try:
                      emit(f"### Comparing `fortuna.db` with `{dbf}`")
                      res = subprocess.run(["python3", "scripts/db_shadow_compare.py", "fortuna.db", dbf], capture_output=True, text=True)
                      emit("```")
                      emit(res.stdout)
                      if res.returncode != 0:
                          emit(f"âŒ DB Integrity Mismatch found in {dbf}")
                          emit(res.stderr)
                      emit("```")
                  except Exception as e:
                      emit(f"- âŒ Error running shadow comparison for `{dbf}`: {e}")

          conn.close()

          emit("---")
          emit("*Paste this to Claude for the final fix.*")
          flush()
          PYEOF
