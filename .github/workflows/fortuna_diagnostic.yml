name: "üî¨ Fortuna Targeted v2"
on:
  workflow_dispatch:
    inputs:
      test_date:
        description: 'Date to test results fetching (YYYY-MM-DD)'
        required: false
        default: ''
        type: string

jobs:
  targeted:
    runs-on: ubuntu-22.04
    timeout-minutes: 20
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-python@v5
        with:
          python-version: '3.11'
      - name: Install dependencies
        run: |
          python3 -m pip install --upgrade pip
          python3 -m pip install -r requirements.txt
          python3 -m browserforge update
          playwright install --with-deps chromium

      - name: Restore Master Database
        uses: actions/cache/restore@v4
        with:
          path: fortuna.db
          key: fortuna-db-v3-master-
          restore-keys: fortuna-db-v3-master-

      - name: "üî¨ Run Targeted Diagnostic"
        env:
          PYTHONPATH: .
          TEST_DATE: "${{ github.event.inputs.test_date }}"
        run: |
          python3 << 'PYEOF'
          import sqlite3, os, sys, asyncio, inspect, json, re
          from datetime import datetime, timedelta
          from pathlib import Path
          from zoneinfo import ZoneInfo

          sys.path.insert(0, '.')
          import fortuna
          import fortuna_analytics

          EASTERN = ZoneInfo("America/New_York")
          S = os.environ.get("GITHUB_STEP_SUMMARY", "/dev/stdout")
          buf = []
          def emit(line=""):
              buf.append(line)
          def flush():
              with open(S, "a") as f:
                  f.write("\n".join(buf) + "\n")
              buf.clear()

          emit("# üî¨ Targeted Diagnostic v2\n")

          # ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
          # Q1: What's audited vs stuck ‚Äî the critical split
          # ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
          emit("## Q1: Audited vs Stuck Breakdown\n")

          db_path = "fortuna.db"
          if not os.path.exists(db_path):
              emit("‚ö†Ô∏è `fortuna.db` not found. Creating empty one for structure test.")
              # Minimal init to avoid crash
              conn = sqlite3.connect(db_path)
              conn.close()

          conn = sqlite3.connect(db_path)
          conn.row_factory = sqlite3.Row

          # Check if tips table exists
          cursor = conn.execute("SELECT name FROM sqlite_master WHERE type='table' AND name='tips'")
          if not cursor.fetchone():
              emit("‚ùå `tips` table does not exist in the database.")
          else:
              emit("### ‚úÖ Audited tips (what works)")
              emit("```")
              for row in conn.execute("""
                  SELECT venue, discipline, verdict, COUNT(*) as n,
                         GROUP_CONCAT(DISTINCT race_id) as sample_ids
                  FROM tips WHERE audit_completed = 1
                  GROUP BY venue, discipline, verdict ORDER BY n DESC
              """):
                  disc = {'H':'Harness','T':'Thorough','G':'Greyhound'}.get(
                      row['discipline'], row['discipline'] or '?')
                  ids = row['sample_ids'][:80] if row['sample_ids'] else ''
                  emit(f"  {row['venue']:25s} [{disc:10s}] {row['verdict']:20s} {row['n']:3d}  ids: {ids}")
              emit("```\n")

              emit("### ‚ùå Stuck tips (what's broken)")
              emit("```")
              for row in conn.execute("""
                  SELECT venue, discipline, COUNT(*) as n,
                         MIN(start_time) as earliest, MAX(start_time) as latest,
                         GROUP_CONCAT(DISTINCT SUBSTR(race_id, 1, CASE WHEN INSTR(race_id, '_') > 0 THEN INSTR(race_id, '_')-1 ELSE LENGTH(race_id) END)) as prefixes
                  FROM tips WHERE audit_completed = 0
                  GROUP BY venue, discipline ORDER BY n DESC
              """):
                  disc = {'H':'Harness','T':'Thorough','G':'Greyhound'}.get(
                      row['discipline'], row['discipline'] or '?')
                  emit(f"  {row['venue']:25s} [{disc:10s}] {row['n']:3d} tips  {row['earliest'][:16]} ‚Üí {row['latest'][:16]}  prefixes: {row['prefixes']}")
              emit("```\n")

          # ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
          # Q2: Adapter registry ‚Äî what exists and how it's classified
          # ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
          emit("## Q2: Adapter Registry & Quality Split\n")

          all_results_classes = fortuna_analytics.get_results_adapter_classes()
          emit(f"**Total results adapter classes:** {len(all_results_classes)}\n")

          # Check SOLID_RESULTS_ADAPTERS
          solid = getattr(fortuna, 'SOLID_RESULTS_ADAPTERS', None)
          emit(f"**`fortuna.SOLID_RESULTS_ADAPTERS`:** `{solid}`\n")

          # Check other adapter lists
          for attr_name in ['USA_RESULTS_ADAPTERS', 'INT_RESULTS_ADAPTERS',
                            'SOLID_ADAPTERS', 'LOUSY_ADAPTERS']:
              val = getattr(fortuna, attr_name, None)
              if val is not None:
                  emit(f"**`fortuna.{attr_name}`:** `{val}`\n")

          emit("### All registered results adapters")
          emit("```")
          for cls in all_results_classes:
              name = cls.SOURCE_NAME
              is_solid = name in (solid or [])
              base_url = getattr(cls, 'BASE_URL', '?')
              engine = 'unknown'
              try:
                  inst = cls()
                  strat = inst._configure_fetch_strategy()
                  engine = str(strat.primary_engine)
              except:
                  pass
              quality_label = "SOLID" if is_solid else "LOUSY"
              emit(f"  {name:40s} [{quality_label:5s}]  {base_url}")
              emit(f"    {'':40s}  engine: {engine}")
          emit("```\n")

          # ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
          # Q3: Actually test adapters with get_races() ‚Äî the RIGHT method
          # ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
          emit("## Q3: Live Adapter Test (using get_races)\n")

          test_date_str = os.environ.get('TEST_DATE', '')
          if test_date_str:
              yesterday = test_date_str
          else:
              yesterday = (datetime.now(EASTERN) - timedelta(days=1)).strftime('%Y-%m-%d')
          emit(f"Testing date: **{yesterday}**\n")

          async def test_one(cls):
              name = cls.SOURCE_NAME
              try:
                  adapter = cls()
                  start = datetime.now()

                  # Step 1: Test full get_races (Interface for all V3 adapters)
                  start2 = datetime.now()
                  races = await adapter.get_races(yesterday)
                  race_time = (datetime.now() - start2).total_seconds()

                  if not races:
                      # If no races, try to see if it's a discovery-link failure or parse failure
                      # for PageFetchingResultsAdapter subclasses
                      if hasattr(adapter, "_discover_result_links"):
                          links = await adapter._discover_result_links(yesterday)
                          if not links:
                              emit(f"- ‚ö†Ô∏è `{name}` ‚Üí **0 links discovered** in {race_time:.1f}s")
                          else:
                              emit(f"- ‚ö†Ô∏è `{name}` ‚Üí **0 races parsed** from {len(links)} links in {race_time:.1f}s")
                      else:
                          emit(f"- ‚ö†Ô∏è `{name}` ‚Üí **0 races fetched** in {race_time:.1f}s")
                  else:
                      venues = sorted(set(r.venue for r in races))
                      emit(f"- ‚úÖ `{name}` ‚Üí **{len(races)} races** parsed in {race_time:.1f}s")
                      emit(f"  Venues: {', '.join(venues[:8])}")
                      # Show canonical keys
                      for r in races[:3]:
                          emit(f"  Key: `{r.canonical_key}` runners: {len(r.runners)}")

                  try:
                      await adapter.close()
                  except:
                      pass

              except Exception as e:
                  import traceback
                  emit(f"- ‚ùå `{name}` ‚Üí **{type(e).__name__}:** `{str(e)[:120]}`")
                  emit(f"  ```\n  {traceback.format_exc()[:500]}\n  ```")

          async def run_all():
              for cls in all_results_classes:
                  await test_one(cls)
                  emit("")
              # Cleanup
              try:
                  await fortuna.GlobalResourceManager.cleanup()
              except:
                  pass

          asyncio.run(run_all())

          # ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
          # Q4: Canonical key comparison ‚Äî do tips match results?
          # ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
          emit("## Q4: Key Format Comparison\n")

          # Check if tips table exists again before Q4
          cursor = conn.execute("SELECT name FROM sqlite_master WHERE type='table' AND name='tips'")
          if cursor.fetchone():
              emit("### Tip canonical keys (computed from tip data)")
              emit("```")
              for row in conn.execute("""
                  SELECT race_id, venue, race_number, start_time, discipline
                  FROM tips WHERE audit_completed = 0
                  LIMIT 10
              """):
                  tip = dict(row)
                  key = fortuna_analytics.AuditorEngine._tip_canonical_key(tip)
                  emit(f"  race_id:  {tip['race_id']}")
                  emit(f"  tip_key:  {key}")
                  emit(f"  venue: {tip['venue']} ‚Üí canonical: {fortuna.get_canonical_venue(tip['venue'])}")
                  emit(f"  start_time: {tip['start_time']}")
                  emit(f"  ---")
              emit("```\n")

          # ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
          # Q5: Network test for the ACTUAL adapter domains
          # ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
          emit("## Q5: Network Test (actual adapter domains)\n")
          import urllib.request
          emit("```")
          domains = sorted(set(
              getattr(cls, 'BASE_URL', '') for cls in all_results_classes
          ))
          # Specific check for subdomains
          domains.append("https://greyhounds.attheraces.com")
          domains = sorted(list(set(domains)))

          for url in domains:
              if not url:
                  continue
              try:
                  req = urllib.request.Request(url, method="HEAD")
                  req.add_header("User-Agent", "Mozilla/5.0")
                  resp = urllib.request.urlopen(req, timeout=8)
                  emit(f"  ‚úÖ {url:50s} ‚Üí {resp.status}")
              except Exception as e:
                  emit(f"  ‚ùå {url:50s} ‚Üí {type(e).__name__}: {str(e)[:60]}")
          emit("```\n")

          # ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
          # Q5.5: Rendered Browser Ping (OVERKILL)
          # ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
          emit("## Q5.5: Rendered Browser Ping (Playwright)\n")

          async def browser_ping(domains):
              from playwright.async_api import async_playwright
              async with async_playwright() as p:
                  browser = await p.chromium.launch(headless=True)
                  context = await browser.new_context(user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36")

                  for url in domains[:5]: # Limit to 5 for speed
                      try:
                          page = await context.new_page()
                          start = datetime.now()
                          resp = await page.goto(url, wait_until="domcontentloaded", timeout=15000)
                          elapsed = (datetime.now() - start).total_seconds()
                          status = resp.status if resp else "no_resp"
                          content_len = len(await page.content())
                          emit(f"- ‚úÖ `{url}` ‚Üí Status: {status} ({content_len} chars) in {elapsed:.1f}s")
                          await page.close()
                      except Exception as e:
                          emit(f"- ‚ùå `{url}` ‚Üí Failed: `{str(e)[:60]}`")
                  await browser.close()

          try:
              asyncio.run(browser_ping(domains))
          except Exception as e:
              emit(f"‚ö†Ô∏è Playwright ping failed: {e}")
          emit("")

          # ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
          # Q6: Check the most recent workflow runs' harvest JSON
          # ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
          emit("## Q6: Last Harvest Summary\n")

          for fname in ['results_harvest.json', 'discovery_harvest.json']:
              p = Path(fname)
              if p.exists():
                  emit(f"### `{fname}`")
                  emit("```json")
                  emit(p.read_text()[:2000])
                  emit("```\n")
              else:
                  emit(f"‚ö†Ô∏è `{fname}` not found\n")

          # Check harvest_logs table
          cursor = conn.execute("SELECT name FROM sqlite_master WHERE type='table' AND name='harvest_logs'")
          if cursor.fetchone():
              emit("### Last 15 harvest log entries")
              emit("```")
              for row in conn.execute("""
                  SELECT timestamp, region, adapter_name, race_count, max_odds
                  FROM harvest_logs ORDER BY id DESC LIMIT 15
              """):
                  emit(f"  {row['timestamp'][:19]}  {row['region'] or 'GLOBAL':6s}  {row['adapter_name']:40s}  races={row['race_count']:3d}  maxOdds={row['max_odds'] or 0:.1f}")
              emit("```\n")

          # ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
          # Q7: DB Shadow Comparison (Fresh vs Cached)
          # ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
          emit("## Q7: DB Shadow Comparison\n")

          # Look for other DB files
          db_files = [f for f in os.listdir('.') if f.endswith('.db') and f != 'fortuna.db']
          if not db_files:
              emit("‚ÑπÔ∏è No shadow database files found to compare against.\n")
          else:
              for dbf in db_files:
                  try:
                      s_conn = sqlite3.connect(dbf)
                      s_count = s_conn.execute("SELECT COUNT(*) FROM tips").fetchone()[0]
                      m_count = conn.execute("SELECT COUNT(*) FROM tips").fetchone()[0]
                      emit(f"- `{dbf}`: {s_count} tips vs `fortuna.db`: {m_count} tips")
                      if s_count > m_count:
                          emit(f"  ‚ö†Ô∏è Shadow DB has MORE tips than master. Potential cache mismatch.")
                      s_conn.close()
                  except Exception as e:
                      emit(f"- ‚ùå Error comparing `{dbf}`: {e}")

          conn.close()

          emit("---")
          emit("*Paste this to Claude for the final fix.*")
          flush()
          PYEOF
