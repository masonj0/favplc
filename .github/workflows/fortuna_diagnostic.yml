name: "ğŸ”¬ Fortuna Targeted v2"
on:
  workflow_dispatch:
    inputs:
      test_date:
        description: 'Date to test results fetching (YYYY-MM-DD)'
        required: false
        default: ''
        type: string

jobs:
  targeted:
    runs-on: ubuntu-22.04
    timeout-minutes: 30        # â† raised from 20; gives real headroom without being unlimited
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-python@v5
        with:
          python-version: '3.11'
      - name: Install dependencies
        run: |
          python3 -m pip install --upgrade pip
          python3 -m pip install -r requirements.txt
          python3 -m browserforge update
          playwright install --with-deps chromium

      - name: Restore Master Database
        uses: actions/cache/restore@v4
        with:
          path: fortuna.db
          key: fortuna-db-v3-master-
          restore-keys: fortuna-db-v3-master-

      - name: "ğŸ”¬ Run Targeted Diagnostic"
        env:
          PYTHONPATH: .
          TEST_DATE: "${{ github.event.inputs.test_date }}"
        run: |
          python3 << 'PYEOF'
          import sqlite3, os, sys, asyncio, inspect, json, re
          from datetime import datetime, timedelta
          from pathlib import Path
          from zoneinfo import ZoneInfo

          sys.path.insert(0, '.')
          import fortuna
          import fortuna_analytics

          EASTERN = ZoneInfo("America/New_York")
          S = os.environ.get("GITHUB_STEP_SUMMARY", "/dev/stdout")
          buf = []

          # â”€â”€ Helpers â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
          def emit(line=""):
              buf.append(line)

          def flush():
              """Write buffered lines to the step summary immediately."""
              with open(S, "a") as f:
                  f.write("\n".join(buf) + "\n")
              buf.clear()

          # Global deadline: bail out gracefully 90 s before the job ceiling so the
          # summary always gets at least a partial write.
          _DEADLINE = datetime.now() + timedelta(minutes=27)   # 3-min buffer inside the 30-min job

          def deadline_exceeded():
              return datetime.now() >= _DEADLINE

          # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
          # Q1: What's audited vs stuck â€” the critical split
          # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
          emit("# ğŸ”¬ Targeted Diagnostic v2\n")
          emit("## Q1: Audited vs Stuck Breakdown\n")

          db_path = "fortuna.db"
          if not os.path.exists(db_path):
              emit("âš ï¸ `fortuna.db` not found. Creating empty one for structure test.")
              conn = sqlite3.connect(db_path)
              conn.close()

          conn = sqlite3.connect(db_path)
          conn.row_factory = sqlite3.Row

          cursor = conn.execute("SELECT name FROM sqlite_master WHERE type='table' AND name='tips'")
          if not cursor.fetchone():
              emit("âŒ `tips` table does not exist in the database.")
          else:
              emit("### âœ… Audited tips (what works)")
              emit("```")
              for row in conn.execute("""
                  SELECT venue, discipline, verdict, COUNT(*) as n,
                         GROUP_CONCAT(DISTINCT race_id) as sample_ids
                  FROM tips WHERE audit_completed = 1
                  GROUP BY venue, discipline, verdict ORDER BY n DESC
              """):
                  disc = {'H':'Harness','T':'Thorough','G':'Greyhound'}.get(
                      row['discipline'], row['discipline'] or '?')
                  ids = row['sample_ids'][:80] if row['sample_ids'] else ''
                  emit(f"  {row['venue']:25s} [{disc:10s}] {row['verdict']:20s} {row['n']:3d}  ids: {ids}")
              emit("```\n")

              emit("### âŒ Stuck tips (what's broken)")
              emit("```")
              for row in conn.execute("""
                  SELECT venue, discipline, COUNT(*) as n,
                         MIN(start_time) as earliest, MAX(start_time) as latest,
                         GROUP_CONCAT(DISTINCT SUBSTR(race_id, 1, CASE WHEN INSTR(race_id, '_') > 0 THEN INSTR(race_id, '_')-1 ELSE LENGTH(race_id) END)) as prefixes
                  FROM tips WHERE audit_completed = 0
                  GROUP BY venue, discipline ORDER BY n DESC
              """):
                  disc = {'H':'Harness','T':'Thorough','G':'Greyhound'}.get(
                      row['discipline'], row['discipline'] or '?')
                  emit(f"  {row['venue']:25s} [{disc:10s}] {row['n']:3d} tips  {row['earliest'][:16]} â†’ {row['latest'][:16]}  prefixes: {row['prefixes']}")
              emit("```\n")

          flush()  # â† Q1 safely on disk before we move on

          # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
          # Q2: Adapter registry â€” what exists and how it's classified
          # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
          emit("## Q2: Adapter Registry & Quality Split\n")

          all_results_classes = fortuna_analytics.get_results_adapter_classes()
          emit(f"**Total results adapter classes:** {len(all_results_classes)}\n")

          solid = getattr(fortuna, 'SOLID_RESULTS_ADAPTERS', None)
          emit(f"**`fortuna.SOLID_RESULTS_ADAPTERS`:** `{solid}`\n")

          for attr_name in ['USA_RESULTS_ADAPTERS', 'INT_RESULTS_ADAPTERS',
                            'SOLID_ADAPTERS', 'LOUSY_ADAPTERS']:
              val = getattr(fortuna, attr_name, None)
              if val is not None:
                  emit(f"**`fortuna.{attr_name}`:** `{val}`\n")

          emit("### All registered results adapters")
          emit("```")
          for cls in all_results_classes:
              name = cls.SOURCE_NAME
              is_solid = name in (solid or [])
              base_url = getattr(cls, 'BASE_URL', '?')
              engine = 'unknown'
              try:
                  inst = cls()
                  strat = inst._configure_fetch_strategy()
                  engine = str(strat.primary_engine)
              except:
                  pass
              quality_label = "SOLID" if is_solid else "LOUSY"
              emit(f"  {name:40s} [{quality_label:5s}]  {base_url}")
              emit(f"    {'':40s}  engine: {engine}")
          emit("```\n")

          flush()  # â† Q2 safely on disk

          # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
          # Q3: Live adapter test â€” concurrent with per-adapter timeout
          # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
          emit("## Q3: Live Adapter Test (using get_races)\n")

          ADAPTER_TIMEOUT_S = 45   # max wall-clock seconds per adapter
          MAX_CONCURRENT    = 4    # run this many adapters in parallel

          test_date_str = os.environ.get('TEST_DATE', '')
          yesterday = test_date_str or (datetime.now(EASTERN) - timedelta(days=1)).strftime('%Y-%m-%d')
          emit(f"Testing date: **{yesterday}**\n")
          flush()  # â† publish the date line immediately so it's visible even on timeout

          # Per-adapter result lines are collected into a dict keyed by SOURCE_NAME
          # so we can emit them in stable order afterwards.
          adapter_lines: dict[str, list[str]] = {}

          async def test_one(cls) -> list[str]:
              name = cls.SOURCE_NAME
              lines = []
              try:
                  adapter = cls()
                  start2 = datetime.now()
                  races = await adapter.get_races(yesterday)
                  race_time = (datetime.now() - start2).total_seconds()

                  if not races:
                      if hasattr(adapter, "_discover_result_links"):
                          links = await adapter._discover_result_links(yesterday)
                          if not links:
                              lines.append(f"- âš ï¸ `{name}` â†’ **0 links discovered** in {race_time:.1f}s")
                          else:
                              lines.append(f"- âš ï¸ `{name}` â†’ **0 races parsed** from {len(links)} links in {race_time:.1f}s")
                      else:
                          lines.append(f"- âš ï¸ `{name}` â†’ **0 races fetched** in {race_time:.1f}s")
                  else:
                      venues = sorted(set(r.venue for r in races))
                      lines.append(f"- âœ… `{name}` â†’ **{len(races)} races** parsed in {race_time:.1f}s")
                      lines.append(f"  Venues: {', '.join(venues[:8])}")
                      for r in races[:3]:
                          lines.append(f"  Key: `{r.canonical_key}` runners: {len(r.runners)}")

                  try:
                      await adapter.close()
                  except:
                      pass

              except asyncio.TimeoutError:
                  lines.append(f"- â±ï¸ `{name}` â†’ **TIMED OUT** after {ADAPTER_TIMEOUT_S}s â€” skipped")
              except Exception as e:
                  import traceback
                  lines.append(f"- âŒ `{name}` â†’ **{type(e).__name__}:** `{str(e)[:120]}`")
                  lines.append(f"  ```\n  {traceback.format_exc()[:500]}\n  ```")

              return lines

          async def run_all():
              sem = asyncio.Semaphore(MAX_CONCURRENT)

              async def bounded(cls):
                  async with sem:
                      return cls.SOURCE_NAME, await asyncio.wait_for(
                          test_one(cls), timeout=ADAPTER_TIMEOUT_S
                      )

              tasks = [asyncio.create_task(bounded(cls)) for cls in all_results_classes]
              # Stream results as they finish so a timeout on the *job* still gives partial output
              for coro in asyncio.as_completed(tasks):
                  try:
                      name, lines = await coro
                  except Exception as e:
                      lines = [f"- âŒ (unknown adapter) â†’ unexpected error: {e}"]
                      name = "unknown"
                  for line in lines:
                      emit(line)
                  emit("")
                  flush()  # â† publish each adapter result as it lands

              try:
                  await fortuna.GlobalResourceManager.cleanup()
              except:
                  pass

          if deadline_exceeded():
              emit("âš ï¸ **Q3 skipped** â€” deadline exceeded before this section.\n")
              flush()
          else:
              asyncio.run(run_all())

          # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
          # Q4: Canonical key comparison â€” do tips match results?
          # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
          emit("## Q4: Key Format Comparison\n")

          cursor = conn.execute("SELECT name FROM sqlite_master WHERE type='table' AND name='tips'")
          if cursor.fetchone():
              emit("### Tip canonical keys (computed from tip data)")
              emit("```")
              for row in conn.execute("""
                  SELECT race_id, venue, race_number, start_time, discipline
                  FROM tips WHERE audit_completed = 0
                  LIMIT 10
              """):
                  tip = dict(row)
                  key = fortuna_analytics.AuditorEngine._tip_canonical_key(tip)
                  emit(f"  race_id:  {tip['race_id']}")
                  emit(f"  tip_key:  {key}")
                  emit(f"  venue: {tip['venue']} â†’ canonical: {fortuna.get_canonical_venue(tip['venue'])}")
                  emit(f"  start_time: {tip['start_time']}")
                  emit(f"  ---")
              emit("```\n")

          flush()  # â† Q4 safely on disk

          # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
          # Q5: Network test for the ACTUAL adapter domains
          # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
          emit("## Q5: Network Test (actual adapter domains)\n")
          import urllib.request
          emit("```")
          domains = sorted(set(
              getattr(cls, 'BASE_URL', '') for cls in all_results_classes
          ))
          domains.append("https://greyhounds.attheraces.com")
          domains = sorted(list(set(domains)))

          for url in domains:
              if not url:
                  continue
              try:
                  req = urllib.request.Request(url, method="HEAD")
                  req.add_header("User-Agent", "Mozilla/5.0")
                  resp = urllib.request.urlopen(req, timeout=8)
                  emit(f"  âœ… {url:50s} â†’ {resp.status}")
              except Exception as e:
                  emit(f"  âŒ {url:50s} â†’ {type(e).__name__}: {str(e)[:60]}")
          emit("```\n")

          flush()  # â† Q5 safely on disk

          # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
          # Q5.5: Rendered Browser Ping (Playwright)
          # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
          emit("## Q5.5: Rendered Browser Ping (Playwright)\n")

          async def browser_ping(domains):
              from playwright.async_api import async_playwright
              async with async_playwright() as p:
                  browser = await p.chromium.launch(headless=True, args=["--disable-gpu"])
                  test_urls = [
                      "https://www.racingpost.com/results",
                      "https://www.equibase.com/static/chart/summary/index.html",
                      "https://www.attheraces.com/results"
                  ]
                  for d in domains:
                      if d and d not in test_urls:
                          test_urls.append(d)

                  context = await browser.new_context(
                      user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/125.0.0.0 Safari/537.36",
                      viewport={'width': 1280, 'height': 720}
                  )

                  block_sigs = ["cloudflare", "incapsula", "just a moment",
                                "pardon our interruption", "attention required"]

                  for url in test_urls[:6]:   # â† capped at 6 (was 8) to save ~40 s
                      try:
                          page = await context.new_page()
                          start = datetime.now()
                          resp = await page.goto(
                              url,
                              wait_until="domcontentloaded",  # â† was networkidle; saves ~5-10 s per URL
                              timeout=12000                   # â† was 20000; fail faster on blocked pages
                          )
                          elapsed = (datetime.now() - start).total_seconds()

                          content = await page.content()
                          content_len = len(content)
                          title = await page.title()
                          status = resp.status if resp else "no_resp"

                          blocked = any(sig in content.lower() or sig in title.lower()
                                        for sig in block_sigs)

                          if blocked:
                              emit(f"- âŒ `{url}` â†’ **BOT BLOCKED** (Title: {title}) Status: {status}")
                          else:
                              emit(f"- âœ… `{url}` â†’ Title: {title[:40]}... | Status: {status} ({content_len} chars) in {elapsed:.1f}s")
                          await page.close()
                          flush()  # â† publish each browser result as it lands
                      except Exception as e:
                          emit(f"- âŒ `{url}` â†’ Failed: `{str(e)[:60]}`")
                          flush()
                  await browser.close()

          if deadline_exceeded():
              emit("âš ï¸ **Q5.5 skipped** â€” deadline exceeded before this section.\n")
              flush()
          else:
              try:
                  asyncio.run(browser_ping(domains))
              except Exception as e:
                  emit(f"âš ï¸ Playwright ping failed: {e}")
                  flush()
          emit("")

          # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
          # Q6: Check the most recent workflow runs' harvest JSON
          # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
          emit("## Q6: Last Harvest Summary\n")

          for fname in ['results_harvest.json', 'discovery_harvest.json']:
              p = Path(fname)
              if p.exists():
                  emit(f"### `{fname}`")
                  emit("```json")
                  emit(p.read_text()[:2000])
                  emit("```\n")
              else:
                  emit(f"âš ï¸ `{fname}` not found\n")

          cursor = conn.execute("SELECT name FROM sqlite_master WHERE type='table' AND name='harvest_logs'")
          if cursor.fetchone():
              emit("### Last 15 harvest log entries")
              emit("```")
              for row in conn.execute("""
                  SELECT timestamp, region, adapter_name, race_count, max_odds
                  FROM harvest_logs ORDER BY id DESC LIMIT 15
              """):
                  emit(f"  {row['timestamp'][:19]}  {row['region'] or 'GLOBAL':6s}  {row['adapter_name']:40s}  races={row['race_count']:3d}  maxOdds={row['max_odds'] or 0:.1f}")
              emit("```\n")

          flush()  # â† Q6 safely on disk

          # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
          # Q7: DB Shadow Comparison (Fresh vs Cached)
          # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
          emit("## Q7: DB Shadow Comparison\n")

          if os.path.exists("scripts/consolidate_parallel_runs.py"):
              if any(f.endswith('.db') and f != 'fortuna.db' for f in os.listdir('.')):
                  import subprocess
                  subprocess.run(["python3", "scripts/consolidate_parallel_runs.py",
                                  "--output-db", "fortuna_rebuilt_snapshot.db"])

          db_files = [f for f in os.listdir('.') if f.endswith('.db') and f != 'fortuna.db']
          if not db_files:
              emit("â„¹ï¸ No shadow database files found to compare against.\n")
          else:
              import subprocess
              for dbf in db_files:
                  try:
                      emit(f"### Comparing `fortuna.db` with `{dbf}`")
                      res = subprocess.run(
                          ["python3", "scripts/db_shadow_compare.py", "fortuna.db", dbf],
                          capture_output=True, text=True
                      )
                      emit("```")
                      emit(res.stdout)
                      if res.returncode != 0:
                          emit(f"âŒ DB Integrity Mismatch found in {dbf}")
                          emit(res.stderr)
                      emit("```")
                  except Exception as e:
                      emit(f"- âŒ Error running shadow comparison for `{dbf}`: {e}")

          conn.close()

          emit("---")
          emit(\"*Paste this to Claude for the final fix.*\")
          flush()   # â† final flush â€” but all sections already flushed above
          PYEOF
