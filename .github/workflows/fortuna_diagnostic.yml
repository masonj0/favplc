name: "ğŸ‡ Fortuna Diagnostic v4 â€” USA Thoroughbred Pipeline"
on:
  workflow_dispatch:
    inputs:
      test_date:
        description: 'General results date (YYYY-MM-DD, default: yesterday ET)'
        required: false
        default: ''
        type: string
      usa_date:
        description: 'USA adapter date (YYYY-MM-DD, default: last Saturday â€” peak US card)'
        required: false
        default: ''
        type: string

jobs:
  usa-tb-diagnostic:
    runs-on: ubuntu-22.04
    timeout-minutes: 35
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-python@v5
        with:
          python-version: '3.11'
      - name: Install dependencies
        run: |
          python3 -m pip install --upgrade pip
          python3 -m pip install -r requirements.txt
          python3 -m browserforge update
          playwright install --with-deps chromium

      - name: Restore Master Database
        uses: actions/cache/restore@v4
        with:
          path: fortuna.db
          key: fortuna-db-v3-master-
          restore-keys: fortuna-db-v3-master-

      - name: "ğŸ”¬ USA Thoroughbred Pipeline Diagnostic"
        env:
          PYTHONPATH: .
          TEST_DATE: "${{ github.event.inputs.test_date }}"
          USA_DATE: "${{ github.event.inputs.usa_date }}"
        run: |
          python3 << 'PYEOF'
          import sqlite3, os, sys, asyncio, json, re, traceback, urllib.request
          from datetime import datetime, timedelta
          from pathlib import Path
          from zoneinfo import ZoneInfo

          sys.path.insert(0, '.')
          import fortuna
          import fortuna_analytics

          # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
          # SETUP
          # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
          EASTERN   = ZoneInfo("America/New_York")
          S         = os.environ.get("GITHUB_STEP_SUMMARY", "/dev/stdout")
          SNAP_DIR  = Path("_diag_snapshots")
          SNAP_DIR.mkdir(exist_ok=True)
          buf       = []
          _evidence = {}
          _DEADLINE = datetime.now() + timedelta(minutes=32)
          today     = datetime.now(EASTERN)

          BLOCK_SIGS = (
              "pardon our interruption", "checking your browser",
              "just a moment", "cloudflare", "access denied",
              "captcha", "incapsula", "attention required",
          )

          # â”€â”€ Date logic (Saturday default for USA) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
          results_date = os.environ.get("TEST_DATE", "").strip() or (
              today - timedelta(days=1)).strftime("%Y-%m-%d")

          usa_date_env = os.environ.get("USA_DATE", "").strip()
          if usa_date_env:
              usa_test_date = usa_date_env
          else:
              days_since_sat = (today.weekday() - 5) % 7
              last_sat = today - timedelta(
                  days=days_since_sat if days_since_sat > 0 else 7)
              usa_test_date = last_sat.strftime("%Y-%m-%d")

          EQB_BASE   = "https://www.equibase.com"
          DATE_SHORT = datetime.strptime(
              usa_test_date, "%Y-%m-%d").strftime("%m%d%y")

          # â”€â”€ Shared helpers â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
          def emit(line=""):
              buf.append(line)

          def flush():
              with open(S, "a") as f:
                  f.write("\n".join(buf) + "\n")
              buf.clear()

          def deadline_exceeded():
              return datetime.now() >= _DEADLINE

          def remaining_minutes():
              return max(0, (_DEADLINE - datetime.now()).total_seconds() / 60)

          def save_snapshot(name, content, url=""):
              safe = re.sub(r'[^\w\-.]', '_', name)[:80]
              (SNAP_DIR / f"{safe}.html").write_text(content[:120_000])
              (SNAP_DIR / f"{safe}.meta.txt").write_text(
                  f"URL: {url}\nLength: {len(content)}\n"
                  f"Saved: {datetime.now(EASTERN).isoformat()}\n")

          def is_blocked(html):
              if len(html) > 15_000:
                  return False
              return any(sig in html.lower() for sig in BLOCK_SIGS)

          def count_tables(html):
              return len(re.findall(r'<table[\s>]', html, re.I))

          def http_get(url):
              req = urllib.request.Request(url)
              req.add_header("User-Agent",
                  "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
                  "AppleWebKit/537.36 Chrome/120.0.0.0 Safari/537.36")
              req.add_header("Accept", "text/html,*/*;q=0.8")
              req.add_header("Referer", f"{EQB_BASE}/")
              resp = urllib.request.urlopen(req, timeout=15)
              return resp.status, resp.read().decode("utf-8", errors="replace")

          def cffi_get(url, impersonate="chrome120"):
              try:
                  from curl_cffi import requests as cffi_http
                  r = cffi_http.get(url, impersonate=impersonate, timeout=20,
                      headers={"Accept": "text/html,*/*;q=0.8",
                               "Referer": f"{EQB_BASE}/"})
                  return r.status_code, r.text
              except ImportError:
                  return None, None

          def _all_subs(cls):
              subs = set(cls.__subclasses__())
              return subs.union(s for c in subs for s in _all_subs(c))

          def get_adapters(atype):
              return [c for c in _all_subs(fortuna.BaseAdapterV3)
                      if not getattr(c, "__abstractmethods__", None)
                      and getattr(c, "ADAPTER_TYPE", "discovery") == atype
                      and hasattr(c, "SOURCE_NAME")]

          # â”€â”€ Pre-compute adapter lists â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
          all_results_classes = fortuna_analytics.get_results_adapter_classes()
          discovery_classes   = get_adapters("discovery")
          usa_adapter_names   = set(getattr(fortuna, "USA_RESULTS_ADAPTERS", []))
          usa_results_classes = [c for c in all_results_classes
                                 if c.SOURCE_NAME in usa_adapter_names]

          ADAPTER_TIMEOUT = 60
          MAX_CONCURRENT  = 4

          # â”€â”€ DB connection â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
          db_path = "fortuna.db"
          if not os.path.exists(db_path):
              sqlite3.connect(db_path).close()
          conn = sqlite3.connect(db_path)
          conn.row_factory = sqlite3.Row
          has_tips = bool(conn.execute(
              "SELECT 1 FROM sqlite_master "
              "WHERE type='table' AND name='tips'"
          ).fetchone())
          has_harvest = bool(conn.execute(
              "SELECT 1 FROM sqlite_master "
              "WHERE type='table' AND name='harvest_logs'"
          ).fetchone())

          # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
          # HEADER
          # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
          emit("# ğŸ‡ Fortuna Diagnostic v4 â€” USA Thoroughbred Pipeline\n")
          emit(f"> General date: **{results_date}** Â· "
               f"USA date: **{usa_test_date}** (Saturday) Â· "
               f"Generated: {today.strftime('%Y-%m-%d %H:%M ET')}\n")
          flush()

          # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
          # Q1 â€” DATABASE REALITY CHECK
          # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
          emit("## Q1: Database Reality Check\n")

          if not has_tips:
              emit("âŒ `tips` table does not exist.\n")
              _evidence['db_has_tips'] = False
          else:
              _evidence['db_has_tips'] = True

              # Discipline Ã— audit breakdown
              emit("### Discipline breakdown (all time)")
              emit("| Discipline | Total | Audited | Stuck | Hit Rate |")
              emit("|------------|------:|--------:|------:|---------:|")
              for row in conn.execute("""
                  SELECT COALESCE(discipline,'?') as disc,
                         COUNT(*) as total,
                         SUM(CASE WHEN audit_completed=1 THEN 1 ELSE 0 END) as ok,
                         SUM(CASE WHEN audit_completed=0 THEN 1 ELSE 0 END) as stuck,
                         ROUND(AVG(CASE WHEN verdict IN ('CASHED','CASHED_ESTIMATED')
                                        THEN 1.0 ELSE 0.0 END)*100,1) as hit
                  FROM tips GROUP BY discipline ORDER BY total DESC
              """):
                  d = {'H':'Harness','T':'Thoroughbred','G':'Greyhound'}.get(
                      row['disc'], row['disc'])
                  emit(f"| {d} | {row['total']} | {row['ok']} | "
                       f"{row['stuck']} | {row['hit'] or 0:.1f}% |")
              emit("")

              # USA T per-venue
              emit("### USA Thoroughbred venues")
              emit("```")
              usa_t_total = usa_t_stuck = 0
              for row in conn.execute("""
                  SELECT venue, COUNT(*) as n,
                         SUM(CASE WHEN audit_completed=1 THEN 1 ELSE 0 END) as ok,
                         SUM(CASE WHEN audit_completed=0 THEN 1 ELSE 0 END) as stuck,
                         MIN(start_time) as first_seen,
                         MAX(start_time) as last_seen
                  FROM tips WHERE discipline='T'
                  GROUP BY venue ORDER BY n DESC
              """):
                  usa_t_total += row['n']
                  usa_t_stuck += row['stuck']
                  flag = "âŒ" if row['stuck'] > 0 and row['ok'] == 0 else (
                         "âš ï¸" if row['stuck'] > row['ok'] else "âœ…")
                  emit(f"  {flag} {row['venue']:28s}  total={row['n']:3d}  "
                       f"ok={row['ok']:3d}  stuck={row['stuck']:3d}  "
                       f"{(row['first_seen'] or '')[:10]}â†’"
                       f"{(row['last_seen'] or '')[:10]}")
              emit("```")
              _evidence['usa_t_total'] = usa_t_total
              _evidence['usa_t_stuck'] = usa_t_stuck

              # Last 7 days trend
              emit("\n### Last 7 days: new T tips per day")
              emit("```")
              week_counts = []
              for row in conn.execute("""
                  SELECT DATE(start_time) as d, COUNT(*) as n
                  FROM tips WHERE discipline='T'
                    AND start_time >= DATE('now','-7 days')
                  GROUP BY d ORDER BY d
              """):
                  week_counts.append(row['n'])
                  emit(f"  {row['d']}  {row['n']:3d} tips")
              if not week_counts:
                  emit("  (none)")
              emit("```")
              _evidence['usa_t_last_7d'] = sum(week_counts)

              # Time-of-day histogram for stuck T tips
              emit("\n### ğŸ‡ºğŸ‡¸ Stuck T tips â€” time-of-day distribution (Eastern)")
              emit("```")
              usa_t_stuck_rows = conn.execute("""
                  SELECT venue, start_time, race_id, race_number
                  FROM tips
                  WHERE audit_completed=0 AND discipline='T'
                  ORDER BY start_time
              """).fetchall()
              hour_counts = {}
              stuck_prefixes = set()
              for row in usa_t_stuck_rows:
                  prefix = row['race_id'].split('_')[0] if '_' in row['race_id'] else row['race_id']
                  stuck_prefixes.add(prefix)
                  try:
                      st = datetime.fromisoformat(
                          str(row['start_time']).replace('Z', '+00:00'))
                      st_et = st.astimezone(EASTERN)
                      h = st_et.hour
                      hour_counts[h] = hour_counts.get(h, 0) + 1
                      emit(f"  {row['venue']:28s} "
                           f"{st_et.strftime('%Y-%m-%d %H:%M ET'):<18s}  "
                           f"{row['race_id']}")
                  except Exception:
                      emit(f"  {row['venue']:28s} "
                           f"{str(row['start_time'])[:16]:<18s}  "
                           f"{row['race_id']}")
              if not usa_t_stuck_rows:
                  emit("  (no stuck Thoroughbred tips)")
              emit("```")
              _evidence['stuck_prefixes'] = stuck_prefixes

              if hour_counts:
                  emit("\n**Hour buckets (Eastern):**")
                  for h in sorted(hour_counts):
                      bar = "â–ˆ" * hour_counts[h]
                      emit(f"  {h:02d}:xx ET  {bar} ({hour_counts[h]})")
              emit("")

          flush()

          # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
          # Q2 â€” ADAPTER INVENTORY
          # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
          emit("## Q2: Adapter Inventory\n")
          emit(f"**Discovery adapters:** {len(discovery_classes)} Â· "
               f"**Results adapters:** {len(all_results_classes)}\n")

          solid = set(getattr(fortuna, 'SOLID_RESULTS_ADAPTERS', []))
          for attr in ['SOLID_RESULTS_ADAPTERS', 'USA_RESULTS_ADAPTERS',
                       'INT_RESULTS_ADAPTERS']:
              val = getattr(fortuna, attr, None)
              if val is not None:
                  emit(f"- `fortuna.{attr}` = `{val}`")
          emit("")

          emit("### Results adapters")
          emit("```")
          for cls in all_results_classes:
              name = cls.SOURCE_NAME
              q = "SOLID" if name in solid else "LOUSY"
              usa = " â—€ USA" if name in usa_adapter_names else ""
              base = getattr(cls, 'BASE_URL', '?')
              engine = '?'
              try:
                  engine = str(cls()._configure_fetch_strategy(
                      ).primary_engine).split('.')[-1]
              except Exception:
                  pass
              emit(f"  {name:40s} [{q:5s}]{usa}")
              emit(f"    {'':40s}  {base}  ({engine})")
          emit("```")

          emit("\n### Discovery adapters")
          emit("```")
          usa_disc_count = 0
          for cls in discovery_classes:
              name = cls.SOURCE_NAME
              base = getattr(cls, 'BASE_URL', '?')
              is_usa = any(k in name.lower() or k in base.lower()
                          for k in ("equibase", "usa", "twinspires"))
              if is_usa:
                  usa_disc_count += 1
              tag = " â—€ USA" if is_usa else ""
              emit(f"  {name:40s}{tag}  {base}")
          emit("```\n")
          _evidence['usa_discovery_adapters'] = usa_disc_count
          _evidence['usa_results_adapters'] = len(usa_results_classes)

          if usa_disc_count == 0:
              emit("> â€¼ï¸ **Zero discovery adapters identified as USA-relevant.** "
                   "Check naming heuristic or `ADAPTER_TYPE` setting.\n")

          flush()

          # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
          # Q3 â€” NETWORK REACHABILITY (fast HEAD checks)
          # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
          emit("## Q3: Network Reachability\n")
          domains = sorted(set(
              getattr(cls, 'BASE_URL', '') for cls in all_results_classes
          ) | {
              "https://greyhounds.attheraces.com",
              "https://www.twinspires.com",
              "https://www.equibase.com",
          })
          emit("```")
          for url in domains:
              if not url:
                  continue
              try:
                  req = urllib.request.Request(url, method="HEAD")
                  req.add_header("User-Agent", "Mozilla/5.0")
                  resp = urllib.request.urlopen(req, timeout=8)
                  emit(f"  âœ… {url:55s} â†’ {resp.status}")
              except Exception as e:
                  emit(f"  âŒ {url:55s} â†’ {type(e).__name__}: "
                       f"{str(e)[:50]}")
          emit("```\n")
          flush()

          # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
          # Q4 â€” EQUIBASE SURGICAL PROBE
          # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
          emit("## Q4: Equibase Surgical Probe\n")
          emit("Tests both entry (discovery) and results (chart) URLs "
               "with raw HTTP and curl_cffi impersonation.\n")

          eqb_urls = {
              "entry_index":     f"{EQB_BASE}/static/entry/index.html?SAP=TN",
              "entry_dated":     f"{EQB_BASE}/static/entry/{DATE_SHORT}USA-TB.html",
              "results_index":   f"{EQB_BASE}/static/chart/summary/index.html?SAP=TN",
              "results_dated":   f"{EQB_BASE}/static/chart/summary/{DATE_SHORT}sum.html",
          }

          emit("### 4a: urllib (baseline)")
          emit("| Key | Status | Length | Blocked? | Tables | Date Links |")
          emit("|-----|-------:|-------:|:--------:|-------:|-----------:|")
          eqb_urllib_ok = False
          for key, url in eqb_urls.items():
              try:
                  status, html = http_get(url)
                  blocked = is_blocked(html)
                  tables = count_tables(html)
                  dlinks = len(re.findall(
                      rf'href="[^"]*{DATE_SHORT}[^"]*\.html"', html, re.I))
                  save_snapshot(f"q4_urllib_{key}", html, url)
                  flag = "ğŸ”´ YES" if blocked else "ğŸŸ¢ no"
                  emit(f"| `{key}` | {status} | {len(html):,} | "
                       f"{flag} | {tables} | {dlinks} |")
                  if not blocked and tables > 0:
                      eqb_urllib_ok = True
              except Exception as e:
                  emit(f"| `{key}` | ERR | â€” | â€” | â€” | "
                       f"`{type(e).__name__}: {str(e)[:40]}` |")
          emit("")
          _evidence['eqb_urllib_reachable'] = eqb_urllib_ok

          emit("### 4b: curl_cffi (adapter-realistic)")
          emit("| Key | Impersonate | Status | Length | Blocked? | Tables |")
          emit("|-----|-------------|-------:|-------:|:--------:|-------:|")
          eqb_cffi_ok = False
          cffi_targets = [
              ("entry_index",   eqb_urls["entry_index"]),
              ("results_index", eqb_urls["results_index"]),
          ]
          for label, url in cffi_targets:
              for imp in ("chrome120", "chrome110", "safari15_5"):
                  try:
                      status, html = cffi_get(url, imp)
                      if status is None:
                          emit(f"| `{label}` | {imp} | â€” | â€” | â€” | "
                               f"curl_cffi N/A |")
                          continue
                      blocked = is_blocked(html)
                      tables = count_tables(html)
                      save_snapshot(f"q4_cffi_{label}_{imp}", html, url)
                      flag = "ğŸ”´ YES" if blocked else "ğŸŸ¢ no"
                      emit(f"| `{label}` | {imp} | {status} | "
                           f"{len(html):,} | {flag} | {tables} |")
                      if not blocked and tables > 0:
                          eqb_cffi_ok = True
                  except Exception as e:
                      emit(f"| `{label}` | {imp} | ERR | â€” | â€” | "
                           f"`{str(e)[:35]}` |")
          emit("")
          _evidence['eqb_cffi_reachable'] = eqb_cffi_ok

          # Extract track names from best snapshot
          emit("### 4c: Track links in best Equibase response")
          best_html = ""
          for suf in ("q4_cffi_entry_index_chrome120",
                       "q4_cffi_entry_index_chrome110",
                       "q4_urllib_entry_index"):
              p = SNAP_DIR / f"{suf}.html"
              if p.exists():
                  cand = p.read_text()
                  if not is_blocked(cand) and count_tables(cand) > 0:
                      best_html = cand
                      emit(f"Using: `{suf}`")
                      break
          if best_html:
              emit("```")
              tracks_found = set()
              for href, text in re.findall(
                  r'href="([^"]*\.html)"[^>]*>\s*([^<]+)', best_html):
                  if DATE_SHORT in href and "index" not in href.lower():
                      t = text.strip()
                      if t:
                          tracks_found.add(t)
                          emit(f"  âœ… {t:30s} â†’ {href[:70]}")
              if not tracks_found:
                  emit("  (no track links for target date)")
              emit("```")
              _evidence['eqb_tracks_in_html'] = tracks_found
          else:
              emit("> âš ï¸ No usable Equibase HTML â€” all blocked or empty.\n")
              _evidence['eqb_tracks_in_html'] = set()
          emit("")

          flush()

          # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
          # Q5 â€” RACING POST SLUG AUDIT (THE KEY SECTION)
          # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
          emit("## Q5: Racing Post Index â€” Slug Audit\n")
          emit(f"Fetching `https://www.racingpost.com/results/"
               f"{usa_test_date}` to find slug gaps.\n")
          flush()

          _evidence['rp_slug_gaps'] = []
          _evidence['rp_usa_links_count'] = 0
          _evidence['rp_all_links_count'] = 0

          if deadline_exceeded():
              emit("âš ï¸ Skipped â€” deadline.\n")
          else:
              try:
                  from curl_cffi import requests as cffi_req
                  rp_url = (f"https://www.racingpost.com/results/"
                            f"{usa_test_date}")
                  rp_resp = cffi_req.get(rp_url, impersonate="chrome120",
                      timeout=35, headers={
                          "User-Agent": "Mozilla/5.0 (Windows NT 10.0; "
                              "Win64; x64) AppleWebKit/537.36 Chrome/120",
                          "Accept": "text/html,*/*;q=0.8",
                          "Referer": "https://www.racingpost.com/",
                      })
                  emit(f"**HTTP {rp_resp.status_code}** â€” "
                       f"{len(rp_resp.text):,} chars\n")
                  save_snapshot("q5_rp_index", rp_resp.text, rp_url)

                  if (rp_resp.status_code == 200
                      and len(rp_resp.text) > 5000):
                      # Extract all /results/ hrefs
                      try:
                          from selectolax.parser import HTMLParser as _SL
                          _hrefs = [a.attributes.get("href", "")
                                    for a in _SL(rp_resp.text).css(
                                        'a[href*="/results/"]')]
                      except ImportError:
                          _hrefs = re.findall(
                              r'href="(/results/[^"]+)"', rp_resp.text)

                      _rp_base = "https://www.racingpost.com"
                      race_links = sorted(set(
                          (h if h.startswith("http") else _rp_base + h)
                          for h in _hrefs
                          if usa_test_date in h
                          and len(h.split("/")) >= 4
                      ))
                      _evidence['rp_all_links_count'] = len(race_links)
                      emit(f"**Total dated links:** {len(race_links)}\n")

                      def _slug(u):
                          try:
                              return u.split("/results/")[1].split("/")[0]
                          except Exception:
                              return ""

                      slugs_on_page = sorted(set(
                          _slug(u) for u in race_links if _slug(u)))

                      # Cross-ref against _USA_TRACK_SLUGS
                      try:
                          _rp_usa_cls = next(
                              c for c in all_results_classes
                              if c.SOURCE_NAME == "RacingPostUSAResults")
                          _known = _rp_usa_cls._USA_TRACK_SLUGS
                          _known_bare = {s.replace("-", ""): s
                                         for s in _known}

                          usa_links = [u for u in race_links
                                       if _rp_usa_cls._is_usa_link(u)]
                          _evidence['rp_usa_links_count'] = len(usa_links)
                          emit(f"**Links passing `_is_usa_link()`:** "
                               f"{len(usa_links)}\n")

                          if usa_links:
                              emit("### âœ… USA links that passed")
                              emit("```")
                              for lnk in usa_links[:20]:
                                  emit(f"  {_slug(lnk):<30s}  "
                                       f"{lnk[:80]}")
                              emit("```\n")
                          else:
                              emit("### âŒ ZERO links passed the "
                                   "USA slug filter\n")

                          emit("### All slugs on RP results page")
                          emit("```")
                          for s in slugs_on_page:
                              if not s:
                                  continue
                              ok = (s in _known or
                                    s.replace("-", "") in _known_bare)
                              emit(f"  {'âœ…' if ok else 'âŒ'}  {s}")
                          emit("```\n")

                          gaps = [s for s in slugs_on_page
                                  if s and s not in _known
                                  and s.replace("-", "")
                                  not in _known_bare]
                          _evidence['rp_slug_gaps'] = gaps

                          if gaps:
                              emit("### âš ï¸ Slugs NOT in "
                                   "`_USA_TRACK_SLUGS` â€” review")
                              emit("```python")
                              emit("# Add US/CAN tracks to "
                                   "_USA_TRACK_SLUGS:")
                              for g in gaps:
                                  emit(f'    "{g}",')
                              emit("```\n")

                      except StopIteration:
                          emit("âš ï¸ `RacingPostUSAResultsAdapter` "
                               "not found.\n")
                          emit("```")
                          for s in slugs_on_page:
                              emit(f"  {s}")
                          emit("```\n")

                  elif rp_resp.status_code == 429:
                      emit("âŒ **HTTP 429** â€” RP rate-limiting "
                           "GitHub Actions IPs.\n")
                  else:
                      emit(f"âŒ Unusable response "
                           f"({len(rp_resp.text)} chars).\n")

              except ImportError:
                  emit("âš ï¸ `curl_cffi` unavailable.\n")
              except Exception as e:
                  emit(f"âŒ RP fetch failed: {e}\n```\n"
                       f"{traceback.format_exc()[:500]}\n```\n")

          flush()

          # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
          # Q6 â€” SLUG COVERAGE + KEY ANATOMY (stuck tips â†” slugs)
          # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
          emit("## Q6: Slug Coverage & Key Anatomy â€” Stuck Tips\n")

          _evidence['venue_slug_misses'] = []

          if has_tips:
              # Key anatomy
              emit("### Canonical keys for stuck T tips")
              emit("```")
              emit(f"  {'race_id':<45s}  {'tip_key':<42s}  "
                   f"{'canonical'}")
              emit(f"  {'â”€'*45}  {'â”€'*42}  {'â”€'*22}")
              for row in conn.execute("""
                  SELECT race_id, venue, race_number,
                         start_time, discipline
                  FROM tips WHERE audit_completed=0 AND discipline='T'
                  ORDER BY start_time LIMIT 20
              """):
                  tip = dict(row)
                  key = fortuna_analytics.AuditorEngine._tip_canonical_key(
                      tip)
                  canon = fortuna.get_canonical_venue(tip['venue'])
                  emit(f"  {tip['race_id']:<45s}  {str(key):<42s}  "
                       f"{canon}")
              emit("```")
              emit("\n**Key format:** "
                   "`{canonical_venue}|{race}|{YYYYMMDD}|{HHMM}|T`\n")

              # Slug coverage audit
              try:
                  _rp_usa_cls = next(
                      c for c in all_results_classes
                      if c.SOURCE_NAME == "RacingPostUSAResults")
                  _slugs = _rp_usa_cls._USA_TRACK_SLUGS
                  _slug_bare = {s.replace("-", ""): s for s in _slugs}

                  stuck_venues = [row[0] for row in conn.execute(
                      "SELECT DISTINCT venue FROM tips "
                      "WHERE audit_completed=0 AND discipline='T' "
                      "ORDER BY venue"
                  ).fetchall()]

                  emit("### Stuck venue â†’ slug match")
                  emit("```")
                  emit(f"  {'VENUE':<30s} {'CANONICAL':<22s} "
                       f"{'VERDICT':<10s}  NOTE")
                  emit(f"  {'â”€'*30} {'â”€'*22} {'â”€'*10}  {'â”€'*30}")
                  misses = []
                  for venue in stuck_venues:
                      canon = fortuna.get_canonical_venue(venue)
                      direct = next((s for s in _slugs
                                     if s.replace("-", "") in canon),
                                    None)
                      reverse = next((orig for bare, orig
                                      in _slug_bare.items()
                                      if canon in bare), None)
                      match = direct or reverse
                      if match:
                          emit(f"  {venue:<30s} {canon:<22s} "
                               f"{'âœ… MATCH':<10s}  slug: '{match}'")
                      else:
                          suggested = re.sub(r"\s+", "-",
                                            venue.lower().strip())
                          emit(f"  {venue:<30s} {canon:<22s} "
                               f"{'âŒ MISS':<10s}  "
                               f'try: "{suggested}"')
                          misses.append(venue)
                  emit("```")
                  _evidence['venue_slug_misses'] = misses

                  if misses:
                      emit("\n### âŒ Must add to `_USA_TRACK_SLUGS`")
                      emit("```python")
                      for v in misses:
                          slug = re.sub(r"\s+", "-", v.lower().strip())
                          emit(f'    "{slug}",   # â† {v}')
                      emit("```\n")
                  else:
                      emit("\nâœ… All stuck venues have slug coverage. "
                           "Gap is elsewhere â€” check Q5 and Q8.\n")

              except StopIteration:
                  emit("âš ï¸ `RacingPostUSAResultsAdapter` not found.\n")
          else:
              emit("â„¹ï¸ No tips table â€” skipping.\n")

          flush()

          # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
          # Q7 â€” FULL RESULTS ADAPTER SWEEP (control group)
          # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
          emit("## Q7: Full Results Adapter Sweep "
               f"({results_date})\n")
          emit("Control group â€” proves other regions work.\n")
          flush()

          async def _test_adapter(cls, date):
              name = cls.SOURCE_NAME
              lines = []
              try:
                  adapter = cls()
                  t0 = datetime.now()

                  link_count = None
                  if hasattr(adapter, "_discover_result_links"):
                      try:
                          links = await adapter._discover_result_links(date)
                          link_count = len(links)
                      except Exception:
                          pass

                  races = await adapter.get_races(date)
                  elapsed = (datetime.now() - t0).total_seconds()
                  lk = f" ({link_count} links)" if link_count is not None else ""

                  if not races:
                      lines.append(f"- âš ï¸ `{name}` â†’ **0 races**"
                                   f"{lk} in {elapsed:.1f}s")
                  else:
                      venues = sorted(set(r.venue for r in races))
                      lines.append(
                          f"- âœ… `{name}` â†’ **{len(races)} races**"
                          f"{lk} in {elapsed:.1f}s")
                      lines.append(
                          f"  Venues: {', '.join(venues[:8])}")
                      for r in races[:3]:
                          lines.append(
                              f"  Key: `{r.canonical_key}` "
                              f"runners: {len(r.runners)}")
                  try:
                      await adapter.close()
                  except Exception:
                      pass
              except asyncio.TimeoutError:
                  lines.append(f"- â±ï¸ `{name}` â†’ TIMED OUT")
              except Exception as e:
                  lines.append(
                      f"- âŒ `{name}` â†’ **{type(e).__name__}:** "
                      f"`{str(e)[:100]}`")
                  lines.append(
                      f"  ```\n  {traceback.format_exc()[:400]}\n  ```")
              return name, lines, (len(races) if 'races' in dir() and races else 0)

          async def _run_sweep(classes, date, label):
              sem = asyncio.Semaphore(MAX_CONCURRENT)
              total_found = 0

              async def bounded(cls):
                  async with sem:
                      return await asyncio.wait_for(
                          _test_adapter(cls, date),
                          timeout=ADAPTER_TIMEOUT)

              tasks = [asyncio.create_task(bounded(c))
                       for c in classes]
              for coro in asyncio.as_completed(tasks):
                  try:
                      name, lines, count = await coro
                      total_found += count
                  except Exception as e:
                      lines = [f"- âŒ (unknown) â†’ {e}"]
                  for line in lines:
                      emit(line)
                  emit("")
                  flush()

              try:
                  await fortuna.GlobalResourceManager.cleanup()
              except Exception:
                  pass
              return total_found

          if deadline_exceeded():
              emit("âš ï¸ Skipped â€” deadline.\n")
              flush()
          else:
              asyncio.run(_run_sweep(
                  all_results_classes, results_date, "all"))

          # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
          # Q8 â€” USA RESULTS ADAPTERS (Saturday date)
          # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
          emit(f"## Q8: USA Results Adapters â€” Saturday "
               f"({usa_test_date})\n")
          emit(f"Testing **{len(usa_results_classes)}** adapters: "
               f"`{[c.SOURCE_NAME for c in usa_results_classes]}`\n")
          emit("> A weekday often returns 0 even when working. "
               "Saturday is the fairest test.\n")
          flush()

          if deadline_exceeded():
              emit("âš ï¸ Skipped â€” deadline.\n")
              _evidence['res_usa_found'] = 0
              flush()
          else:
              try:
                  n = asyncio.run(_run_sweep(
                      usa_results_classes, usa_test_date, "usa"))
                  _evidence['res_usa_found'] = n
              except Exception as e:
                  emit(f"âŒ Sweep failed: {e}")
                  _evidence['res_usa_found'] = 0
              flush()

          # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
          # Q9 â€” DISCOVERY PIPELINE TEST
          # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
          emit("## Q9: Discovery Pipeline Test\n")
          emit("Tests adapters that find **upcoming** races "
               "(entries/racecards).\n")
          flush()

          if not discovery_classes:
              emit("âŒ No discovery adapters registered.\n")
              _evidence['disc_tb_found'] = 0
          elif deadline_exceeded():
              emit("âš ï¸ Skipped â€” deadline.\n")
              _evidence['disc_tb_found'] = 0
          else:
              async def _test_discovery():
                  sem = asyncio.Semaphore(3)
                  tb_total = 0

                  async def probe(cls):
                      name = cls.SOURCE_NAME
                      try:
                          adapter = cls()
                          races = await asyncio.wait_for(
                              adapter.get_races(results_date),
                              timeout=ADAPTER_TIMEOUT)
                          try:
                              await adapter.close()
                          except Exception:
                              pass
                          tb = [r for r in (races or [])
                                if (getattr(r, 'discipline', '')
                                    or '').upper().startswith('T')]
                          vs = sorted(set(r.venue for r in tb)
                                      ) if tb else []
                          return name, len(races or []), len(tb), vs
                      except asyncio.TimeoutError:
                          return name, -1, 0, []
                      except Exception as e:
                          return name, -2, 0, [str(e)[:60]]

                  async def bounded(cls):
                      async with sem:
                          return await probe(cls)

                  tasks = [asyncio.create_task(bounded(c))
                           for c in discovery_classes]
                  for coro in asyncio.as_completed(tasks):
                      name, total, tb, info = await coro
                      nonlocal tb_total
                      tb_total += max(tb, 0)
                      if total == -1:
                          emit(f"- â±ï¸ `{name}`: TIMED OUT")
                      elif total == -2:
                          emit(f"- âŒ `{name}`: {info[0] if info else '?'}")
                      elif tb == 0:
                          emit(f"- âš ï¸ `{name}`: {total} total, "
                               f"**0 thoroughbred**")
                      else:
                          emit(f"- âœ… `{name}`: {total} total, "
                               f"**{tb} thoroughbred** â€” "
                               f"{', '.join(info[:6])}")
                      flush()

                  try:
                      await fortuna.GlobalResourceManager.cleanup()
                  except Exception:
                      pass
                  return tb_total

              try:
                  _evidence['disc_tb_found'] = asyncio.run(
                      _test_discovery())
              except Exception as e:
                  emit(f"âŒ Discovery test failed: {e}")
                  _evidence['disc_tb_found'] = 0
          emit("")

          flush()

          # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
          # Q10 â€” PLAYWRIGHT BROWSER PING
          # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
          emit("## Q10: Playwright Browser Ping\n")

          async def _browser_ping():
              from playwright.async_api import async_playwright
              urls = [
                  f"https://www.racingpost.com/results/{usa_test_date}",
                  f"{EQB_BASE}/static/chart/summary/index.html",
                  "https://www.attheraces.com/results",
                  "https://greyhounds.attheraces.com",
              ]
              extra = [getattr(c, 'BASE_URL', '')
                       for c in all_results_classes
                       if getattr(c, 'BASE_URL', '') not in urls]
              urls = list(dict.fromkeys(urls + [u for u in extra if u]))[:8]

              async with async_playwright() as p:
                  browser = await p.chromium.launch(
                      headless=True, args=["--disable-gpu", "--no-sandbox"])
                  ctx = await browser.new_context(
                      user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; "
                          "x64) AppleWebKit/537.36 Chrome/125.0.0.0",
                      viewport={"width": 1280, "height": 720})

                  for url in urls:
                      try:
                          page = await ctx.new_page()
                          t0 = datetime.now()
                          resp = await page.goto(
                              url, wait_until="domcontentloaded",
                              timeout=12000)
                          el = (datetime.now() - t0).total_seconds()
                          html = await page.content()
                          title = await page.title()
                          status = resp.status if resp else "?"
                          blocked = any(
                              sig in html.lower() or sig in title.lower()
                              for sig in BLOCK_SIGS)
                          save_snapshot(
                              f"q10_pw_{re.sub(r'[^a-z0-9]','_',url.split('//')[1][:40])}",
                              html, url)
                          if blocked:
                              emit(f"- âŒ `{url}` â†’ **BOT BLOCKED** "
                                   f"({title[:30]}) {status}")
                          else:
                              emit(f"- âœ… `{url}` â†’ {title[:40]}â€¦ | "
                                   f"{status} ({len(html):,}c) "
                                   f"{el:.1f}s")
                          await page.close()
                      except Exception as e:
                          emit(f"- âŒ `{url}` â†’ `{str(e)[:60]}`")
                      flush()
                  await browser.close()

          _evidence['eqb_pw_reachable'] = None
          if deadline_exceeded():
              emit("âš ï¸ Skipped â€” deadline.\n")
          else:
              try:
                  asyncio.run(_browser_ping())
              except Exception as e:
                  emit(f"âš ï¸ Playwright failed: {e}")
          emit("")

          flush()

          # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
          # Q11 â€” HARVEST HISTORY
          # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
          emit("## Q11: Harvest History\n")

          for fname in ('results_harvest.json', 'discovery_harvest.json'):
              p = Path(fname)
              if p.exists():
                  try:
                      data = json.loads(p.read_text())
                      usa_keys = {k: v for k, v in data.items()
                                  if any(s in k.lower() for s in
                                         ('equibase','usa','america'))}
                      if usa_keys:
                          emit(f"### `{fname}` â€” USA entries")
                          emit(f"```json\n"
                               f"{json.dumps(usa_keys, indent=2)}\n```\n")
                      else:
                          emit(f"â„¹ï¸ `{fname}` â€” no USA-keyed entries.\n")
                  except Exception:
                      pass
              else:
                  emit(f"âš ï¸ `{fname}` not found.\n")

          if has_harvest:
              emit("### Last 20 harvest entries (â—€ = USA adapter)")
              emit("```")
              for row in conn.execute("""
                  SELECT timestamp, region, adapter_name,
                         race_count, max_odds
                  FROM harvest_logs ORDER BY id DESC LIMIT 20
              """):
                  rc = row['race_count'] or 0
                  tag = " â—€" if row['adapter_name'] in usa_adapter_names else "  "
                  flag = "âœ…" if rc > 0 else "âŒ"
                  emit(f"  {flag} {(row['timestamp'] or '')[:19]}  "
                       f"{row['region'] or '?':6s}  "
                       f"{row['adapter_name']:35s}{tag}  "
                       f"races={rc:3d}  "
                       f"maxOdds={row['max_odds'] or 0:.1f}")
              emit("```\n")

              emit("### 30-day USA harvest success rate")
              emit("```")
              for row in conn.execute("""
                  SELECT adapter_name,
                         COUNT(*) as attempts,
                         SUM(CASE WHEN race_count>0 THEN 1 ELSE 0 END) as ok,
                         ROUND(AVG(race_count),1) as avg_rc
                  FROM harvest_logs
                  WHERE (adapter_name LIKE '%Equibase%'
                      OR adapter_name LIKE '%USA%'
                      OR region='USA')
                    AND timestamp >= DATE('now','-30 days')
                  GROUP BY adapter_name ORDER BY attempts DESC
              """):
                  rate = (row['ok']/row['attempts']*100
                          ) if row['attempts'] else 0
                  emit(f"  {'âœ…' if rate>50 else 'âŒ'} "
                       f"{row['adapter_name']:35s}  "
                       f"{row['ok']}/{row['attempts']} ({rate:.0f}%)  "
                       f"avg={row['avg_rc']} races/run")
                  _evidence['harvest_success_rate'] = rate
              emit("```\n")
          else:
              emit("â„¹ï¸ `harvest_logs` table not found.\n")

          flush()

          # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
          # Q12 â€” AUTOMATED ROOT-CAUSE DIAGNOSIS
          # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
          emit("## Q12: ğŸ¯ Automated Root-Cause Diagnosis\n")

          diagnosis = []
          severity = "INFO"

          # Layer 1: Discovery
          disc_tb = _evidence.get('disc_tb_found', 0)
          t_7d = _evidence.get('usa_t_last_7d', 0)

          if disc_tb == 0 and t_7d == 0:
              severity = "CRITICAL"
              diagnosis.append(
                  "ğŸ”´ **CRITICAL: Zero USA T races from discovery "
                  "AND zero new tips in 7 days.**")
              if _evidence.get('usa_discovery_adapters', 0) == 0:
                  diagnosis.append(
                      "   â†’ No discovery adapter is USA-relevant. "
                      "Check `ADAPTER_TYPE` and `SOURCE_NAME`.")
              elif (not _evidence.get('eqb_cffi_reachable')
                    and not _evidence.get('eqb_urllib_reachable')):
                  diagnosis.append(
                      "   â†’ Equibase is **blocked** from GitHub "
                      "Actions (both curl_cffi and urllib).")
                  diagnosis.append(
                      "   â†’ Fix: (a) residential proxy, "
                      "(b) RP USA as primary discovery, "
                      "(c) non-datacenter scheduling.")
              elif _evidence.get('eqb_tracks_in_html'):
                  diagnosis.append(
                      f"   â†’ Equibase IS reachable and lists "
                      f"{len(_evidence['eqb_tracks_in_html'])} tracks, "
                      f"but adapter returns 0 â€” parser may be broken. "
                      f"Check Q4c HTML snapshots.")
          elif disc_tb > 0:
              diagnosis.append(
                  f"ğŸŸ¢ Discovery found **{disc_tb}** T race(s).")

          # Layer 2: Results
          res_usa = _evidence.get('res_usa_found', 0)
          if res_usa == 0:
              if severity != "CRITICAL":
                  severity = "HIGH"
              diagnosis.append(
                  "ğŸŸ  **No USA results adapter returned data** "
                  "on the Saturday test date.")
          else:
              diagnosis.append(
                  f"ğŸŸ¢ USA results adapters returned "
                  f"**{res_usa}** race(s) on Saturday.")

          # Layer 3: Slug gaps
          slug_gaps = _evidence.get('rp_slug_gaps', [])
          if slug_gaps:
              if severity == "INFO":
                  severity = "MEDIUM"
              diagnosis.append(
                  f"ğŸŸ¡ **{len(slug_gaps)} RP slug(s)** on page "
                  f"but NOT in `_USA_TRACK_SLUGS`: "
                  f"`{slug_gaps[:8]}`")
              diagnosis.append(
                  "   â†’ See Q5 for copy-paste fix code.")

          # Layer 4: Venue canon mismatches
          v_misses = _evidence.get('venue_slug_misses', [])
          if v_misses:
              if severity == "INFO":
                  severity = "MEDIUM"
              diagnosis.append(
                  f"ğŸŸ¡ **{len(v_misses)} stuck venue(s)** have "
                  f"no slug coverage: `{v_misses[:5]}`")
              diagnosis.append(
                  "   â†’ See Q6 for suggested slugs to add.")

          # Layer 5: Stuck tip ratio
          stuck = _evidence.get('usa_t_stuck', 0)
          total = _evidence.get('usa_t_total', 0)
          if total > 0 and stuck > 0:
              diagnosis.append(
                  f"â„¹ï¸  {stuck}/{total} USA T tips unaudited "
                  f"({stuck/total*100:.0f}%).")

          # Layer 6: Harvest trend
          hr = _evidence.get('harvest_success_rate', -1)
          if 0 <= hr < 30:
              diagnosis.append(
                  f"ğŸŸ  USA harvest success rate is only "
                  f"**{hr:.0f}%** over 30 days.")

          emit(f"### Severity: **{severity}**\n")
          for line in diagnosis:
              emit(line)
          emit("")

          # Recommended steps
          emit("### Recommended Next Steps\n")
          if slug_gaps or v_misses:
              emit("1. **Add missing slugs** to "
                   "`RacingPostUSAResultsAdapter._USA_TRACK_SLUGS` "
                   "â€” see Q5/Q6 for the exact lines.")
              emit("2. Re-run this diagnostic to confirm the fix.")
          elif (not _evidence.get('eqb_cffi_reachable')
                and not _evidence.get('eqb_urllib_reachable')):
              emit("1. **Equibase is blocked.** Download "
                   "`usa-tb-html-snapshots` to confirm.")
              emit("2. **Promote RP USA** â€” ensure "
                   "`RacingPostUSAResultsAdapter` is in "
                   "`USA_RESULTS_ADAPTERS`.")
              emit("3. Consider a residential proxy or "
                   "RP-based discovery adapter for US tracks.")
          elif disc_tb == 0:
              emit("1. **Check the discovery adapter** for USA T â€” "
                   "it may not be registered or `get_races()` "
                   "is silently failing.")
              emit("2. Run locally with `--debug` and inspect "
                   "discovery logs.")
          elif res_usa == 0:
              emit("1. **USA results adapters returned 0** on "
                   "Saturday â€” check adapter logs for errors.")
              emit("2. Verify `USA_RESULTS_ADAPTERS` list in "
                   "`fortuna.py` includes the right names.")
          else:
              emit("1. Pipeline appears functional. If tips still "
                   "aren't appearing, check "
                   "`SimplySuccessAnalyzer.qualify_races()` "
                   "thresholds â€” races may be discovered but "
                   "filtered out by gap/odds criteria.")
          emit("")

          # DB shadow comparison (quick)
          emit("---\n## Appendix: DB Shadow Comparison\n")
          db_files = [f for f in os.listdir('.')
                      if f.endswith('.db') and f != 'fortuna.db']
          if not db_files:
              emit("â„¹ï¸ No shadow databases found.\n")
          else:
              import subprocess
              for dbf in db_files:
                  try:
                      res = subprocess.run(
                          ["python3", "scripts/db_shadow_compare.py",
                           "fortuna.db", dbf],
                          capture_output=True, text=True)
                      emit(f"### `{dbf}`\n```\n{res.stdout}\n```\n")
                      if res.returncode != 0:
                          emit(f"```\n{res.stderr}\n```\n")
                  except Exception as e:
                      emit(f"âš ï¸ `{dbf}`: {e}\n")

          conn.close()
          emit(f"\n---\n*Diagnostic v4 complete â€” "
               f"{remaining_minutes():.1f} min remaining.*")
          flush()
          PYEOF

      - name: Upload HTML Forensics
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: usa-tb-html-snapshots
          path: _diag_snapshots/
          retention-days: 5
          if-no-files-found: ignore
