name: List of Best Bets

on:
  push:
    branches: [main]
    paths-ignore: ["**.md", ".github/workflows/build_monolith.yml", "docs/**"]
  schedule:
    - cron: '0 * * * *'
  workflow_dispatch:
    inputs:
      hours:
        description: 'Discovery window in hours'
        required: false
        default: '8'
        type: string
      days:
        description: 'Audit lookback days'
        required: false
        default: '4'
        type: string

concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true

jobs:
  # PHASE 1: Parallel Discovery
  # Fetches racing data from distinct sets of adapters.
  fetch:
    runs-on: ubuntu-22.04
    timeout-minutes: 30
    strategy:
      matrix:
        include:
          - type: entry
            quality: solid
            slug: entry-solid
          - type: entry
            quality: lousy
            slug: entry-lousy
      fail-fast: false
    steps:
      - uses: actions/checkout@v6
      - uses: actions/setup-python@v5
        with:
          python-version: '3.11'
      - uses: actions/cache@v4
        with:
          path: |
            ~/.cache/pip
            ~/.cache/ms-playwright
          key: deps-${{ runner.os }}-py311-${{ hashFiles('requirements.txt') }}

      - name: Restore Regional Database
        id: cache-db
        uses: actions/cache/restore@v4
        with:
          path: fortuna.db
          # Scenario A: Regions maintain separate history baseline to avoid race conditions.
          # These are merged in the audit-results job later.
          key: fortuna-db-v3-${{ matrix.slug }}-${{ github.run_number }}
          restore-keys: fortuna-db-v3-${{ matrix.slug }}-

      - name: Install dependencies
        run: |
          python3 -m pip install --upgrade pip
          python3 -m pip install -r requirements.txt
          python3 -m browserforge update
          playwright install --with-deps chromium

      - name: Initialize DB if needed
        if: steps.cache-db.outputs.cache-hit != 'true'
        run: python3 -c "import fortuna; import asyncio; asyncio.run(fortuna.FortunaDB().initialize())"

      - name: Run Fetcher (Entry or Results)
        timeout-minutes: 20
        env:
          PYTHONPATH: .
        run: |
          START_TIME=$(date +%s)

          echo "Running ${{ matrix.quality }} Entry Discovery..."
          python3 fortuna.py \
            --quality ${{ matrix.quality }} \
            --save "races_${{ matrix.slug }}.json" \
            --hours "${{ github.event.inputs.hours || '8' }}" \
          || echo '[]' > "races_${{ matrix.slug }}.json"

          END_TIME=$(date +%s)
          DURATION=$((END_TIME - START_TIME))
          echo "::notice::${{ matrix.slug }} completed in ${DURATION}s"

      - name: Prepare Artifact
        if: always()
        run: |
          cp fortuna.db "fortuna_${{ matrix.slug }}.db"
          mv discovery_harvest.json "discovery_harvest_${{ matrix.slug }}.json" 2>/dev/null \
            || echo '{}' > "discovery_harvest_${{ matrix.slug }}.json"
          mv results_harvest.json "results_harvest_${{ matrix.slug }}.json" 2>/dev/null \
            || echo '{}' > "results_harvest_${{ matrix.slug }}.json"

      - uses: actions/upload-artifact@v4
        if: always()
        with:
          name: results-${{ matrix.slug }}
          path: |
            races_${{ matrix.slug }}.json
            fortuna_${{ matrix.slug }}.db
            discovery_harvest_${{ matrix.slug }}.json
          retention-days: 1

  # PHASE 2: Data Consolidation & Audit
  # Merges regional data and verifies results against the historical database.
  audit-results:
    needs: [fetch]
    if: always()
    runs-on: ubuntu-22.04
    timeout-minutes: 25
    steps:
      - uses: actions/checkout@v6
      - uses: actions/setup-python@v5
        with:
          python-version: '3.11'
      - uses: actions/cache@v4
        with:
          path: ~/.cache/pip
          key: deps-${{ runner.os }}-py311-audit-${{ hashFiles('requirements.txt') }}

      - name: Restore Master Database
        id: cache-db
        uses: actions/cache/restore@v4
        with:
          path: fortuna.db
          key: fortuna-db-v3-master-${{ github.run_number }}
          restore-keys: fortuna-db-v3-master-

      - name: Install dependencies
        run: |
          python3 -m pip install --upgrade pip
          python3 -m pip install -r requirements.txt
          python3 -m browserforge update

      - name: Initialize DB if needed
        if: steps.cache-db.outputs.cache-hit != 'true'
        run: python3 -c "import fortuna; import asyncio; asyncio.run(fortuna.FortunaDB().initialize())"

      - name: Download Discovery Artifacts
        uses: actions/download-artifact@v4

      - name: Consolidate Discovery Data
        run: |
          set -eo pipefail
          echo "Validating parallel artifacts..."

          ARTIFACT_COUNT=0
          for slug in entry-solid entry-lousy; do
            DIR="results-${slug}"
            if [ -d "$DIR" ]; then
              ARTIFACT_COUNT=$((ARTIFACT_COUNT + 1))
              cp "$DIR/races_${slug}.json" . 2>/dev/null || echo '[]' > "races_${slug}.json"
              # Try to get the regional DB, fallback to master ONLY if it exists
              cp "$DIR/fortuna_${slug}.db" . 2>/dev/null || { [ -f fortuna.db ] && cp fortuna.db "fortuna_${slug}.db" || true; }
              cp "$DIR/discovery_harvest_${slug}.json" . 2>/dev/null || echo '{}' > "discovery_harvest_${slug}.json"
              cp "$DIR/results_harvest_${slug}.json" . 2>/dev/null || echo '{}' > "results_harvest_${slug}.json"
            else
              echo "::warning::Artifact directory $DIR missing"
              echo '[]' > "races_${slug}.json"
              [ -f fortuna.db ] && cp fortuna.db "fortuna_${slug}.db" || true
              echo '{}' > "discovery_harvest_${slug}.json"
              echo '{}' > "results_harvest_${slug}.json"
            fi
          done

          if [ $ARTIFACT_COUNT -eq 0 ]; then
            echo "::error::No discovery artifacts found! All fetch jobs failed."
            exit 1
          fi

          echo "Consolidating $ARTIFACT_COUNT regional artifacts..."
          python3 scripts/consolidate_parallel_runs.py \
            --json-pattern "races_*.json" \
            --db-pattern "fortuna_*.db" \
            --output-json "raw_races.json" \
            --output-db "fortuna.db"

      - name: Run Results Audit
        timeout-minutes: 15
        env:
          PYTHONPATH: .
        run: python3 fortuna_analytics.py --days "${{ github.event.inputs.days || '4' }}"

      - name: Prepare Audit Artifact
        if: always()
        run: |
          cp results_harvest.json results_harvest_audit.json 2>/dev/null \
            || echo '{}' > results_harvest_audit.json

      - uses: actions/upload-artifact@v4
        if: always()
        with:
          name: audit-results
          path: |
            analytics_report.txt
            fortuna.db
            raw_races.json
            results_harvest_audit.json
            discovery_harvest_*.json
          retention-days: 1

  # PHASE 3: Reporting & Summary
  # Generates the dashboard and persists the updated master database.
  conclude:
    needs: [audit-results]
    if: always()
    runs-on: ubuntu-22.04
    timeout-minutes: 15
    steps:
      - uses: actions/checkout@v6
      - uses: actions/setup-python@v5
        with:
          python-version: '3.11'
      - uses: actions/cache@v4
        with:
          path: ~/.cache/pip
          key: deps-conclude-${{ runner.os }}-py311-${{ hashFiles('requirements.txt') }}

      - name: Download Audit Artifact
        uses: actions/download-artifact@v4
        with:
          name: audit-results

      - name: Install summary dependencies
        run: |
          python3 -m pip install --upgrade pip
          python3 -m pip install -r requirements.txt

      - name: Generate Workflow Metadata
        run: |
          cat > workflow_metadata.json <<EOF
          {
            "run_number": ${{ github.run_number }},
            "run_id": "${{ github.run_id }}",
            "workflow": "${{ github.workflow }}",
            "ref": "${{ github.ref }}",
            "sha": "${{ github.sha }}",
            "actor": "${{ github.actor }}",
            "event": "${{ github.event_name }}",
            "timestamp": "$(date -u +%Y-%m-%dT%H:%M:%SZ)"
          }
          EOF

      - name: Run Monitor Report
        run: |
          if [ -s raw_races.json ] && python3 -c "import json; d=json.load(open('raw_races.json')); assert len(d)>0" 2>/dev/null; then
            python3 fortuna.py --load raw_races.json --monitor --once
          else
            echo "::warning::No valid race data to monitor"
          fi

      - name: Generate Job Summary
        if: always()
        run: python3 scripts/generate_gha_summary.py

      - name: Save Master Database
        if: always()
        uses: actions/cache/save@v4
        with:
          path: fortuna.db
          key: fortuna-db-v3-master-${{ github.run_number }}

      - name: Upload Final Reports
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: best-bet-reports-${{ github.run_number }}
          if-no-files-found: warn
          path: |
            summary_grid.txt
            goldmine_report.txt
            analytics_report.txt
            race_data.json
            fortuna.db
            workflow_metadata.json
          retention-days: 1
