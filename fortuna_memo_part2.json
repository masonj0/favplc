{
  "memo_type": "monolith_structure",
  "source_file": "fortuna.py",
  "part": 2,
  "total_parts": 2,
  "blocks": [
    {
      "type": "miscellaneous",
      "content": "\n\n"
    },
    {
      "type": "class",
      "content": "class TinyFieldTrifectaAnalyzer(TrifectaAnalyzer):\n    \"\"\"A specialized TrifectaAnalyzer that only considers races with 6 or fewer runners.\"\"\"\n\n    def __init__(self, **kwargs):\n        # Override the max_field_size to 6 for \"tiny field\" analysis\n        # Set low odds thresholds to \"let them through\" as per user request\n        super().__init__(max_field_size=6, min_favorite_odds=0.01, min_second_favorite_odds=0.01, **kwargs)\n\n    @property\n    def name(self) -> str:\n        return \"tiny_field_trifecta_analyzer\"\n",
      "name": "TinyFieldTrifectaAnalyzer"
    },
    {
      "type": "miscellaneous",
      "content": "\n\n"
    },
    {
      "type": "class",
      "content": "class SimplySuccessAnalyzer(BaseAnalyzer):\n    \"\"\"An analyzer that qualifies every race to show maximum successes (HTTP 200).\"\"\"\n\n    @property\n    def name(self) -> str:\n        return \"simply_success\"\n\n    def qualify_races(self, races: List[Race]) -> Dict[str, Any]:\n        \"\"\"Returns races with a perfect score, applying global timing and chalk filters.\"\"\"\n        qualified = []\n        now = datetime.now(EASTERN)\n\n        # Success Playbook Hardening (Council of Superbrains)\n        TRUSTWORTHY_RATIO_MIN = self.config.get(\"analysis\", {}).get(\"trustworthy_ratio_min\", 0.7)\n\n        for race in races:\n            # 1. Timing Filter: Relaxed for \"News\" mode (GPT5: Caller handles strict timing)\n            st = race.start_time\n            if st.tzinfo is None:\n                st = st.replace(tzinfo=EASTERN)\n\n            # Goldmine Detection: 2nd favorite >= 4.5 decimal\n            is_goldmine = False\n            is_best_bet = False\n            active_runners = [r for r in race.runners if not r.scratched]\n            total_active = len(active_runners)\n\n            # Trustworthiness Airlock (Success Playbook Item)\n            if total_active > 0:\n                trustworthy_count = sum(1 for r in active_runners if r.metadata.get(\"odds_source_trustworthy\"))\n                if trustworthy_count / total_active < TRUSTWORTHY_RATIO_MIN:\n                    self.logger.warning(\"Not enough trustworthy odds; skipping race\", venue=race.venue, race=race.race_number, ratio=round(trustworthy_count/total_active, 2))\n                    continue\n\n            gap12 = 0.0\n            all_odds = []\n\n            # 1. Collect and Enrich Odds\n            for runner in active_runners:\n                odds = _get_best_win_odds(runner)\n                if odds is not None:\n                    # Propagate fresh odds to runner object for reporting\n                    runner.win_odds = float(odds)\n                    all_odds.append(odds)\n\n            # Sort odds ascending\n            all_odds.sort()\n\n            # Uniform Odds Check: If all runners have identical odds, it's likely a placeholder card (Memory Directive Fix)\n            if len(all_odds) >= 3 and len(set(all_odds)) == 1:\n                self.logger.warning(\"Race contains uniform odds; likely placeholder data. Skipping.\", venue=race.venue, race=race.race_number, odds=float(all_odds[0]))\n                continue\n\n            # Stability Check: Ensure we have at least 2 active runners to compare\n            if len(active_runners) < 2:\n                log.debug(\"Excluding race with < 2 runners\", venue=race.venue)\n                continue\n\n            # 2. Derive Selection (2nd favorite) and Top 5\n            # Collect valid runners with their enriched odds\n            valid_r_with_odds = sorted(\n                [(r, Decimal(str(r.win_odds))) for r in active_runners if r.win_odds is not None],\n                key=lambda x: x[1]\n            )\n            race.top_five_numbers = \", \".join([str(r[0].number or '?') for r in valid_r_with_odds[:5]])\n\n            if len(valid_r_with_odds) >= 2:\n                sec_fav = valid_r_with_odds[1][0]\n                race.metadata['selection_number'] = sec_fav.number\n                race.metadata['selection_name'] = sec_fav.name\n\n            # 3. Apply Best Bet Logic\n            if len(all_odds) >= 2:\n                fav, sec = all_odds[0], all_odds[1]\n                gap12 = round(float(sec - fav), 2)\n\n                # Enforce gap requirement\n                if gap12 <= 0.25:\n                    log.debug(\"Insufficient gap detected (1Gap2 <= 0.25), ineligible for Best Bet treatment\", venue=race.venue, race=race.race_number, gap=gap12)\n                else:\n                    # Goldmine = 2nd Fav >= 4.5, Field <= 11, Gap > 0.25\n                    if len(active_runners) <= 11 and sec >= Decimal(\"4.5\"):\n                        is_goldmine = True\n\n                    # You Might Like = 2nd Fav >= 3.5, Field <= 11, Gap > 0.25\n                    if len(active_runners) <= 11 and sec >= Decimal(\"3.5\"):\n                        is_best_bet = True\n\n                race.metadata['predicted_2nd_fav_odds'] = float(sec)\n            else:\n                # Fallback if insufficient odds data\n                race.metadata['predicted_2nd_fav_odds'] = None\n\n            race.metadata['is_goldmine'] = is_goldmine\n            race.metadata['is_best_bet'] = is_best_bet\n            race.metadata['1Gap2'] = gap12\n            race.qualification_score = 100.0\n            qualified.append(race)\n\n        if not qualified:\n            log.warning(\"\ud83d\udd2d SimplySuccess analyzer pass returned 0 qualified races\", input_count=len(races))\n\n        return {\n            \"criteria\": {\n                \"mode\": \"simply_success\",\n                \"timing_filter\": \"45m_past_to_120m_future\",\n                \"chalk_filter\": \"disabled\",\n                \"goldmine_threshold\": 4.5\n            },\n            \"races\": qualified\n        }\n",
      "name": "SimplySuccessAnalyzer"
    },
    {
      "type": "miscellaneous",
      "content": "\n\n"
    },
    {
      "type": "class",
      "content": "class AnalyzerEngine:\n    \"\"\"Discovers and manages all available analyzer plugins.\"\"\"\n\n    def __init__(self, config: Optional[Dict[str, Any]] = None):\n        self.analyzers: Dict[str, Type[BaseAnalyzer]] = {}\n        self.config = config or {}\n        self._discover_analyzers()\n\n    def _discover_analyzers(self):\n        # In a real plugin system, this would inspect a folder.\n        # For now, we register them manually.\n        self.register_analyzer(\"trifecta\", TrifectaAnalyzer)\n        self.register_analyzer(\"tiny_field_trifecta\", TinyFieldTrifectaAnalyzer)\n        self.register_analyzer(\"simply_success\", SimplySuccessAnalyzer)\n        log.info(\n            \"AnalyzerEngine discovered plugins\",\n            available_analyzers=list(self.analyzers.keys()),\n        )\n\n    def register_analyzer(self, name: str, analyzer_class: Type[BaseAnalyzer]):\n        self.analyzers[name] = analyzer_class\n\n    def get_analyzer(self, name: str, **kwargs) -> BaseAnalyzer:\n        analyzer_class = self.analyzers.get(name)\n        if not analyzer_class:\n            log.error(\"Requested analyzer not found\", requested_analyzer=name)\n            raise ValueError(f\"Analyzer '{name}' not found.\")\n        return analyzer_class(config=self.config, **kwargs)\n",
      "name": "AnalyzerEngine"
    },
    {
      "type": "miscellaneous",
      "content": "\n\n"
    },
    {
      "type": "class",
      "content": "class AudioAlertSystem:\n    \"\"\"Plays sound alerts for important events.\"\"\"\n\n    def __init__(self):\n        self.sounds = {\n            \"high_value\": Path(__file__).resolve().parent / \"assets\" / \"sounds\" / \"alert_premium.wav\",\n        }\n        self.enabled = winsound is not None\n\n    def play(self, sound_type: str):\n        if not self.enabled:\n            return\n\n        sound_file = self.sounds.get(sound_type)\n        if sound_file and sound_file.exists():\n            try:\n                winsound.PlaySound(str(sound_file), winsound.SND_FILENAME | winsound.SND_ASYNC)\n            except Exception as e:\n                log.warning(\"Could not play sound\", file=sound_file, error=e)\n",
      "name": "AudioAlertSystem"
    },
    {
      "type": "miscellaneous",
      "content": "\n\n"
    },
    {
      "type": "class",
      "content": "class RaceNotifier:\n    \"\"\"Handles sending native notifications and audio alerts for high-value races.\"\"\"\n\n    def __init__(self):\n        self.notifier = DesktopNotifier() if HAS_NOTIFICATIONS else None\n        self.audio_system = AudioAlertSystem()\n        self.notified_races = set()\n        self.notifications_enabled = self.notifier is not None\n        if not self.notifications_enabled:\n            log.debug(\"Native notifications disabled (platform not supported or library missing)\")\n\n    def notify_qualified_race(self, race):\n        if race.id in self.notified_races:\n            return\n\n        # Always log the high-value opportunity regardless of notification setting\n        log.info(\n            \"High-value opportunity identified\",\n            venue=race.venue,\n            race=race.race_number,\n            score=race.qualification_score\n        )\n\n        if not self.notifications_enabled or self.notifier is None:\n            return\n\n        title = \"\ud83d\udc0e High-Value Opportunity!\"\n        message = f\"{race.venue} - Race {race.race_number}\\nScore: {race.qualification_score:.0f}%\\nPost Time: {race.start_time.strftime('%I:%M %p')}\"\n\n        try:\n            # Use keyword arguments for better compatibility (AI Review Fix)\n            self.notifier.send(\n                title=title,\n                message=message,\n                urgency=\"high\" if race.qualification_score >= 80 else \"normal\"\n            )\n            self.notified_races.add(race.id)\n            self.audio_system.play(\"high_value\")\n            log.info(\"Notification and audio alert sent for high-value race\", race_id=race.id)\n        except Exception as e:\n            log.error(\"Failed to send notification\", error=str(e))\n",
      "name": "RaceNotifier"
    },
    {
      "type": "miscellaneous",
      "content": "\n\n# ----------------------------------------\n"
    },
    {
      "type": "function",
      "content": "def get_track_category(races_at_track: List[Any]) -> str:\n    \"\"\"Categorize the track as T (Thoroughbred), H (Harness), or G (Greyhounds).\"\"\"\n    if not races_at_track:\n        return 'T'\n\n    # Never allow any track with a field size above 7 to be G\n    has_large_field = False\n    for r in races_at_track:\n        runners = get_field(r, 'runners', [])\n        active_runners = len([run for run in runners if not get_field(run, 'scratched', False)])\n        if active_runners > 7:\n            has_large_field = True\n            break\n\n    for race in races_at_track:\n        source = get_field(race, 'source', '') or \"\"\n        race_id = (get_field(race, 'id', '') or \"\").lower()\n        discipline = get_field(race, 'discipline', '') or \"\"\n\n        if discipline == \"Harness\" or '_h' in race_id: return 'H'\n        if (discipline == \"Greyhound\" or '_g' in race_id) and not has_large_field:\n            return 'G'\n\n        source_lower = source.lower()\n        if (\"greyhound\" in source_lower or source in [\"GBGB\", \"Greyhound\", \"AtTheRacesGreyhound\"]) and not has_large_field:\n            return 'G'\n        if source in [\"USTrotting\", \"StandardbredCanada\", \"Harness\"] or any(kw in source_lower for kw in ['harness', 'standardbred', 'trot', 'pace']):\n            return 'H'\n\n    # Distance consistency check (Disabled - was mis-identifying Thoroughbred tracks)\n    # dist_counts = defaultdict(int)\n    # for r in races_at_track:\n    #     dist = get_field(r, 'distance')\n    #     if dist:\n    #         dist_counts[dist] += 1\n    # if dist_counts and max(dist_counts.values()) >= 4:\n    #     return 'H'\n\n    return 'T'\n",
      "name": "get_track_category"
    },
    {
      "type": "miscellaneous",
      "content": "\n\n"
    },
    {
      "type": "function",
      "content": "def generate_fortuna_fives(races: List[Any], all_races: Optional[List[Any]] = None) -> str:\n    \"\"\"Generate the FORTUNA FIVES appendix.\"\"\"\n    lines = [\"\", \"\", \"FORTUNA FIVES\", \"-------------\"]\n    fives = []\n    for race in (all_races or races):\n        runners = get_field(race, 'runners', [])\n        field_size = len([r for r in runners if not get_field(r, 'scratched', False)])\n        if field_size == 5:\n            fives.append(race)\n\n    if not fives:\n        lines.append(\"No qualifying races.\")\n        return \"\\n\".join(lines)\n\n    track_odds_sums = defaultdict(float)\n    track_odds_counts = defaultdict(int)\n    stats_races = all_races if all_races is not None else races\n    for race in stats_races:\n        v = get_field(race, 'venue')\n        track = normalize_venue_name(v)\n        for runner in get_field(race, 'runners', []):\n            win_odds = get_field(runner, 'win_odds')\n            if not get_field(runner, 'scratched') and win_odds:\n                track_odds_sums[track] += float(win_odds)\n                track_odds_counts[track] += 1\n\n    track_avgs = {}\n    for track, total in track_odds_sums.items():\n        count = track_odds_counts[track]\n        if count > 0:\n            track_avgs[track] = str(int(total / count))\n\n    track_to_nums = defaultdict(list)\n    for r in fives:\n        v = get_field(r, 'venue')\n        if v:\n            track_to_nums[normalize_venue_name(v)].append(get_field(r, 'race_number'))\n\n    for track in sorted(track_to_nums.keys()):\n        nums = sorted(list(set(track_to_nums[track])))\n        avg_str = f\" [{track_avgs[track]}]\" if track in track_avgs else \"\"\n        lines.append(f\"{track}{avg_str}: {', '.join(map(str, nums))}\")\n\n    return \"\\n\".join(lines)\n",
      "name": "generate_fortuna_fives"
    },
    {
      "type": "miscellaneous",
      "content": "\n\n"
    },
    {
      "type": "function",
      "content": "def generate_field_matrix(races: List[Any]) -> str:\n    \"\"\"\n    Generates a Markdown table matrix of races by Track and Field Size.\n    Cells contain alphabetic race codes (lowercase=normal, uppercase=goldmine).\n    \"\"\"\n    if not races:\n        return \"No races available for field matrix.\"\n\n    # Group races by Track and Field Size\n    matrix = defaultdict(lambda: defaultdict(list))\n\n    for r in races:\n        track = normalize_venue_name(get_field(r, 'venue'))\n        field_size = len([run for run in get_field(r, 'runners', []) if not get_field(run, 'scratched', False)])\n\n        # Only interested in field sizes 3-11 for this report\n        if 3 <= field_size <= 11:\n            is_gold = get_field(r, 'metadata', {}).get('is_goldmine', False)\n            race_num = get_field(r, 'race_number')\n            matrix[track][field_size].append((race_num, is_gold))\n\n    if not matrix:\n        return \"No qualifying races for field matrix (3-11 runners).\"\n\n    # Header: Display sizes 3 to 11\n    display_sizes = range(3, 12)\n\n    header = \"| TRACK / FIELD | \" + \" | \".join(map(str, display_sizes)) + \" |\"\n    separator = \"| :--- | \" + \" | \".join([\":---:\"] * len(display_sizes)) + \" |\"\n    lines = [header, separator]\n\n    for track in sorted(matrix.keys()):\n        row = [track]\n        for size in display_sizes:\n            race_list = matrix[track].get(size, [])\n            if race_list:\n                # Standardize formatting of race codes\n                code_parts = format_grid_code(race_list, wrap_width=12)\n                row.append(\"<br>\".join(code_parts))\n            else:\n                row.append(\" \")\n        lines.append(\"| \" + \" | \".join(row) + \" |\")\n\n    return \"\\n\".join(lines)\n",
      "name": "generate_field_matrix"
    },
    {
      "type": "miscellaneous",
      "content": "\n\n"
    },
    {
      "type": "function",
      "content": "def generate_goldmines(races: List[Any], all_races: Optional[List[Any]] = None) -> str:\n    \"\"\"Generate the GOLDMINE RACES appendix, filtered to Superfecta races.\"\"\"\n    lines = [\"\", \"\", \"GOLDMINE RACES\", \"--------------\"]\n\n    # Pre-calculate track categories\n    track_categories = {}\n    source_races_for_cat = all_races if all_races is not None else races\n    races_by_track = defaultdict(list)\n    for r in source_races_for_cat:\n        v = get_field(r, 'venue')\n        track = normalize_venue_name(v)\n        races_by_track[track].append(r)\n    for track, tr_races in races_by_track.items():\n        track_categories[track] = get_track_category(tr_races)\n\n    def is_superfecta_effective(r):\n        available_bets = get_field(r, 'available_bets', [])\n        metadata_bets = get_field(r, 'metadata', {}).get('available_bets', [])\n        if 'Superfecta' in available_bets or 'Superfecta' in metadata_bets:\n            return True\n\n        track = normalize_venue_name(get_field(r, 'venue'))\n        cat = track_categories.get(track, 'T')\n        runners = get_field(r, 'runners', [])\n        field_size = len([run for run in runners if not get_field(run, 'scratched', False)])\n        if cat == 'T' and field_size >= 6:\n            return True\n        return False\n\n    goldmines = [r for r in races if get_field(r, 'metadata', {}).get('is_goldmine') and is_superfecta_effective(r)]\n\n    if not goldmines:\n        lines.append(\"No qualifying races.\")\n        return \"\\n\".join(lines)\n\n    track_to_nums = defaultdict(list)\n    for r in goldmines:\n        v = get_field(r, 'venue')\n        if v:\n            track = normalize_venue_name(v)\n            track_to_nums[track].append(get_field(r, 'race_number'))\n\n    # Sort tracks descending by category (T > H > G)\n    cat_map = {'T': 3, 'H': 2, 'G': 1}\n\n    formatted_tracks = []\n    for track in track_to_nums.keys():\n        cat = track_categories.get(track, 'T')\n        display_name = f\"{cat}~{track}\"\n        formatted_tracks.append((cat, track, display_name))\n\n    # Sort: Category Descending, then Track Name Ascending\n    formatted_tracks.sort(key=lambda x: (-cat_map.get(x[0], 0), x[1]))\n\n    for cat, track, display_name in formatted_tracks:\n        nums = sorted(list(set(track_to_nums[track])))\n        lines.append(f\"{display_name}: {', '.join(map(str, nums))}\")\n    return \"\\n\".join(lines)\n",
      "name": "generate_goldmines"
    },
    {
      "type": "miscellaneous",
      "content": "\n\n"
    },
    {
      "type": "function",
      "content": "def generate_goldmine_report(races: List[Any], all_races: Optional[List[Any]] = None) -> str:\n    \"\"\"Generate a detailed report for Goldmine races.\"\"\"\n    # 1. Reuse category logic\n    track_categories = {}\n    source_races_for_cat = all_races if all_races is not None else races\n    races_by_track = defaultdict(list)\n    for r in source_races_for_cat:\n        v = get_field(r, 'venue')\n        track = normalize_venue_name(v)\n        races_by_track[track].append(r)\n    for track, tr_races in races_by_track.items():\n        track_categories[track] = get_track_category(tr_races)\n\n    def is_superfecta_available(r):\n        available_bets = get_field(r, 'available_bets', [])\n        metadata_bets = get_field(r, 'metadata', {}).get('available_bets', [])\n        if 'Superfecta' in available_bets or 'Superfecta' in metadata_bets:\n            return True\n        track = normalize_venue_name(get_field(r, 'venue'))\n        cat = track_categories.get(track, 'T')\n        runners = get_field(r, 'runners', [])\n        field_size = len([run for run in runners if not get_field(run, 'scratched', False)])\n        return cat == 'T' and field_size >= 6\n\n    # Include all goldmines (2nd fav >= 4.5)\n    # Deduplicate to prevent double-reporting (e.g. from multiple sources)\n    goldmines = []\n    seen_gold = set()\n    for r in races:\n        if get_field(r, 'metadata', {}).get('is_goldmine'):\n            track = get_canonical_venue(get_field(r, 'venue'))\n            num = get_field(r, 'race_number')\n            st = get_field(r, 'start_time')\n            st_str = st.strftime('%Y%m%d') if isinstance(st, datetime) else str(st)\n            # Use canonical key for cross-adapter deduplication\n            key = (track, num, st_str)\n            if key not in seen_gold:\n                seen_gold.add(key)\n                goldmines.append(r)\n\n    if not goldmines:\n        return \"No Goldmine races found.\"\n\n    # Sort goldmines: Cat descending, Track asc, Race num asc\n    cat_map = {'T': 3, 'H': 2, 'G': 1}\n    def goldmine_sort_key(r):\n        track = normalize_venue_name(get_field(r, 'venue'))\n        cat = track_categories.get(track, 'T')\n        return (-cat_map.get(cat, 0), track, get_field(r, 'race_number', 0))\n\n    goldmines.sort(key=goldmine_sort_key)\n\n    now = datetime.now(EASTERN)\n    immediate_gold_superfecta = []\n    immediate_gold = []\n    remaining_gold = []\n\n    for r in goldmines:\n        start_time = get_field(r, 'start_time')\n        if isinstance(start_time, str):\n            try:\n                start_time = datetime.fromisoformat(start_time.replace('Z', '+00:00'))\n            except ValueError:\n                remaining_gold.append(r)\n                continue\n\n        if start_time:\n            if start_time.tzinfo is None:\n                start_time = start_time.replace(tzinfo=EASTERN)\n\n            diff = (start_time - now).total_seconds() / 60\n            if 0 <= diff <= 20:\n                if is_superfecta_available(r):\n                    immediate_gold_superfecta.append(r)\n                else:\n                    immediate_gold.append(r)\n            else:\n                remaining_gold.append(r)\n        else:\n            remaining_gold.append(r)\n\n    report_lines = [\"LIST OF BEST BETS - GOLDMINE REPORT\", \"===================================\", \"\"]\n\n    def render_races(races_to_render, label):\n        if not races_to_render:\n            return\n        report_lines.append(f\"--- {label.upper()} ---\")\n        report_lines.append(\"-\" * (len(label) + 8))\n        report_lines.append(\"\")\n\n        for r in races_to_render:\n            track = normalize_venue_name(get_field(r, 'venue'))\n            cat = track_categories.get(track, 'T')\n            race_num = get_field(r, 'race_number')\n            start_time = get_field(r, 'start_time')\n            if isinstance(start_time, datetime):\n                # Ensure it's in Eastern for the display\n                st_eastern = to_eastern(start_time)\n                time_str = st_eastern.strftime(\"%H:%M ET\")\n            else:\n                time_str = str(start_time)\n\n            # Identify Top 5\n            runners = get_field(r, 'runners', [])\n            active_with_odds = []\n            for run in runners:\n                if get_field(run, 'scratched'): continue\n                wo = _get_best_win_odds(run)\n                if wo: active_with_odds.append((run, wo))\n\n            sorted_by_odds = sorted(active_with_odds, key=lambda x: x[1])\n            top_5_nums = \", \".join([str(get_field(run[0], 'number') or '?') for run in sorted_by_odds[:5]])\n            if hasattr(r, 'top_five_numbers'):\n                r.top_five_numbers = top_5_nums\n\n            gap12 = get_field(r, 'metadata', {}).get('1Gap2', 0.0)\n            report_lines.append(f\"{cat}~{track} - Race {race_num} ({time_str})\")\n            report_lines.append(f\"PREDICTED TOP 5: [{top_5_nums}] | 1Gap2: {gap12:.2f}\")\n            report_lines.append(\"-\" * 40)\n\n            # Sort runners by number\n            sorted_runners = sorted(runners, key=lambda x: get_field(x, 'number') or 0)\n\n            for run in sorted_runners:\n                if get_field(run, 'scratched'):\n                    continue\n                name = get_field(run, 'name')\n                num = get_field(run, 'number')\n                odds = get_field(run, 'win_odds')\n                odds_str = f\"{odds:.2f}\" if odds else \"N/A\"\n                report_lines.append(f\"  #{num:<2} {name:<25}  ~ {odds_str}\")\n\n            report_lines.append(\"\")\n\n    if immediate_gold_superfecta:\n        render_races(immediate_gold_superfecta, \"Immediate Gold (superfecta)\")\n\n    if immediate_gold:\n        render_races(immediate_gold, \"Immediate Gold\")\n\n    if remaining_gold:\n        render_races(remaining_gold, \"All Remaining Goldmine Races\")\n\n    return \"\\n\".join(report_lines)\n",
      "name": "generate_goldmine_report"
    },
    {
      "type": "miscellaneous",
      "content": "\n\n"
    },
    {
      "type": "function",
      "content": "def generate_historical_goldmine_report(audited_tips: List[Dict[str, Any]]) -> str:\n    \"\"\"Generate a report for recently audited Goldmine races.\"\"\"\n    if not audited_tips:\n        return \"\"\n\n    lines = [\"\", \"RECENT AUDITED GOLDMINES\", \"------------------------\"]\n\n    # Calculate simple stats\n    total = len(audited_tips)\n    cashed = sum(1 for t in audited_tips if t.get(\"verdict\") == \"CASHED\")\n    total_profit = sum((t.get(\"net_profit\") or 0.0) for t in audited_tips)\n    sr = (cashed / total * 100) if total > 0 else 0\n\n    lines.append(f\"Performance Summary (Last {total} Goldmines):\")\n    lines.append(f\"  Strike Rate: {sr:.1f}% | Total Net Profit: ${total_profit:+.2f}\")\n    lines.append(\"\")\n\n    for tip in audited_tips:\n        venue = tip.get(\"venue\", \"Unknown\")\n        race_num = tip.get(\"race_number\", \"?\")\n        verdict = tip.get(\"verdict\", \"?\")\n        profit = tip.get(\"net_profit\", 0.0)\n        start_time_raw = tip.get(\"start_time\", \"\")\n\n        try:\n            st = datetime.fromisoformat(start_time_raw.replace('Z', '+00:00'))\n            time_str = to_eastern(st).strftime(\"%Y-%m-%d %H:%M ET\")\n        except Exception:\n            time_str = str(start_time_raw)[:16]\n\n        emoji = \"\u2705\" if verdict == \"CASHED\" else \"\u274c\" if verdict == \"BURNED\" else \"\u26aa\"\n\n        line = f\"{emoji} {time_str} | {venue} R{race_num} | {verdict:<6} | Profit: ${profit:+.2f}\"\n\n        # Add top place payouts for proof\n        p1 = tip.get(\"top1_place_payout\")\n        p2 = tip.get(\"top2_place_payout\")\n        if p1 or p2:\n            line += f\" | Place: {p1 or 0:.2f}/{p2 or 0:.2f}\"\n\n        # Prioritize Superfecta info to \"prove\" with payouts\n        super_payout = tip.get(\"superfecta_payout\")\n        tri_payout = tip.get(\"trifecta_payout\")\n\n        if super_payout:\n            line += f\" | Super: ${super_payout:.2f}\"\n        elif tri_payout:\n            line += f\" | Tri: ${tri_payout:.2f}\"\n\n        lines.append(line)\n\n    return \"\\n\".join(lines)\n",
      "name": "generate_historical_goldmine_report"
    },
    {
      "type": "miscellaneous",
      "content": "\n\n"
    },
    {
      "type": "function",
      "content": "def generate_next_to_jump(races: List[Any]) -> str:\n    \"\"\"Generate the NEXT TO JUMP section.\"\"\"\n    lines = [\"\", \"\", \"NEXT TO JUMP\", \"------------\"]\n    now = datetime.now(EASTERN)\n    upcoming = []\n    for r in races:\n        r_time = get_field(r, 'start_time')\n        if isinstance(r_time, str):\n            try:\n                r_time = datetime.fromisoformat(r_time.replace('Z', '+00:00'))\n            except ValueError:\n                continue\n\n        if r_time:\n            if r_time.tzinfo is None:\n                r_time = r_time.replace(tzinfo=EASTERN)\n            if r_time > now:\n                upcoming.append((r, r_time))\n\n    if upcoming:\n        next_r, next_r_time = min(upcoming, key=lambda x: x[1])\n        diff = next_r_time - now\n        minutes = int(diff.total_seconds() / 60)\n        lines.append(f\"{normalize_venue_name(get_field(next_r, 'venue'))} Race {get_field(next_r, 'race_number')} in {minutes}m\")\n    else:\n        lines.append(\"All races complete for today.\")\n\n    return \"\\n\".join(lines)\n",
      "name": "generate_next_to_jump"
    },
    {
      "type": "miscellaneous",
      "content": "\n\n"
    },
    {
      "type": "async_function",
      "content": "async def generate_friendly_html_report(races: List[Any], stats: Dict[str, Any]) -> str:\n    \"\"\"Generates a high-impact, friendly HTML report for the Fortuna Faucet.\"\"\"\n    now_str = datetime.now(EASTERN).strftime('%Y-%m-%d %H:%M:%S')\n\n    # 1. Best Bet Opportunities\n    rows = []\n    for r in sorted(races, key=lambda x: getattr(x, 'start_time', '')):\n        # Get selection (2nd favorite)\n        runners = getattr(r, 'runners', [])\n        active = [run for run in runners if not getattr(run, 'scratched', False)]\n        if len(active) < 2: continue\n\n        active.sort(key=lambda x: getattr(x, 'win_odds', 999.0) or 999.0)\n        sel = active[1]\n\n        st = getattr(r, 'start_time', '')\n        if isinstance(st, datetime):\n            # Ensure it's in Eastern for display (GPT5 Improvement)\n            st_str = to_eastern(st).strftime('%H:%M')\n        elif isinstance(st, str):\n            try:\n                dt = datetime.fromisoformat(st.replace('Z', '+00:00'))\n                st_str = to_eastern(dt).strftime('%H:%M')\n            except Exception:\n                st_str = str(st)[11:16]\n        else:\n            st_str = str(st)[11:16]\n\n        is_gold = getattr(r, 'metadata', {}).get('is_goldmine', False)\n        gold_badge = '<span class=\"badge gold\">GOLD</span>' if is_gold else ''\n\n        d_str = '??/??'\n        if isinstance(st, datetime):\n            d_str = st.strftime('%m/%d')\n        elif isinstance(st, str):\n            try:\n                dt = datetime.fromisoformat(st.replace('Z', '+00:00'))\n                d_str = dt.strftime('%m/%d')\n            except Exception: pass\n\n        rows.append(f\"\"\"\n            <tr>\n                <td>{st_str} ({d_str})</td>\n                <td>{getattr(r, 'venue', 'Unknown')}</td>\n                <td>R{getattr(r, 'race_number', '?')}</td>\n                <td>#{getattr(sel, 'number', '?')} {getattr(sel, 'name', 'Unknown')}</td>\n                <td>{ (getattr(sel, 'win_odds') or 0.0):.2f}</td>\n                <td>{gold_badge}</td>\n            </tr>\n        \"\"\")\n\n    tips_count = stats.get('tips', 0)\n    cashed_count = stats.get('cashed', 0)\n    profit = stats.get('profit', 0.0)\n\n    html = f\"\"\"\n    <!DOCTYPE html>\n    <html lang=\"en\">\n    <head>\n        <meta charset=\"UTF-8\">\n        <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n        <title>Fortuna Faucet Intelligence Report</title>\n        <style>\n            body {{ font-family: 'Segoe UI', Arial, sans-serif; background-color: #0f172a; color: #f8fafc; margin: 0; padding: 20px; }}\n            .container {{ max-width: 1000px; margin: 0 auto; background-color: #1e293b; padding: 30px; border-radius: 12px; box-shadow: 0 10px 25px rgba(0,0,0,0.5); }}\n            h1 {{ color: #fbbf24; text-align: center; text-transform: uppercase; letter-spacing: 3px; border-bottom: 2px solid #fbbf24; padding-bottom: 15px; }}\n            .stats-grid {{ display: grid; grid-template-columns: repeat(3, 1fr); gap: 20px; margin: 30px 0; }}\n            .stat-card {{ background-color: #334155; padding: 20px; border-radius: 8px; text-align: center; }}\n            .stat-value {{ font-size: 24px; font-weight: bold; color: #fbbf24; }}\n            .stat-label {{ font-size: 14px; color: #94a3b8; text-transform: uppercase; }}\n            table {{ width: 100%; border-collapse: collapse; margin-top: 20px; }}\n            th {{ background-color: #334155; color: #fbbf24; text-align: left; padding: 12px; }}\n            td {{ padding: 12px; border-bottom: 1px solid #334155; }}\n            tr:hover {{ background-color: #334155; }}\n            .badge {{ padding: 4px 8px; border-radius: 4px; font-size: 12px; font-weight: bold; }}\n            .gold {{ background-color: #fbbf24; color: #0f172a; }}\n            .footer {{ margin-top: 40px; text-align: center; font-size: 12px; color: #64748b; }}\n        </style>\n    </head>\n    <body>\n        <div class=\"container\">\n            <h1>Fortuna Faucet Intelligence</h1>\n            <p style=\"text-align:center;\">Real-time global racing analysis generated at {now_str} ET</p>\n\n            <div class=\"stats-grid\">\n                <div class=\"stat-card\">\n                    <div class=\"stat-value\">{tips_count}</div>\n                    <div class=\"stat-label\">Total Selections</div>\n                </div>\n                <div class=\"stat-card\">\n                    <div class=\"stat-value\">{cashed_count}</div>\n                    <div class=\"stat-label\">Recently Audited Wins</div>\n                </div>\n                <div class=\"stat-card\">\n                    <div class=\"stat-value\">${profit:+.2f}</div>\n                    <div class=\"stat-label\">Estimated Profit</div>\n                </div>\n            </div>\n\n            <h2>\ud83d\udd25 Best Bet Opportunities</h2>\n            <table>\n                <thead>\n                    <tr>\n                        <th>Time</th>\n                        <th>Venue</th>\n                        <th>Race</th>\n                        <th>Selection</th>\n                        <th>Odds</th>\n                        <th>Type</th>\n                    </tr>\n                </thead>\n                <tbody>\n                    {''.join(rows) if rows else '<tr><td colspan=\"6\" style=\"text-align:center;\">No immediate opportunities identified.</td></tr>'}\n                </tbody>\n            </table>\n\n            {await _generate_audit_history_html()}\n\n            <div class=\"footer\">\n                Fortuna Faucet Portable App - Sci-Fi Intelligence Edition<br>\n                Powered by the Council of Superbrains\n            </div>\n        </div>\n    </body>\n    </html>\n    \"\"\"\n    return html\n",
      "name": "generate_friendly_html_report"
    },
    {
      "type": "miscellaneous",
      "content": "\n\n"
    },
    {
      "type": "async_function",
      "content": "async def _generate_audit_history_html() -> str:\n    \"\"\"Generates HTML for recent audited results.\"\"\"\n    db = FortunaDB()\n    history = await db.get_all_audited_tips()\n    if not history:\n        return \"\"\n\n    # Take latest 15\n    history = sorted(history, key=lambda x: x.get('audit_timestamp', ''), reverse=True)[:15]\n\n    rows = []\n    for t in history:\n        verdict = t.get(\"verdict\", \"?\")\n        emoji = \"\u2705\" if verdict == \"CASHED\" else \"\u274c\" if verdict == \"BURNED\" else \"\u26aa\"\n        profit = t.get(\"net_profit\", 0.0)\n        p_class = \"profit-pos\" if profit > 0 else \"profit-neg\" if profit < 0 else \"\"\n\n        po = t.get(\"predicted_2nd_fav_odds\")\n        ao = t.get(\"actual_2nd_fav_odds\")\n        odds_str = f\"{po or '?':.1f} \u2192 {ao or '?':.1f}\"\n\n        rows.append(f\"\"\"\n            <tr>\n                <td>{emoji} {verdict}</td>\n                <td>{t.get('venue', 'Unknown')}</td>\n                <td>R{t.get('race_number', '?')}</td>\n                <td>{odds_str}</td>\n                <td class=\"{p_class}\">${profit:+.2f}</td>\n            </tr>\n        \"\"\")\n\n    return f\"\"\"\n        <style>\n            .profit-pos {{ color: #4ade80; font-weight: bold; }}\n            .profit-neg {{ color: #f87171; }}\n        </style>\n        <h2 style=\"margin-top: 40px;\">\ud83d\udcb0 Recent Audit Results</h2>\n        <table>\n            <thead>\n                <tr>\n                    <th>Verdict</th>\n                    <th>Venue</th>\n                    <th>Race</th>\n                    <th>Odds (Pred \u2192 Act)</th>\n                    <th>Net Profit</th>\n                </tr>\n            </thead>\n            <tbody>\n                {''.join(rows)}\n            </tbody>\n        </table>\n    \"\"\"\n",
      "name": "_generate_audit_history_html"
    },
    {
      "type": "miscellaneous",
      "content": "\n\n"
    },
    {
      "type": "function",
      "content": "def generate_summary_grid(races: List[Any], all_races: Optional[List[Any]] = None) -> str:\n    \"\"\"\n    Generates a Markdown table summary of upcoming races.\n    Sorted by MTP, ceiling of 4 hours from now.\n    \"\"\"\n    now = datetime.now(EASTERN)\n    cutoff = now + timedelta(hours=18)\n\n    # 1. Pre-calculate track categories\n    track_categories = {}\n    source_races = all_races if all_races is not None else races\n    races_by_track = defaultdict(list)\n    for r in source_races:\n        venue = get_field(r, 'venue')\n        track = normalize_venue_name(venue)\n        races_by_track[track].append(r)\n\n    for track, tr_races in races_by_track.items():\n        track_categories[track] = get_track_category(tr_races)\n\n    table_races = []\n    seen = set()\n    for race in (all_races or races):\n        st = get_field(race, 'start_time')\n        if isinstance(st, str):\n            try: st = datetime.fromisoformat(st.replace('Z', '+00:00'))\n            except Exception: continue\n        if st and st.tzinfo is None: st = st.replace(tzinfo=EASTERN)\n\n        # Ceiling of 18 hours, ignore races more than 10 mins past\n        if not st or st < now - timedelta(minutes=10) or st > cutoff:\n            continue\n\n        track = normalize_venue_name(get_field(race, 'venue'))\n        canonical_track = get_canonical_venue(get_field(race, 'venue'))\n        num = get_field(race, 'race_number')\n        # Deduplication key: Use canonical track/num/date\n        key = (canonical_track, num, st.strftime('%Y%m%d'))\n        if key in seen: continue\n        seen.add(key)\n\n        mtp = int((st - now).total_seconds() / 60)\n        runners = get_field(race, 'runners', [])\n        field_size = len([run for run in runners if not get_field(run, 'scratched', False)])\n        top5 = getattr(race, 'top_five_numbers', 'N/A')\n        gap12 = get_field(race, 'metadata', {}).get('1Gap2', 0.0)\n        is_gold = get_field(race, 'metadata', {}).get('is_goldmine', False)\n\n        table_races.append({\n            'mtp': mtp,\n            'cat': track_categories.get(track, 'T'),\n            'track': track,\n            'num': num,\n            'field': field_size,\n            'top5': top5,\n            'gap': gap12,\n            'gold': '[G]' if is_gold else ''\n        })\n\n    # Sort by MTP\n    table_races.sort(key=lambda x: x['mtp'])\n\n    if not table_races:\n        return \"No upcoming races in the next 4 hours.\"\n\n    lines = [\n        \"| MTP | CAT | TRACK | R# | FLD | TOP 5 | GAP | |\",\n        \"|:---:|:---:|:---|:---:|:---:|:---|:---:|:---:|\"\n    ]\n    for tr in table_races:\n        # Better alignment: leading zero for single digits (Memory Directive Fix)\n        mtp_val = tr['mtp']\n        mtp_str = f\"{mtp_val:02d}\" if 0 <= mtp_val < 10 else str(mtp_val)\n        lines.append(f\"| {mtp_str}m | {tr['cat']} | {tr['track'][:20]} | {tr['num']} | {tr['field']} | `{tr['top5']}` | {tr['gap']:.2f} | {tr['gold']} |\")\n\n    return \"\\n\".join(lines)\n",
      "name": "generate_summary_grid"
    },
    {
      "type": "miscellaneous",
      "content": "\n\n"
    },
    {
      "type": "function",
      "content": "def normalize_course_name(name: str) -> str:\n    if not name:\n        return \"\"\n    name = name.lower().strip()\n    name = re.sub(r\"[^a-z0-9\\s-]\", \"\", name)\n    name = re.sub(r\"[\\s-]+\", \"_\", name)\n    return name\n",
      "name": "normalize_course_name"
    },
    {
      "type": "miscellaneous",
      "content": "\n\n"
    },
    {
      "type": "function",
      "content": "def num_to_alpha(n, is_goldmine=False):\n    \"\"\"Convert race number to alphabetic code. Goldmines are uppercase.\"\"\"\n    if not isinstance(n, int) or n < 1:\n        return '?'\n    letter = chr(ord('a') + n - 1) if n <= 26 else str(n)\n    return letter.upper() if is_goldmine else letter\n",
      "name": "num_to_alpha"
    },
    {
      "type": "miscellaneous",
      "content": "\n\n"
    },
    {
      "type": "function",
      "content": "def wrap_text(text, width):\n    \"\"\"Wrap string into a list of fixed-width segments.\"\"\"\n    if not text:\n        return [\"\"]\n    return [text[i:i+width] for i in range(0, len(text), width)]\n",
      "name": "wrap_text"
    },
    {
      "type": "miscellaneous",
      "content": "\n\n"
    },
    {
      "type": "function",
      "content": "def format_grid_code(race_info_list, wrap_width=4):\n    \"\"\"\n    Standardizes the formatting of race code strings for the grid.\n    Includes midpoint space for readability if length exceeds 5.\n\n    Args:\n        race_info_list: List of (race_num, is_goldmine) tuples\n        wrap_width: Width to wrap at\n    \"\"\"\n    if not race_info_list:\n        return [\"\"]\n\n    code = \"\".join([num_to_alpha(n, gm) for n, gm in sorted(list(set(race_info_list)))])\n\n    # Midpoint space logic for readability (Project Convention)\n    if len(code) > 5:\n        mid = len(code) // 2\n        code = code[:mid] + \" \" + code[mid:]\n\n    return wrap_text(code, wrap_width)\n",
      "name": "format_grid_code"
    },
    {
      "type": "miscellaneous",
      "content": "\n\n"
    },
    {
      "type": "function",
      "content": "def format_prediction_row(race: Race) -> str:\n    \"\"\"Formats a single race prediction for the GHA Job Summary table.\"\"\"\n    metadata = getattr(race, 'metadata', {})\n\n    st = race.start_time\n    if isinstance(st, str):\n        try: st = datetime.fromisoformat(st.replace('Z', '+00:00'))\n        except Exception: st = None\n    date_str = st.strftime('%m/%d') if st else '??/??'\n\n    gold = '\u2705' if metadata.get('is_goldmine') else '\u2014'\n    selection = metadata.get('selection_name') or f\"#{metadata.get('selection_number', '?')}\"\n    odds = metadata.get('predicted_2nd_fav_odds')\n    odds_str = f\"{odds:.2f}\" if odds else 'N/A'\n    top5 = getattr(race, 'top_five_numbers', 'TBD')\n    gap = metadata.get('1Gap2', 0.0)\n    gap_str = f\"{gap:.2f}\"\n\n    payouts = []\n    # Check both metadata and attributes for payouts\n    for label in ('top1_place_payout', 'trifecta_payout', 'superfecta_payout'):\n        val = metadata.get(label) or getattr(race, label, None)\n        if val:\n            display_label = label.replace('_', ' ').title().replace('Top1 ', '')\n            payouts.append(f\"{display_label}: ${float(val):.2f}\")\n\n    payout_text = ' | '.join(payouts) or 'Awaiting Results'\n    return f\"| {date_str} | {race.venue} | {race.race_number} | {selection} | {odds_str} | {gap_str} | {gold} | {top5} | {payout_text} |\"\n",
      "name": "format_prediction_row"
    },
    {
      "type": "miscellaneous",
      "content": "\n\n"
    },
    {
      "type": "function",
      "content": "def format_predictions_section(qualified_races: List[Race]) -> str:\n    \"\"\"Generates the Predictions & Proof section for the GHA Job Summary.\"\"\"\n    lines = [\"### \ud83d\udd2e Fortuna Predictions & Proof\", \"\"]\n    if not qualified_races:\n        lines.append(\"No Goldmine predictions available for this run.\")\n        return \"\\n\".join(lines)\n\n    now = datetime.now(EASTERN)\n\n    def get_mtp(r):\n        st = r.start_time\n        if isinstance(st, str):\n            try:\n                st = datetime.fromisoformat(st.replace('Z', '+00:00'))\n            except Exception:\n                return 9999\n        if st and st.tzinfo is None:\n            st = st.replace(tzinfo=EASTERN)\n        return (st - now).total_seconds() / 60 if st else 9999\n\n    # Sort by MTP ascending\n    sorted_races = sorted(qualified_races, key=get_mtp)\n    # Take top 10 opportunities\n    top_10 = sorted_races[:10]\n\n    lines.extend([\n        \"| Date | Venue | Race# | Selection | Odds | Gap | Goldmine? | Pred Top 5 | Payout Proof |\",\n        \"| --- | --- | --- | --- | --- | --- | --- | --- | --- |\"\n    ])\n    for r in top_10:\n        lines.append(format_prediction_row(r))\n    return \"\\n\".join(lines)\n",
      "name": "format_predictions_section"
    },
    {
      "type": "miscellaneous",
      "content": "\n\n"
    },
    {
      "type": "async_function",
      "content": "async def format_proof_section(db: FortunaDB) -> str:\n    \"\"\"Generates the Recent Audited Proof subsection for the GHA Job Summary.\"\"\"\n    lines = [\"\", \"#### \ud83d\udcb0 Recent Audited Proof\", \"\"]\n    try:\n        # First attempt to get recent goldmines\n        tips = await db.get_recent_audited_goldmines(limit=10)\n        # Fallback to any audited tips if no goldmines found\n        if not tips:\n            tips = await db.get_all_audited_tips()\n            tips = tips[:10]\n\n        if not tips:\n            lines.append(\"Awaiting race results; nothing audited yet.\")\n            return \"\\n\".join(lines)\n\n        lines.extend([\n            \"| Verdict | Profit | Venue | R# | Actual Top 5 | Actual 2nd Fav Odds | Payout Details |\",\n            \"| :--- | :--- | :--- | :--- | :--- | :--- | :--- |\"\n        ])\n        for tip in tips:\n            payouts = []\n            if tip.get('superfecta_payout'):\n                payouts.append(f\"Superfecta ${tip['superfecta_payout']:.2f}\")\n            if tip.get('trifecta_payout'):\n                payouts.append(f\"Trifecta ${tip['trifecta_payout']:.2f}\")\n            if tip.get('top1_place_payout'):\n                payouts.append(f\"Place ${tip['top1_place_payout']:.2f}\")\n\n            payout_text = ' / '.join(payouts) if payouts else 'No payout data'\n\n            verdict = tip.get(\"verdict\", \"?\")\n            emoji = \"\u2705\" if verdict == \"CASHED\" else \"\u274c\" if verdict == \"BURNED\" else \"\u26aa\"\n            profit = tip.get('net_profit', 0.0)\n            actual_odds = tip.get('actual_2nd_fav_odds')\n            actual_odds_str = f\"{actual_odds:.2f}\" if actual_odds else \"N/A\"\n\n            lines.append(\n                f\"| {emoji} {verdict} | ${profit:+.2f} | {tip['venue']} | {tip['race_number']} | {tip.get('actual_top_5', 'N/A')} | {actual_odds_str} | {payout_text} |\"\n            )\n    except Exception as e:\n        lines.append(f\"Error generating audited proof: {e}\")\n\n    return \"\\n\".join(lines)\n",
      "name": "format_proof_section"
    },
    {
      "type": "miscellaneous",
      "content": "\n\n"
    },
    {
      "type": "function",
      "content": "def build_harvest_table(summary: Dict[str, Any], title: str) -> str:\n    \"\"\"Generates a harvest performance table for the GHA Job Summary.\"\"\"\n    lines = [f\"### {title}\", \"\"]\n    if not summary:\n        lines.extend([\n            \"| Adapter | Races | Max Odds | Status |\",\n            \"| --- | --- | --- | --- |\",\n            \"| N/A | 0 | 0.0 | \u26a0\ufe0f No harvest data |\"\n        ])\n        return \"\\n\".join(lines)\n\n    lines.extend([\n        \"| Adapter | Races | Max Odds | Status |\",\n        \"| --- | --- | --- | --- |\"\n    ])\n\n    # Sort by Records Found (descending), then alphabetically\n    def sort_key(item):\n        adapter, data = item\n        count = data.get('count', 0) if isinstance(data, dict) else data\n        return (-count, adapter)\n\n    sorted_adapters = sorted(summary.items(), key=sort_key)\n\n    for adapter, data in sorted_adapters:\n        if isinstance(data, dict):\n            count = data.get('count', 0)\n            max_odds = data.get('max_odds', 0.0)\n        else:\n            count = data\n            max_odds = 0.0\n\n        status = '\u2705' if count > 0 else '\u26a0\ufe0f No Data'\n        lines.append(f\"| {adapter} | {count} | {max_odds:.1f} | {status} |\")\n    return \"\\n\".join(lines)\n",
      "name": "build_harvest_table"
    },
    {
      "type": "miscellaneous",
      "content": "\n\n"
    },
    {
      "type": "function",
      "content": "def format_artifact_links() -> str:\n    \"\"\"Generates the report artifacts links for the GHA Job Summary.\"\"\"\n    return '\\n'.join([\n        \"### \ud83d\udcc1 Report Artifacts\",\n        \"\",\n        \"- [Summary Grid](summary_grid.txt)\",\n        \"- [Field Matrix](field_matrix.txt)\",\n        \"- [Goldmine Report](goldmine_report.txt)\",\n        \"- [HTML Report](fortuna_report.html)\",\n        \"- [Analytics Log](analytics_report.txt)\"\n    ])\n",
      "name": "format_artifact_links"
    },
    {
      "type": "miscellaneous",
      "content": "\n\n"
    },
    {
      "type": "import",
      "content": "from contextlib import contextmanager\n"
    },
    {
      "type": "miscellaneous",
      "content": "\n@contextmanager\n"
    },
    {
      "type": "function",
      "content": "def open_summary():\n    \"\"\"Context manager for writing to GHA Job Summary with fallback to stdout.\"\"\"\n    path = os.environ.get('GITHUB_STEP_SUMMARY')\n    if path:\n        with open(path, 'a', encoding='utf-8') as f:\n            yield f\n    else:\n        # Fallback to stdout if not in GHA\n        yield sys.stdout\n",
      "name": "open_summary"
    },
    {
      "type": "miscellaneous",
      "content": "\n"
    },
    {
      "type": "function",
      "content": "def write_job_summary(predictions_md: str, harvest_md: str, proof_md: str, artifacts_md: str) -> None:\n    \"\"\"Writes the consolidated sections to $GITHUB_STEP_SUMMARY using an efficient context manager.\"\"\"\n    with open_summary() as f:\n        # Narrate the entire workflow\n        summary = '\\n'.join([\n            predictions_md,\n            '',\n            harvest_md,\n            '',\n            proof_md,\n            '',\n            artifacts_md,\n        ])\n        try:\n            f.write(summary + '\\n')\n        except Exception:\n            pass\n",
      "name": "write_job_summary"
    },
    {
      "type": "miscellaneous",
      "content": "\n\n"
    },
    {
      "type": "function",
      "content": "def get_db_path() -> str:\n    \"\"\"Returns the path to the SQLite database, using AppData in frozen mode.\"\"\"\n    if is_frozen() and sys.platform == \"win32\":\n        appdata = os.getenv('APPDATA')\n        if appdata:\n            db_dir = Path(appdata) / \"Fortuna\"\n            db_dir.mkdir(parents=True, exist_ok=True)\n            return str(db_dir / \"fortuna.db\")\n\n    return os.environ.get(\"FORTUNA_DB_PATH\", \"fortuna.db\")\n",
      "name": "get_db_path"
    },
    {
      "type": "miscellaneous",
      "content": "\n\n"
    },
    {
      "type": "class",
      "content": "class FortunaDB:\n    \"\"\"\n    Thread-safe SQLite backend for Fortuna using the standard library.\n    Handles persistence for tips, predictions, and audit outcomes.\n    \"\"\"\n    def __init__(self, db_path: Optional[str] = None):\n        self.db_path = db_path or get_db_path()\n        self._executor = ThreadPoolExecutor(max_workers=1)\n        self._conn = None\n        self._conn_lock = threading.Lock()\n\n        self._initialized = False\n        self.logger = structlog.get_logger(self.__class__.__name__)\n\n    def _get_conn(self):\n        with self._conn_lock:\n            if not self._conn:\n                self._conn = sqlite3.connect(self.db_path, check_same_thread=False)\n            self._conn.row_factory = sqlite3.Row\n            # Enable WAL mode for better concurrency\n            self._conn.execute(\"PRAGMA journal_mode=WAL\")\n        return self._conn\n\n    @asynccontextmanager\n    async def get_connection(self):\n        \"\"\"Returns an async context manager for a database connection.\"\"\"\n        try:\n            import aiosqlite\n        except ImportError:\n            self.logger.error(\"aiosqlite not installed. Async database features will fail.\")\n            raise\n\n        async with aiosqlite.connect(self.db_path) as conn:\n            conn.row_factory = aiosqlite.Row\n            yield conn\n\n    async def _run_in_executor(self, func, *args):\n        loop = asyncio.get_running_loop()\n        return await loop.run_in_executor(self._executor, func, *args)\n\n    async def initialize(self):\n        \"\"\"Creates the database schema if it doesn't exist.\"\"\"\n        if self._initialized: return\n\n        def _init():\n            conn = self._get_conn()\n            with conn:\n                conn.execute(\"\"\"\n                    CREATE TABLE IF NOT EXISTS schema_version (\n                        version INTEGER PRIMARY KEY,\n                        applied_at TEXT NOT NULL\n                    )\n                \"\"\")\n                conn.execute(\"\"\"\n                    CREATE TABLE IF NOT EXISTS harvest_logs (\n                        id INTEGER PRIMARY KEY AUTOINCREMENT,\n                        timestamp TEXT NOT NULL,\n                        region TEXT,\n                        adapter_name TEXT NOT NULL,\n                        race_count INTEGER NOT NULL,\n                        max_odds REAL\n                    )\n                \"\"\")\n                conn.execute(\"\"\"\n                    CREATE TABLE IF NOT EXISTS tips (\n                        id INTEGER PRIMARY KEY AUTOINCREMENT,\n                        race_id TEXT NOT NULL,\n                        venue TEXT NOT NULL,\n                        race_number INTEGER NOT NULL,\n                        discipline TEXT,\n                        start_time TEXT NOT NULL,\n                        report_date TEXT NOT NULL,\n                        is_goldmine INTEGER NOT NULL,\n                        gap12 TEXT,\n                        top_five TEXT,\n                        selection_number INTEGER,\n                        selection_name TEXT,\n                        audit_completed INTEGER DEFAULT 0,\n                        verdict TEXT,\n                        net_profit REAL,\n                        selection_position INTEGER,\n                        actual_top_5 TEXT,\n                        actual_2nd_fav_odds REAL,\n                        trifecta_payout REAL,\n                        trifecta_combination TEXT,\n                        superfecta_payout REAL,\n                        superfecta_combination TEXT,\n                        top1_place_payout REAL,\n                        top2_place_payout REAL,\n                        predicted_2nd_fav_odds REAL,\n                        audit_timestamp TEXT\n                    )\n                \"\"\")\n                # Composite index for deduplication - changed to race_id only for better deduplication\n                conn.execute(\"DROP INDEX IF EXISTS idx_race_report\")\n\n                # Cleanup potential duplicates before creating unique index (Memory Directive Fix)\n                try:\n                    self.logger.info(\"Cleaning up duplicate race_ids before indexing\")\n                    conn.execute(\"\"\"\n                        DELETE FROM tips\n                        WHERE id NOT IN (\n                            SELECT MAX(id)\n                            FROM tips\n                            GROUP BY race_id\n                        )\n                    \"\"\")\n                    self.logger.info(\"Duplicates removed, creating unique index\")\n                    conn.execute(\"CREATE UNIQUE INDEX IF NOT EXISTS idx_race_id ON tips (race_id)\")\n                except Exception as e:\n                    self.logger.error(\"Failed to cleanup or create unique index\", error=str(e))\n                    # If index exists but table has duplicates, we might get IntegrityError\n                    # Just log it and continue - better than crashing the whole app\n                # Composite index for audit performance\n                conn.execute(\"CREATE INDEX IF NOT EXISTS idx_audit_time ON tips (audit_completed, start_time)\")\n                conn.execute(\"CREATE INDEX IF NOT EXISTS idx_venue ON tips (venue)\")\n                conn.execute(\"CREATE INDEX IF NOT EXISTS idx_discipline ON tips (discipline)\")\n\n                # Add missing columns for existing databases\n                cursor = conn.execute(\"PRAGMA table_info(tips)\")\n                columns = [column[1] for column in cursor.fetchall()]\n                if \"superfecta_payout\" not in columns:\n                    conn.execute(\"ALTER TABLE tips ADD COLUMN superfecta_payout REAL\")\n                if \"superfecta_combination\" not in columns:\n                    conn.execute(\"ALTER TABLE tips ADD COLUMN superfecta_combination TEXT\")\n                if \"top1_place_payout\" not in columns:\n                    conn.execute(\"ALTER TABLE tips ADD COLUMN top1_place_payout REAL\")\n                if \"top2_place_payout\" not in columns:\n                    conn.execute(\"ALTER TABLE tips ADD COLUMN top2_place_payout REAL\")\n                if \"discipline\" not in columns:\n                    conn.execute(\"ALTER TABLE tips ADD COLUMN discipline TEXT\")\n                if \"predicted_2nd_fav_odds\" not in columns:\n                    conn.execute(\"ALTER TABLE tips ADD COLUMN predicted_2nd_fav_odds REAL\")\n                if \"actual_2nd_fav_odds\" not in columns:\n                    conn.execute(\"ALTER TABLE tips ADD COLUMN actual_2nd_fav_odds REAL\")\n                if \"selection_name\" not in columns:\n                    conn.execute(\"ALTER TABLE tips ADD COLUMN selection_name TEXT\")\n\n                # Maintenance: Purge garbage data (Memory Directive Fix)\n                try:\n                    res = conn.execute(\"DELETE FROM tips WHERE selection_name = 'Runner 2' OR predicted_2nd_fav_odds IN (2.75)\")\n                    if res.rowcount > 0:\n                        self.logger.info(\"Garbage data purged\", count=res.rowcount)\n                except Exception as e:\n                    self.logger.error(\"Failed to purge garbage data\", error=str(e))\n\n        await self._run_in_executor(_init)\n\n        # Track and execute migrations based on schema version\n        def _get_version():\n            cursor = self._get_conn().execute(\"SELECT MAX(version) FROM schema_version\")\n            row = cursor.fetchone()\n            return row[0] if row and row[0] is not None else 0\n\n        current_version = await self._run_in_executor(_get_version)\n\n        if current_version < 2:\n            await self.migrate_utc_to_eastern()\n            def _update_version():\n                with self._get_conn() as conn:\n                    conn.execute(\"INSERT OR REPLACE INTO schema_version (version, applied_at) VALUES (2, ?)\", (datetime.now(EASTERN).isoformat(),))\n            await self._run_in_executor(_update_version)\n            self.logger.info(\"Schema migrated to version 2\")\n\n        if current_version < 3:\n            def _declutter():\n                # Delete old records to keep database lean (30-day retention cleanup)\n                cutoff = (datetime.now(EASTERN) - timedelta(days=30)).isoformat()\n                with self._get_conn() as conn:\n                    cursor = conn.execute(\"DELETE FROM tips WHERE report_date < ?\", (cutoff,))\n                    self.logger.info(\"Database decluttered (30-day retention cleanup)\", deleted_count=cursor.rowcount)\n                    conn.execute(\"INSERT OR REPLACE INTO schema_version (version, applied_at) VALUES (3, ?)\", (datetime.now(EASTERN).isoformat(),))\n            await self._run_in_executor(_declutter)\n            self.logger.info(\"Schema migrated to version 3\")\n\n        if current_version < 4:\n            # Migration to version 4: Housekeeping & Long-term retention.\n            # 1. Clear the tips table for a fresh start as requested by JB.\n            # 2. Historical retention is now enabled (auto-cleanup removed from future migrations).\n            def _housekeeping():\n                self.logger.warning(\"Applying destructive migration: Clearing all historical tips for version 4 fresh start.\")\n                with self._get_conn() as conn:\n                    conn.execute(\"DELETE FROM tips\")\n                    conn.execute(\"INSERT OR REPLACE INTO schema_version (version, applied_at) VALUES (4, ?)\", (datetime.now(EASTERN).isoformat(),))\n            await self._run_in_executor(_housekeeping)\n            self.logger.info(\"Schema migrated to version 4 (Housekeeping complete, long-term retention enabled)\")\n\n        self._initialized = True\n        self.logger.info(\"Database initialized\", path=self.db_path, schema_version=max(current_version, 4))\n\n    async def migrate_utc_to_eastern(self) -> None:\n        \"\"\"Migrates existing database records from UTC to US Eastern Time.\"\"\"\n        def _migrate():\n            conn = self._get_conn()\n            cursor = conn.execute(\"\"\"\n                SELECT id, start_time, report_date, audit_timestamp FROM tips\n                WHERE start_time LIKE '%+00:00' OR start_time LIKE '%Z'\n                OR report_date LIKE '%+00:00' OR report_date LIKE '%Z'\n                OR audit_timestamp LIKE '%+00:00' OR audit_timestamp LIKE '%Z'\n            \"\"\")\n            rows = cursor.fetchall()\n            if not rows: return\n\n            total = len(rows)\n            self.logger.info(\"Migrating legacy UTC timestamps to Eastern\", count=total)\n            converted = 0\n            errors = 0\n\n            # Process in chunks of 1000 for safety (Memory Directive Fix)\n            for i in range(0, total, 1000):\n                chunk = rows[i:i+1000]\n                with conn:\n                    for row in chunk:\n                        updates = {}\n                        for col in [\"start_time\", \"report_date\", \"audit_timestamp\"]:\n                            if col not in row.keys(): continue\n                            val = row[col]\n                            if val:\n                                try:\n                                    dt = datetime.fromisoformat(val.replace(\"Z\", \"+00:00\"))\n                                    dt_eastern = ensure_eastern(dt)\n                                    updates[col] = dt_eastern.isoformat()\n                                except Exception: pass\n                        if updates:\n                            try:\n                                set_clause = \", \".join([f\"{k} = ?\" for k in updates.keys()])\n                                conn.execute(f\"UPDATE tips SET {set_clause} WHERE id = ?\", (*updates.values(), row[\"id\"]))\n                                converted += 1\n                            except Exception as e:\n                                errors += 1\n                                self.logger.warning(\"Failed to migrate row\", row_id=row[\"id\"], error=str(e))\n                self.logger.info(\"Migration progress\", processed=min(i + 1000, total), total=total)\n\n            self.logger.info(\"Migration complete\", total=total, converted=converted, errors=errors)\n        await self._run_in_executor(_migrate)\n\n    async def log_harvest(self, harvest_summary: Dict[str, Any], region: Optional[str] = None):\n        \"\"\"Logs harvest performance metrics to the database.\"\"\"\n        if not self._initialized: await self.initialize()\n\n        def _log():\n            conn = self._get_conn()\n            now = datetime.now(EASTERN).isoformat()\n            to_insert = []\n            for adapter, data in harvest_summary.items():\n                if isinstance(data, dict):\n                    count = data.get(\"count\", 0)\n                    max_odds = data.get(\"max_odds\", 0.0)\n                else:\n                    count = data\n                    max_odds = 0.0\n\n                to_insert.append((now, region, adapter, count, max_odds))\n\n            if to_insert:\n                with conn:\n                    conn.executemany(\"\"\"\n                        INSERT INTO harvest_logs (timestamp, region, adapter_name, race_count, max_odds)\n                        VALUES (?, ?, ?, ?, ?)\n                    \"\"\", to_insert)\n\n        await self._run_in_executor(_log)\n\n    async def get_adapter_scores(self, days: int = 30) -> Dict[str, float]:\n        \"\"\"Calculates historical performance scores for each adapter.\"\"\"\n        if not self._initialized: await self.initialize()\n\n        def _get():\n            conn = self._get_conn()\n            cutoff = (datetime.now(EASTERN) - timedelta(days=days)).isoformat()\n            cursor = conn.execute(\"\"\"\n                SELECT adapter_name,\n                       AVG(race_count) as avg_count,\n                       AVG(max_odds) as avg_max_odds\n                FROM harvest_logs\n                WHERE timestamp > ?\n                GROUP BY adapter_name\n            \"\"\", (cutoff,))\n\n            scores = {}\n            for row in cursor.fetchall():\n                # Heuristic: Score = Avg Race Count + (Avg Max Odds * 2)\n                # This prioritizes adapters that find races and high longshots\n                scores[row[\"adapter_name\"]] = (row[\"avg_count\"] or 0) + ((row[\"avg_max_odds\"] or 0) * 2)\n            return scores\n\n        return await self._run_in_executor(_get)\n\n    async def log_tips(self, tips: List[Dict[str, Any]], dedup_window_hours: int = 12):\n        \"\"\"Logs new tips to the database with batch deduplication.\"\"\"\n        if not self._initialized: await self.initialize()\n\n        def _log():\n            conn = self._get_conn()\n            now = datetime.now(EASTERN)\n\n            # Batch check for recently logged tips to avoid redundant entries\n            race_ids = [t.get(\"race_id\") for t in tips if t.get(\"race_id\")]\n            if not race_ids: return\n\n            placeholders = \",\".join([\"?\"] * len(race_ids))\n\n            # Use a more absolute check to ensure distinct races across all time\n            cursor = conn.execute(\n                f\"SELECT race_id FROM tips WHERE race_id IN ({placeholders})\",\n                (*race_ids,)\n            )\n            already_logged = {row[\"race_id\"] for row in cursor.fetchall()}\n\n            to_insert = []\n            for tip in tips:\n                rid = tip.get(\"race_id\")\n                if rid and rid not in already_logged:\n                    report_date = tip.get(\"report_date\") or now.isoformat()\n                    to_insert.append((\n                        rid, tip.get(\"venue\"), tip.get(\"race_number\"),\n                        tip.get(\"discipline\"), tip.get(\"start_time\"), report_date,\n                        1 if tip.get(\"is_goldmine\") else 0,\n                        str(tip.get(\"1Gap2\", 0.0)),\n                        tip.get(\"top_five\"), tip.get(\"selection_number\"), tip.get(\"selection_name\"),\n                        float(tip.get(\"predicted_2nd_fav_odds\")) if tip.get(\"predicted_2nd_fav_odds\") is not None else None\n                    ))\n                    already_logged.add(rid) # Avoid duplicates within the same batch\n\n            if to_insert:\n                with conn:\n                    conn.executemany(\"\"\"\n                        INSERT OR IGNORE INTO tips (\n                            race_id, venue, race_number, discipline, start_time, report_date,\n                            is_goldmine, gap12, top_five, selection_number, selection_name, predicted_2nd_fav_odds\n                        ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\n                    \"\"\", to_insert)\n                self.logger.info(\"Hot tips batch logged\", count=len(to_insert))\n\n        await self._run_in_executor(_log)\n\n    async def get_unverified_tips(self, lookback_hours: int = 48) -> List[Dict[str, Any]]:\n        \"\"\"Returns tips that haven't been audited yet but have likely finished.\"\"\"\n        if not self._initialized: await self.initialize()\n\n        def _get():\n            conn = self._get_conn()\n            now = datetime.now(EASTERN)\n            cutoff = (now - timedelta(hours=lookback_hours)).isoformat()\n\n            cursor = conn.execute(\n                \"\"\"SELECT * FROM tips\n                   WHERE audit_completed = 0\n                   AND report_date > ?\n                   AND start_time < ?\"\"\",\n                (cutoff, now.isoformat())\n            )\n            return [dict(row) for row in cursor.fetchall()]\n        return await self._run_in_executor(_get)\n\n    async def get_recent_tips(self, limit: int = 20) -> List[Dict[str, Any]]:\n        \"\"\"Returns the most recent tips regardless of audit status, ordered by discovery time.\"\"\"\n        if not self._initialized: await self.initialize()\n        def _get():\n            # Use ID DESC to show most recently discovered tips first\n            cursor = self._get_conn().execute(\n                \"SELECT * FROM tips ORDER BY id DESC LIMIT ?\",\n                (limit,)\n            )\n            return [dict(row) for row in cursor.fetchall()]\n        return await self._run_in_executor(_get)\n\n    async def update_audit_result(self, race_id: str, outcome: Dict[str, Any]):\n        \"\"\"Updates a single tip with its audit outcome.\"\"\"\n        if not self._initialized: await self.initialize()\n\n        def _update():\n            conn = self._get_conn()\n            with conn:\n                conn.execute(\"\"\"\n                    UPDATE tips SET\n                        audit_completed = 1,\n                        verdict = ?,\n                        net_profit = ?,\n                        selection_position = ?,\n                        actual_top_5 = ?,\n                        actual_2nd_fav_odds = ?,\n                        trifecta_payout = ?,\n                        trifecta_combination = ?,\n                        superfecta_payout = ?,\n                        superfecta_combination = ?,\n                        top1_place_payout = ?,\n                        top2_place_payout = ?,\n                        audit_timestamp = ?\n                    WHERE id = (\n                        SELECT id FROM tips\n                        WHERE race_id = ? AND audit_completed = 0\n                        LIMIT 1\n                    )\n                \"\"\", (\n                    outcome.get(\"verdict\"), outcome.get(\"net_profit\"),\n                    outcome.get(\"selection_position\"), outcome.get(\"actual_top_5\"),\n                    outcome.get(\"actual_2nd_fav_odds\"), outcome.get(\"trifecta_payout\"),\n                    outcome.get(\"trifecta_combination\"),\n                    outcome.get(\"superfecta_payout\"),\n                    outcome.get(\"superfecta_combination\"),\n                    outcome.get(\"top1_place_payout\"),\n                    outcome.get(\"top2_place_payout\"),\n                    datetime.now(EASTERN).isoformat(),\n                    race_id\n                ))\n        await self._run_in_executor(_update)\n\n    async def update_audit_results_batch(self, outcomes: List[Tuple[str, Dict[str, Any]]]):\n        \"\"\"Updates multiple tips with their audit outcomes in a single transaction.\"\"\"\n        if not outcomes: return\n        if not self._initialized: await self.initialize()\n\n        def _update():\n            conn = self._get_conn()\n            with conn:\n                for race_id, outcome in outcomes:\n                    conn.execute(\"\"\"\n                        UPDATE tips SET\n                            audit_completed = 1,\n                            verdict = ?,\n                            net_profit = ?,\n                            selection_position = ?,\n                            actual_top_5 = ?,\n                            actual_2nd_fav_odds = ?,\n                            trifecta_payout = ?,\n                            trifecta_combination = ?,\n                            superfecta_payout = ?,\n                            superfecta_combination = ?,\n                            top1_place_payout = ?,\n                            top2_place_payout = ?,\n                            audit_timestamp = ?\n                        WHERE id = (\n                            SELECT id FROM tips\n                            WHERE race_id = ? AND audit_completed = 0\n                            LIMIT 1\n                        )\n                    \"\"\", (\n                        outcome.get(\"verdict\"), outcome.get(\"net_profit\"),\n                        outcome.get(\"selection_position\"), outcome.get(\"actual_top_5\"),\n                        outcome.get(\"actual_2nd_fav_odds\"), outcome.get(\"trifecta_payout\"),\n                        outcome.get(\"trifecta_combination\"),\n                        outcome.get(\"superfecta_payout\"),\n                        outcome.get(\"superfecta_combination\"),\n                        outcome.get(\"top1_place_payout\"),\n                        outcome.get(\"top2_place_payout\"),\n                        outcome.get(\"audit_timestamp\"),\n                        race_id\n                    ))\n        await self._run_in_executor(_update)\n\n    async def get_all_audited_tips(self) -> List[Dict[str, Any]]:\n        \"\"\"Returns all audited tips for reporting.\"\"\"\n        if not self._initialized: await self.initialize()\n        def _get():\n            cursor = self._get_conn().execute(\n                \"SELECT * FROM tips WHERE audit_completed = 1 ORDER BY start_time DESC\"\n            )\n            return [dict(row) for row in cursor.fetchall()]\n        return await self._run_in_executor(_get)\n\n    async def get_recent_audited_goldmines(self, limit: int = 15) -> List[Dict[str, Any]]:\n        \"\"\"Returns recent successfully audited goldmine tips.\"\"\"\n        if not self._initialized: await self.initialize()\n        def _get():\n            cursor = self._get_conn().execute(\n                \"SELECT * FROM tips WHERE audit_completed = 1 AND is_goldmine = 1 ORDER BY start_time DESC LIMIT ?\",\n                (limit,)\n            )\n            return [dict(row) for row in cursor.fetchall()]\n        return await self._run_in_executor(_get)\n\n    async def clear_all_tips(self):\n        \"\"\"Wipes all records from the tips table.\"\"\"\n        if not self._initialized: await self.initialize()\n        def _clear():\n            conn = self._get_conn()\n            with conn:\n                conn.execute(\"DELETE FROM tips\")\n            conn.execute(\"VACUUM\")\n            self.logger.info(\"Database cleared (all tips deleted)\")\n        await self._run_in_executor(_clear)\n\n    async def migrate_from_json(self, json_path: str = \"hot_tips_db.json\"):\n        \"\"\"Migrates data from existing JSON file to SQLite with detailed error logging.\"\"\"\n        path = Path(json_path)\n        if not path.exists(): return\n        try:\n            with open(path, \"r\") as f:\n                data = json.load(f)\n            if not isinstance(data, list): return\n            self.logger.info(\"Migrating data from JSON\", count=len(data))\n            if not self._initialized: await self.initialize()\n\n            def _migrate():\n                conn = self._get_conn()\n                success_count = 0\n                for entry in data:\n                    try:\n                        with conn:\n                            conn.execute(\"\"\"\n                                INSERT OR IGNORE INTO tips (\n                                    race_id, venue, race_number, start_time, report_date,\n                                    is_goldmine, gap12, top_five, selection_number,\n                                    audit_completed, verdict, net_profit, selection_position,\n                                    actual_top_5, actual_2nd_fav_odds, trifecta_payout,\n                                    trifecta_combination, superfecta_payout,\n                                    superfecta_combination, top1_place_payout,\n                                    top2_place_payout, audit_timestamp\n                                ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\n                            \"\"\", (\n                                entry.get(\"race_id\"), entry.get(\"venue\"), entry.get(\"race_number\"),\n                                entry.get(\"start_time\"), entry.get(\"report_date\"),\n                                1 if entry.get(\"is_goldmine\") else 0, str(entry.get(\"1Gap2\", 0.0)),\n                                entry.get(\"top_five\"), entry.get(\"selection_number\"),\n                                1 if entry.get(\"audit_completed\") else 0, entry.get(\"verdict\"),\n                                entry.get(\"net_profit\"), entry.get(\"selection_position\"),\n                                entry.get(\"actual_top_5\"), entry.get(\"actual_2nd_fav_odds\"),\n                                entry.get(\"trifecta_payout\"), entry.get(\"trifecta_combination\"),\n                                entry.get(\"superfecta_payout\"), entry.get(\"superfecta_combination\"),\n                                entry.get(\"top1_place_payout\"), entry.get(\"top2_place_payout\"),\n                                entry.get(\"audit_timestamp\")\n                            ))\n                        success_count += 1\n                    except Exception as e:\n                        self.logger.error(\"Failed to migrate entry\", race_id=entry.get(\"race_id\"), error=str(e))\n                return success_count\n\n            count = await self._run_in_executor(_migrate)\n            self.logger.info(\"Migration complete\", successful=count)\n        except Exception as e:\n            self.logger.error(\"Migration failed\", error=str(e))\n\n    async def close(self):\n        def _close():\n            if self._conn:\n                self._conn.close()\n                self._conn = None\n\n        await self._run_in_executor(_close)\n        self._executor.shutdown(wait=True)\n",
      "name": "FortunaDB"
    },
    {
      "type": "miscellaneous",
      "content": "\n\n"
    },
    {
      "type": "class",
      "content": "class HotTipsTracker:\n    \"\"\"Logs reported opportunities to a SQLite database.\"\"\"\n    def __init__(self, db_path: Optional[str] = None):\n        self.db = FortunaDB(db_path) if db_path else FortunaDB()\n        self.logger = structlog.get_logger(self.__class__.__name__)\n\n    async def log_tips(self, races: List[Race]):\n        if not races:\n            return\n\n        await self.db.initialize()\n        now = datetime.now(EASTERN)\n        report_date = now.isoformat()\n        new_tips = []\n\n        # Strict future cutoff to prevent leakage (Never log more than 20 mins ahead)\n        future_limit = now + timedelta(minutes=45)\n\n        for r in races:\n            # Only store \"Best Bets\" (Goldmine, BET NOW, or You Might Like)\n            # These are marked in metadata by the analyzer.\n            if not r.metadata.get('is_best_bet') and not r.metadata.get('is_goldmine'):\n                continue\n\n            # Trustworthiness Airlock Safeguard (Council of Superbrains Directive)\n            active_runners = [run for run in r.runners if not run.scratched]\n            total_active = len(active_runners)\n\n            # Ensure trustworthy odds exist before logging (Memory Directive Fix)\n            if r.metadata.get('predicted_2nd_fav_odds') is None:\n                continue\n\n            if total_active > 0:\n                trustworthy_count = sum(1 for run in active_runners if run.metadata.get(\"odds_source_trustworthy\"))\n                trust_ratio = trustworthy_count / total_active\n                if trust_ratio < 0.5:\n                    self.logger.warning(\"Rejecting race with low trust_ratio for DB logging\", venue=r.venue, race=r.race_number, trust_ratio=round(trust_ratio, 2))\n                    continue\n\n            st = r.start_time\n            if isinstance(st, str):\n                try: st = datetime.fromisoformat(st.replace('Z', '+00:00'))\n                except Exception: continue\n            if st.tzinfo is None: st = st.replace(tzinfo=EASTERN)\n\n            # Reject races too far in the future\n            if st > future_limit or st < now - timedelta(minutes=10):\n                self.logger.debug(\"Rejecting far-future race\", venue=r.venue, start_time=st)\n                continue\n\n            is_goldmine = r.metadata.get('is_goldmine', False)\n            gap12 = r.metadata.get('1Gap2', 0.0)\n\n            tip_data = {\n                \"report_date\": report_date,\n                \"race_id\": r.id,\n                \"venue\": r.venue,\n                \"race_number\": r.race_number,\n                \"start_time\": r.start_time.isoformat() if isinstance(r.start_time, datetime) else str(r.start_time),\n                \"is_goldmine\": is_goldmine,\n                \"1Gap2\": gap12,\n                \"discipline\": r.discipline,\n                \"top_five\": r.top_five_numbers,\n                \"selection_number\": r.metadata.get('selection_number'),\n                \"selection_name\": r.metadata.get('selection_name'),\n                \"predicted_2nd_fav_odds\": r.metadata.get('predicted_2nd_fav_odds')\n            }\n            new_tips.append(tip_data)\n\n        try:\n            await self.db.log_tips(new_tips)\n            self.logger.info(\"Hot tips processed\", count=len(new_tips))\n        except Exception as e:\n            self.logger.error(\"Failed to log hot tips\", error=str(e))\n",
      "name": "HotTipsTracker"
    },
    {
      "type": "miscellaneous",
      "content": "\n\n# ----------------------------------------\n# MONITOR LOGIC\n# ----------------------------------------\n#!/usr/bin/env python3\n"
    },
    {
      "type": "docstring",
      "content": "\"\"\"\nFortuna Favorite-to-Place Betting Monitor\n=========================================\n\nThis script monitors racing data from multiple adapters and identifies\nbetting opportunities based on:\n1. Second favorite odds >= 4.0 decimal\n2. Races under 120 minutes to post (MTP)\n3. Superfecta availability preferred\n\nUsage:\n    python favorite_to_place_monitor.py [--date YYYY-MM-DD] [--refresh-interval 30]\n\"\"\"\n"
    },
    {
      "type": "miscellaneous",
      "content": "\n@dataclass\n"
    },
    {
      "type": "class",
      "content": "class RaceSummary:\n    \"\"\"Summary of a single race for display.\"\"\"\n    discipline: str  # T/H/G\n    track: str\n    race_number: int\n    field_size: int\n    superfecta_offered: bool\n    adapter: str\n    start_time: datetime\n    mtp: Optional[int] = None  # Minutes to post\n    second_fav_odds: Optional[float] = None\n    second_fav_name: Optional[str] = None\n    selection_number: Optional[int] = None\n    favorite_odds: Optional[float] = None\n    favorite_name: Optional[str] = None\n    top_five_numbers: Optional[str] = None\n    gap12: float = 0.0\n    is_goldmine: bool = False\n    is_best_bet: bool = False\n\n    def to_dict(self) -> dict:\n        \"\"\"Convert to dictionary for JSON serialization.\"\"\"\n        return {\n            \"discipline\": self.discipline,\n            \"track\": self.track,\n            \"race_number\": self.race_number,\n            \"field_size\": self.field_size,\n            \"superfecta_offered\": self.superfecta_offered,\n            \"adapter\": self.adapter,\n            \"start_time\": self.start_time.isoformat(),\n            \"mtp\": self.mtp,\n            \"second_fav_odds\": self.second_fav_odds,\n            \"second_fav_name\": self.second_fav_name,\n            \"selection_number\": self.selection_number,\n            \"favorite_odds\": self.favorite_odds,\n            \"favorite_name\": self.favorite_name,\n            \"top_five_numbers\": self.top_five_numbers,\n            \"gap12\": self.gap12,\n            \"is_goldmine\": self.is_goldmine,\n            \"is_best_bet\": self.is_best_bet,\n        }\n",
      "name": "RaceSummary"
    },
    {
      "type": "miscellaneous",
      "content": "\n\n"
    },
    {
      "type": "function",
      "content": "def get_discovery_adapter_classes() -> List[Type[BaseAdapterV3]]:\n    \"\"\"Returns all non-abstract discovery adapter classes.\"\"\"\n    def get_all_subclasses(cls):\n        return set(cls.__subclasses__()).union(\n            [s for c in cls.__subclasses__() for s in get_all_subclasses(c)]\n        )\n\n    return [\n        c for c in get_all_subclasses(BaseAdapterV3)\n        if not getattr(c, \"__abstractmethods__\", None)\n        and getattr(c, \"ADAPTER_TYPE\", \"discovery\") == \"discovery\"\n    ]\n",
      "name": "get_discovery_adapter_classes"
    },
    {
      "type": "miscellaneous",
      "content": "\n\n"
    },
    {
      "type": "class",
      "content": "class FavoriteToPlaceMonitor:\n    \"\"\"Monitor for favorite-to-place betting opportunities.\"\"\"\n\n    def __init__(self, target_dates: Optional[List[str]] = None, refresh_interval: int = 30, config: Optional[Dict] = None):\n        \"\"\"\n        Initialize monitor.\n\n        Args:\n            target_dates: Dates to fetch races for (YYYY-MM-DD), defaults to today + tomorrow\n            refresh_interval: Seconds between refreshes for BET NOW list\n        \"\"\"\n        if target_dates:\n            self.target_dates = target_dates\n        else:\n            today = datetime.now(EASTERN)\n            tomorrow = today + timedelta(days=1)\n            self.target_dates = [today.strftime(\"%Y-%m-%d\"), tomorrow.strftime(\"%Y-%m-%d\")]\n\n        self.refresh_interval = refresh_interval\n        self.config = config or {}\n        self.all_races: List[RaceSummary] = []\n        self.adapters: List = []\n        self.logger = structlog.get_logger(self.__class__.__name__)\n        self.tracker = HotTipsTracker()\n\n    async def initialize_adapters(self, adapter_names: Optional[List[str]] = None):\n        \"\"\"Initialize all adapters, optionally filtered by name.\"\"\"\n        all_discovery_classes = get_discovery_adapter_classes()\n\n        classes_to_init = all_discovery_classes\n        if adapter_names:\n            classes_to_init = [c for c in all_discovery_classes if c.__name__ in adapter_names or getattr(c, \"SOURCE_NAME\", \"\") in adapter_names]\n\n        self.logger.info(\"Initializing adapters\", count=len(classes_to_init))\n\n        for adapter_class in classes_to_init:\n            try:\n                adapter = adapter_class(config={\"region\": self.config.get(\"region\")})\n                self.adapters.append(adapter)\n                self.logger.debug(\"Adapter initialized\", adapter=adapter_class.__name__)\n            except Exception as e:\n                self.logger.error(\"Adapter initialization failed\", adapter=adapter_class.__name__, error=str(e))\n\n        self.logger.info(\"Adapters initialization complete\", initialized=len(self.adapters))\n\n    async def fetch_all_races(self) -> List[Tuple[Race, str]]:\n        \"\"\"Fetch races from all adapters.\"\"\"\n        self.logger.info(\"Fetching races\", dates=self.target_dates)\n\n        all_races_with_adapters = []\n\n        # Run fetches in parallel for speed\n        async def fetch_one(adapter, date_str):\n            name = adapter.__class__.__name__\n            try:\n                races = await adapter.get_races(date_str)\n                self.logger.info(\"Fetch complete\", adapter=name, date=date_str, count=len(races))\n                return [(r, name) for r in races]\n            except Exception as e:\n                self.logger.error(\"Fetch failed\", adapter=name, date=date_str, error=str(e))\n                return []\n\n        fetch_tasks = []\n        for d in self.target_dates:\n            for a in self.adapters:\n                fetch_tasks.append(fetch_one(a, d))\n\n        results = await asyncio.gather(*fetch_tasks)\n        for r_list in results:\n            all_races_with_adapters.extend(r_list)\n\n        self.logger.info(\"Total races fetched\", total=len(all_races_with_adapters))\n        return all_races_with_adapters\n\n    def _get_discipline_code(self, race: Race) -> str:\n        \"\"\"Get discipline code (T/H/G).\"\"\"\n        if not race.discipline:\n            return \"T\"\n\n        d = race.discipline.lower()\n        if \"harness\" in d or \"standardbred\" in d: return \"H\"\n        if \"greyhound\" in d or \"dog\" in d: return \"G\"\n        return \"T\"\n\n    def _calculate_field_size(self, race: Race) -> int:\n        \"\"\"Calculate active field size.\"\"\"\n        return len([r for r in race.runners if not r.scratched])\n\n    def _has_superfecta(self, race: Race) -> bool:\n        \"\"\"Check if race offers Superfecta.\"\"\"\n        ab = race.available_bets or []\n        # Support metadata fallback if field not populated\n        if not ab and hasattr(race, 'metadata'):\n            ab = race.metadata.get('available_bets', [])\n        return \"Superfecta\" in ab\n\n    def _get_top_runners(self, race: Race, limit: int = 5) -> List[Runner]:\n        \"\"\"Get top runners by odds, sorted lowest first.\"\"\"\n        # Get active runners with valid odds\n        r_with_odds = []\n        for r in race.runners:\n            if r.scratched:\n                continue\n            # Refresh odds to avoid stale metadata in continuous monitor mode\n            wo = _get_best_win_odds(r)\n            if wo is not None and wo > 1.0:\n                # Update runner object with fresh odds for downstream summaries\n                r.win_odds = float(wo)\n                # Store the Decimal odds directly for sorting to avoid conversion\n                r_with_odds.append((r, wo))\n\n        if not r_with_odds:\n            return []\n\n        # Sort by odds (lowest first)\n        sorted_r = sorted(r_with_odds, key=lambda x: x[1])\n        return [x[0] for x in sorted_r[:limit]]\n\n    def _calculate_mtp(self, start_time: Optional[datetime]) -> int:\n        \"\"\"Calculate minutes to post. Returns -9999 if start_time is None.\"\"\"\n        if not start_time: return -9999\n        now = now_eastern()\n        # Use ensure_eastern to handle naive or other timezones correctly\n        st = ensure_eastern(start_time)\n        delta = st - now\n        return int(delta.total_seconds() / 60)\n\n    def _get_top_n_runners(self, race: Race, n: int = 5) -> str:\n        \"\"\"Get top N runners by win odds.\"\"\"\n        top_runners = self._get_top_runners(race, limit=n)\n        return \", \".join([str(r.number) if r.number is not None else \"?\" for r in top_runners])\n\n    def _create_race_summary(self, race: Race, adapter_name: str) -> RaceSummary:\n        \"\"\"Create a RaceSummary from a Race object.\"\"\"\n        top_runners = self._get_top_runners(race, limit=5)\n        favorite = top_runners[0] if len(top_runners) >= 1 else None\n        second_fav = top_runners[1] if len(top_runners) >= 2 else None\n\n        gap12 = 0.0\n        if favorite and second_fav and favorite.win_odds and second_fav.win_odds:\n            gap12 = round(second_fav.win_odds - favorite.win_odds, 2)\n\n        return RaceSummary(\n            discipline=self._get_discipline_code(race),\n            track=normalize_venue_name(race.venue),\n            race_number=race.race_number,\n            field_size=self._calculate_field_size(race),\n            superfecta_offered=self._has_superfecta(race),\n            adapter=adapter_name,\n            start_time=race.start_time,\n            mtp=self._calculate_mtp(race.start_time),\n            second_fav_odds=second_fav.win_odds if second_fav else None,\n            second_fav_name=second_fav.name if second_fav else None,\n            selection_number=second_fav.number if second_fav else None,\n            favorite_odds=favorite.win_odds if favorite else None,\n            favorite_name=favorite.name if favorite else None,\n            top_five_numbers=self._get_top_n_runners(race, 5),\n            gap12=gap12,\n            is_goldmine=race.metadata.get('is_goldmine', False),\n            is_best_bet=race.metadata.get('is_best_bet', False)\n        )\n\n    async def build_race_summaries(self, races_with_adapters: List[Tuple[Race, str]], window_hours: Optional[int] = 12):\n        \"\"\"Build and deduplicate summary list, with optional time window filtering.\"\"\"\n        race_map = {}\n        now = datetime.now(EASTERN)\n        cutoff = now + timedelta(hours=window_hours) if window_hours else None\n\n        for race, adapter_name in races_with_adapters:\n            try:\n                # Time window filtering\n                st = race.start_time\n                if st.tzinfo is None: st = st.replace(tzinfo=EASTERN)\n\n                # Time window filtering removed to ensure all unique races are counted\n\n                summary = self._create_race_summary(race, adapter_name)\n                # Stable key: Canonical Venue + Race Number + Date\n                canonical_venue = get_canonical_venue(summary.track)\n                date_str = summary.start_time.strftime('%Y%m%d') if summary.start_time else \"Unknown\"\n                key = f\"{canonical_venue}|{summary.race_number}|{date_str}\"\n\n                if key not in race_map:\n                    race_map[key] = summary\n                else:\n                    existing = race_map[key]\n                    # Prefer the one with valid second favorite odds\n                    if summary.second_fav_odds and not existing.second_fav_odds:\n                        race_map[key] = summary\n                    # Or prefer more detailed available bets\n                    elif summary.superfecta_offered and not existing.superfecta_offered:\n                        race_map[key] = summary\n            except Exception: pass\n\n        unique_summaries = list(race_map.values())\n        self.all_races = sorted(unique_summaries, key=lambda x: x.start_time)\n\n        # GPT5 Improvement: Keep all races within window for analysis, not just one per track.\n        # Window broadened to 18 hours (News Mode)\n        timing_window_summaries = []\n        now = datetime.now(EASTERN)\n        for summary in unique_summaries:\n            st = summary.start_time\n            if st.tzinfo is None: st = st.replace(tzinfo=EASTERN)\n\n            # Calculate Minutes to Post\n            diff = st - now\n            mtp = diff.total_seconds() / 60\n\n            # Broaden window to 18 hours to ensure yield for \"News\"\n            if -45 < mtp <= 1080: # 18 hours\n                timing_window_summaries.append(summary)\n\n        self.golden_zone_races = timing_window_summaries\n        if not self.golden_zone_races:\n            self.logger.warning(\"\ud83d\udd2d Monitor found 0 races in the Broadened Window (-45m to 18h)\", total_unique=len(unique_summaries))\n\n    def print_full_list(self):\n        \"\"\"Log all fetched races.\"\"\"\n        lines = [\n            \"=\" * 120,\n            \"FULL RACE LIST\".center(120),\n            \"=\" * 120,\n            f\"{'DISC':<5} {'TRACK':<25} {'R#':<4} {'FIELD':<6} {'SUPER':<6} {'ADAPTER':<25} {'START TIME':<20}\",\n            \"-\" * 120\n        ]\n        for r in sorted(self.all_races, key=lambda x: (x.discipline, x.track, x.race_number)):\n            superfecta = \"Yes\" if r.superfecta_offered else \"No\"\n            # Display time in Eastern with ET suffix\n            st = r.start_time.strftime(\"%Y-%m-%d %H:%M ET\") if r.start_time else \"Unknown\"\n            lines.append(f\"{r.discipline:<5} {r.track[:24]:<25} {r.race_number:<4} {r.field_size:<6} {superfecta:<6} {r.adapter[:24]:<25} {st:<20}\")\n        lines.append(\"-\" * 120)\n        lines.append(f\"Total races: {len(self.all_races)}\")\n        self.logger.info(\"\\n\".join(lines))\n\n    def get_bet_now_races(self) -> List[RaceSummary]:\n        \"\"\"Get races meeting BET NOW criteria.\"\"\"\n        # 1. MTP <= 120 (Broadened for yield)\n        # 2. 2nd Fav Odds >= 4.0\n        # 3. Field size <= 11 (User Directive)\n        # 4. Gap > 0.25 (User Directive)\n        bet_now = [\n            r for r in self.golden_zone_races\n            if r.mtp is not None and -10 < r.mtp <= 120\n            and r.second_fav_odds is not None and r.second_fav_odds >= 4.0\n            and r.field_size <= 11\n            and r.gap12 > 0.25\n        ]\n        # Sort by Superfecta desc, then MTP asc\n        bet_now.sort(key=lambda r: (not r.superfecta_offered, r.mtp))\n        return bet_now\n\n    def get_you_might_like_races(self) -> List[RaceSummary]:\n        \"\"\"Get 'You Might Like' races with relaxed criteria.\"\"\"\n        # Criteria: Not in BET NOW, but -10 < MTP <= 240 (4h) and 2nd Fav Odds >= 3.0\n        # and field size <= 11 and Gap > 0.25\n        bet_now_keys = {(r.track, r.race_number) for r in self.get_bet_now_races()}\n        yml = [\n            r for r in self.golden_zone_races\n            if r.mtp is not None and -10 < r.mtp <= 240\n            and r.second_fav_odds is not None and r.second_fav_odds >= 3.0\n            and r.field_size <= 11\n            and r.gap12 > 0.25\n            and (r.track, r.race_number) not in bet_now_keys\n        ]\n        # Sort by MTP asc\n        yml.sort(key=lambda r: r.mtp)\n        return yml[:5]  # Limit to top 5 recommendations\n\n    async def print_bet_now_list(self):\n        \"\"\"Log filtered BET NOW list and recent audited goldmine results.\"\"\"\n        bet_now = self.get_bet_now_races()\n        lines = [\n            \"=\" * 140,\n            \"\ud83c\udfaf BET NOW - FAVORITE TO PLACE OPPORTUNITIES\".center(140),\n            \"=\" * 140,\n            f\"Updated: {datetime.now(EASTERN).strftime('%Y-%m-%d %H:%M:%S')} ET\",\n            \"Criteria: -10 < MTP <= 120 minutes AND 2nd Favorite Odds >= 4.0\",\n            \"-\" * 140\n        ]\n        if not bet_now:\n            lines.append(\"\u23f3 No races currently meet BET NOW criteria.\")\n            yml = self.get_you_might_like_races()\n            if yml:\n                lines.extend([\n                    \"=\" * 160,\n                    \"\ud83c\udf1f YOU MIGHT LIKE - NEAR-MISS OPPORTUNITIES\".center(160),\n                    \"=\" * 160,\n                    f\"{'SUPER':<6} {'MTP':<5} {'DISC':<5} {'TRACK':<20} {'R#':<4} {'FIELD':<6} {'ODDS':<20} {'TOP 5':<20}\",\n                    \"-\" * 160\n                ])\n                for r in yml:\n                    sup = \"\u2705\" if r.superfecta_offered else \"\u274c\"\n                    fo = f\"{r.favorite_odds:.2f}\" if r.favorite_odds else \"N/A\"\n                    so = f\"{r.second_fav_odds:.2f}\" if r.second_fav_odds else \"N/A\"\n                    top5 = r.top_five_numbers or \"N/A\"\n                    # Leading zero alignment (Memory Directive Fix)\n                    m_str = f\"{r.mtp:02d}\" if 0 <= r.mtp < 10 else str(r.mtp)\n                    lines.append(f\"{sup:<6} {m_str:<5} {r.discipline:<5} {r.track[:19]:<20} {r.race_number:<4} {r.field_size:<6}  ~ {fo}, {so:<15} [{top5}]\")\n                lines.append(\"-\" * 160)\n            self.logger.info(\"\\n\".join(lines))\n            return\n\n        lines.extend([\n            f\"{'SUPER':<6} {'MTP':<5} {'DISC':<5} {'TRACK':<20} {'R#':<4} {'FIELD':<6} {'ODDS':<20} {'TOP 5':<20}\",\n            \"-\" * 160\n        ])\n        for r in bet_now:\n            sup = \"\u2705\" if r.superfecta_offered else \"\u274c\"\n            fo = f\"{r.favorite_odds:.2f}\" if r.favorite_odds else \"N/A\"\n            so = f\"{r.second_fav_odds:.2f}\" if r.second_fav_odds else \"N/A\"\n            top5 = r.top_five_numbers or \"N/A\"\n            m_str = f\"{r.mtp:02d}\" if 0 <= r.mtp < 10 else str(r.mtp)\n            lines.append(f\"{sup:<6} {m_str:<5} {r.discipline:<5} {r.track[:19]:<20} {r.race_number:<4} {r.field_size:<6}  ~ {fo}, {so:<15} [{top5}]\")\n        lines.extend([\"-\" * 160, f\"Total opportunities: {len(bet_now)}\"])\n        self.logger.info(\"\\n\".join(lines))\n\n        # Include recent audited results to provide proof of system performance\n        history = await self.tracker.db.get_recent_audited_goldmines(limit=10)\n        if history:\n            historical_report = generate_historical_goldmine_report(history)\n            self.logger.info(historical_report)\n\n    def save_to_json(self, filename: str = \"race_data.json\"):\n        \"\"\"Export to JSON.\"\"\"\n        bn = self.get_bet_now_races()\n        yml = self.get_you_might_like_races()\n\n        if not bn:\n            self.logger.warning(\"\ud83d\udd2d Monitor found 0 BET NOW opportunities\", total_checked=len(self.golden_zone_races))\n            # Structured telemetry for monitoring\n            structlog.get_logger(\"FortunaTelemetry\").warning(\"empty_bet_now_list\", golden_zone_count=len(self.golden_zone_races))\n            # Create an indicator file for downstream monitoring (GPT5 Improvement)\n            try:\n                Path(\"monitor_empty.alert\").write_text(datetime.now(EASTERN).isoformat())\n            except Exception: pass\n        else:\n            # Clear alert if it exists\n            try:\n                alert_file = Path(\"monitor_empty.alert\")\n                if alert_file.exists(): alert_file.unlink()\n            except Exception: pass\n\n        data = {\n            \"generated_at\": datetime.now(EASTERN).isoformat(),\n            \"target_dates\": self.target_dates,\n            \"total_races\": len(self.all_races),\n            \"bet_now_count\": len(bn),\n            \"you_might_like_count\": len(yml),\n            \"all_races\": [r.to_dict() for r in self.all_races],\n            \"bet_now_races\": [r.to_dict() for r in bn],\n            \"you_might_like_races\": [r.to_dict() for r in yml],\n        }\n        try:\n            # Ensure parent directory exists (GPT5 Improvement)\n            Path(filename).parent.mkdir(parents=True, exist_ok=True)\n            with open(filename, 'w', encoding='utf-8') as f:\n                json.dump(data, f, indent=2)\n        except Exception as e:\n            self.logger.error(\"failed_saving_race_data\", path=filename, error=str(e))\n\n        # Persistent history log\n        self._append_to_history(bn + yml)\n\n    def _append_to_history(self, races: List[RaceSummary]):\n        \"\"\"Append races to persistent history for future result matching.\"\"\"\n        if not races: return\n        history_file = \"prediction_history.jsonl\"\n        timestamp = datetime.now(EASTERN).isoformat()\n        try:\n            with open(history_file, 'a') as f:\n                for r in races:\n                    record = r.to_dict()\n                    record[\"logged_at\"] = timestamp\n                    f.write(json.dumps(record) + \"\\n\")\n        except Exception as e:\n            self.logger.error(\"History logging failed\", error=str(e))\n\n    async def run_once(self, loaded_races: Optional[List[Race]] = None, adapter_names: Optional[List[str]] = None):\n        try:\n            if loaded_races is not None:\n                self.logger.info(\"Using loaded races\", count=len(loaded_races))\n                # Map to (Race, AdapterName) tuple expected by build_race_summaries\n                raw = [(r, r.source) for r in loaded_races]\n            else:\n                await self.initialize_adapters(adapter_names=adapter_names)\n                raw = await self.fetch_all_races()\n\n            await self.build_race_summaries(raw, window_hours=12) # Use 12h window for monitor\n            self.print_full_list()\n            await self.print_bet_now_list()\n            self.save_to_json()\n        finally:\n            for a in self.adapters: await a.shutdown()\n            await GlobalResourceManager.cleanup()\n\n    async def run_continuous(self):\n        await self.initialize_adapters()\n        raw = await self.fetch_all_races()\n        await self.build_race_summaries(raw, window_hours=12)\n        self.print_full_list()\n        try:\n            for _ in range(1000): # Iteration limit to prevent potential hangs\n                for r in self.all_races: r.mtp = self._calculate_mtp(r.start_time)\n                await self.print_bet_now_list()\n                self.save_to_json()\n                await asyncio.sleep(self.refresh_interval)\n        except KeyboardInterrupt:\n            self.logger.info(\"Stopped by user\")\n        finally:\n            for a in self.adapters: await a.shutdown()\n            await GlobalResourceManager.cleanup()\n",
      "name": "FavoriteToPlaceMonitor"
    },
    {
      "type": "miscellaneous",
      "content": "\n\n\n\n\n# ----------------------------------------\n# EXPANDED ADAPTERS\n# ----------------------------------------\n# python_service/adapters/oddschecker_adapter.py\n\n\n\n\n\n"
    },
    {
      "type": "class",
      "content": "class OddscheckerAdapter(BrowserHeadersMixin, DebugMixin, BaseAdapterV3):\n    \"\"\"Adapter for scraping horse racing odds from Oddschecker, migrated to BaseAdapterV3.\"\"\"\n\n    SOURCE_NAME = \"Oddschecker\"\n    BASE_URL = \"https://www.oddschecker.com\"\n\n    def __init__(self, config=None):\n        super().__init__(source_name=self.SOURCE_NAME, base_url=self.BASE_URL, config=config)\n\n    def _configure_fetch_strategy(self) -> FetchStrategy:\n        # Oddschecker is heavily protected by Cloudflare; Playwright with high timeout and network idle\n        return FetchStrategy(\n            primary_engine=BrowserEngine.PLAYWRIGHT,\n            enable_js=True,\n            stealth_mode=\"camouflage\",\n            timeout=120,\n            network_idle=True\n        )\n\n    async def make_request(self, method: str, url: str, **kwargs: Any) -> Any:\n        # Playwright doesn't use impersonate but SmartFetcher handles it now\n        return await super().make_request(method, url, **kwargs)\n\n    def _get_headers(self) -> dict:\n        return self._get_browser_headers(host=\"www.oddschecker.com\")\n\n    async def _fetch_data(self, date: str) -> Optional[dict]:\n        \"\"\"\n        Fetches the raw HTML for all race pages for a given date. This involves a multi-level fetch.\n        \"\"\"\n        index_url = f\"/horse-racing/{date}\"\n        index_response = await self.make_request(\"GET\", index_url, headers=self._get_headers())\n        if not index_response or not index_response.text:\n            self.logger.warning(\"Failed to fetch Oddschecker index page\", url=index_url)\n            return None\n\n        self._save_debug_html(index_response.text, f\"oddschecker_index_{date}\")\n\n        parser = HTMLParser(index_response.text)\n        # Find all links to individual race pages\n        metadata = []\n\n        try:\n            target_date = datetime.strptime(date, \"%Y-%m-%d\").date()\n        except Exception:\n            target_date = datetime.now(EASTERN).date()\n\n        site_tz = ZoneInfo(\"Europe/London\")\n        now_site = datetime.now(site_tz)\n\n        # Group by track to pick \"next\" race\n        track_map = defaultdict(list)\n\n        # Broaden selectors for race links\n        for selector in [\"a.race-time-link[href]\", \"a[href*='/horse-racing/'][href*='/20']\", \".rf__link\"]:\n            for a in parser.css(selector):\n                href = a.attributes.get(\"href\")\n                if href and not href.endswith(\"/horse-racing\"):\n                    # Ensure absolute URL\n                    full_url = href if href.startswith(\"http\") else f\"{self.BASE_URL}{href}\"\n\n                    # Extract track from URL if possible, or use parent\n                    # URL usually /horse-racing/venue/date/time\n                    parts = full_url.split(\"/\")\n                    if len(parts) >= 6:\n                        track = parts[4]\n                        txt = node_text(a) # Time is often in text\n                        track_map[track].append({\"url\": full_url, \"time_txt\": txt})\n\n        for track, races in track_map.items():\n            for r in races:\n                if re.match(r\"\\d{1,2}:\\d{2}\", r[\"time_txt\"]):\n                    try:\n                        rt = datetime.strptime(r[\"time_txt\"], \"%H:%M\").replace(\n                            year=target_date.year, month=target_date.month, day=target_date.day, tzinfo=site_tz\n                        )\n                        # Broaden window to capture multiple races (Memory Directive Fix)\n                        diff = (rt - now_site).total_seconds() / 60\n                        if not (-45 < diff <= 1080):\n                            continue\n\n                        metadata.append(r[\"url\"])\n                    except Exception: pass\n\n        if not metadata:\n            self.logger.warning(\"No metadata found\", context=\"Oddschecker Index Parsing\", url=index_url)\n            return None\n\n        async def fetch_single_html(url_path: str):\n            response = await self.make_request(\"GET\", url_path, headers=self._get_headers())\n            return response.text if response else \"\"\n\n        tasks = [fetch_single_html(link) for link in metadata]\n        html_pages = await asyncio.gather(*tasks)\n        return {\"pages\": html_pages, \"date\": date}\n\n    def _parse_races(self, raw_data: Any) -> List[Race]:\n        \"\"\"Parses a list of raw HTML strings from different races into Race objects.\"\"\"\n        if not raw_data or not raw_data.get(\"pages\"):\n            return []\n\n        try:\n            race_date = datetime.strptime(raw_data[\"date\"], \"%Y-%m-%d\").date()\n        except ValueError:\n            self.logger.error(\n                \"Invalid date format provided to OddscheckerAdapter\",\n                date=raw_data.get(\"date\"),\n            )\n            return []\n\n        all_races = []\n        for html in raw_data[\"pages\"]:\n            if not html:\n                continue\n            try:\n                parser = HTMLParser(html)\n                race = self._parse_race_page(parser, race_date)\n                if race:\n                    all_races.append(race)\n            except (AttributeError, IndexError, ValueError):\n                self.logger.warning(\n                    \"Error parsing a race from Oddschecker, skipping race.\",\n                    exc_info=True,\n                )\n                continue\n        return all_races\n\n    def _parse_race_page(self, parser: HTMLParser, race_date) -> Optional[Race]:\n        track_name_node = parser.css_first(\"h1.meeting-name\")\n        if not track_name_node:\n            return None\n        track_name = track_name_node.text(strip=True)\n\n        race_time_node = parser.css_first(\"span.race-time\")\n        if not race_time_node:\n            return None\n        race_time_str = race_time_node.text(strip=True)\n\n        # Heuristic to find race number from navigation\n        active_link = parser.css_first(\"a.race-time-link.active\")\n        race_number = 0\n        if active_link:\n            all_links = parser.css(\"a.race-time-link\")\n            try:\n                for i, link in enumerate(all_links):\n                    if link.html == active_link.html:\n                        race_number = i + 1\n                        break\n            except Exception:\n                pass\n\n        start_time = datetime.combine(race_date, datetime.strptime(race_time_str, \"%H:%M\").time())\n        runners = [runner for row in parser.css(\"tr.race-card-row\") if (runner := self._parse_runner_row(row))]\n\n        if not runners:\n            return None\n\n        return Race(\n            id=f\"oc_{track_name.lower().replace(' ', '')}_{start_time.strftime('%Y%m%d')}_r{race_number}\",\n            venue=track_name,\n            race_number=race_number,\n            start_time=start_time,\n            runners=runners,\n            source=self.source_name,\n        )\n\n    def _parse_runner_row(self, row: Node) -> Optional[Runner]:\n        try:\n            name_node = row.css_first(\"span.selection-name\")\n            if not name_node:\n                return None\n            name = name_node.text(strip=True)\n\n            odds_node = row.css_first(\"span.bet-button-odds-desktop, span.best-price\")\n            if not odds_node:\n                return None\n            odds_str = odds_node.text(strip=True)\n\n            number_node = row.css_first(\"td.runner-number\")\n            number = 0\n            if number_node:\n                num_txt = \"\".join(filter(str.isdigit, number_node.text(strip=True)))\n                if num_txt:\n                    number = int(num_txt)\n\n            if not name or not odds_str:\n                return None\n\n            win_odds = parse_odds_to_decimal(odds_str)\n            odds_dict = {}\n            if odds_data := create_odds_data(self.source_name, win_odds):\n                odds_dict[self.source_name] = odds_data\n\n            return Runner(number=number, name=name, odds=odds_dict)\n        except (AttributeError, ValueError):\n            self.logger.warning(\"Failed to parse a runner on Oddschecker, skipping runner.\")\n            return None\n",
      "name": "OddscheckerAdapter"
    },
    {
      "type": "miscellaneous",
      "content": "\n# python_service/adapters/timeform_adapter.py\n\n\n\n\n\n"
    },
    {
      "type": "class",
      "content": "class TimeformAdapter(JSONParsingMixin, BrowserHeadersMixin, DebugMixin, BaseAdapterV3):\n    \"\"\"\n    Adapter for timeform.com, migrated to BaseAdapterV3 and standardized on selectolax.\n    \"\"\"\n\n    SOURCE_NAME = \"Timeform\"\n    BASE_URL = \"https://www.timeform.com\"\n\n    def __init__(self, config=None):\n        super().__init__(source_name=self.SOURCE_NAME, base_url=self.BASE_URL, config=config)\n        self._semaphore = asyncio.Semaphore(5)\n\n    def _configure_fetch_strategy(self) -> FetchStrategy:\n        # Timeform often blocks basic requests; Playwright is robust\n        return FetchStrategy(\n            primary_engine=BrowserEngine.PLAYWRIGHT,\n            enable_js=True,\n            stealth_mode=\"camouflage\",\n            timeout=90,\n            network_idle=True\n        )\n\n    def _get_headers(self) -> dict:\n        headers = self._get_browser_headers(host=\"www.timeform.com\")\n        headers.update({\n            \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8\",\n            \"Accept-Language\": \"en-US,en;q=0.9\",\n        })\n        return headers\n\n    async def _fetch_data(self, date: str) -> Optional[dict]:\n        \"\"\"\n        Fetches the raw HTML for all race pages for a given date.\n        \"\"\"\n        index_url = f\"/horse-racing/racecards/{date}\"\n        index_response = await self.make_request(\"GET\", index_url, headers=self._get_headers())\n        if not index_response or not index_response.text:\n            self.logger.warning(\"Failed to fetch Timeform index page\", url=index_url)\n            return None\n\n        self._save_debug_snapshot(index_response.text, f\"timeform_index_{date}\")\n\n        parser = HTMLParser(index_response.text)\n        # Updated selector for race links\n        try:\n            target_date = datetime.strptime(date, \"%Y-%m-%d\").date()\n        except Exception:\n            target_date = datetime.now(EASTERN).date()\n\n        site_tz = ZoneInfo(\"Europe/London\")\n        now_site = datetime.now(site_tz)\n\n        track_map = defaultdict(list)\n        # Broaden selectors for Timeform race links\n        for selector in [\"a[href*='/racecards/']\", \".rf__link\", \"a.rf-meeting-race__time\", \".rp-meetingItem__race__time\"]:\n            for a in parser.css(selector):\n                href = a.attributes.get(\"href\")\n                if href and \"/racecards/\" in href and not href.endswith(\"/racecards\"):\n                    # URL usually: /horse-racing/racecards/venue/date/time/...\n                    # or: /racecards/venue/date/time\n                    parts = href.split(\"/\")\n                    # Handle both relative and absolute-ish paths\n                    track = \"unknown\"\n                    for i, p in enumerate(parts):\n                        if p == \"racecards\" and i + 1 < len(parts):\n                            track = parts[i+1]\n                            break\n\n                    txt = node_text(a)\n                    track_map[track].append({\"url\": href, \"time_txt\": txt})\n\n        links = []\n        for track, races in track_map.items():\n            for r in races:\n                # Timeform often uses HH:MM in text\n                time_match = re.search(r\"(\\d{1,2}:\\d{2})\", r[\"time_txt\"])\n                if time_match:\n                    try:\n                        rt = datetime.strptime(time_match.group(1), \"%H:%M\").replace(\n                            year=target_date.year, month=target_date.month, day=target_date.day, tzinfo=site_tz\n                        )\n                        # Broaden window to capture multiple races (Memory Directive Fix)\n                        diff = (rt - now_site).total_seconds() / 60\n                        if not (-45 < diff <= 1080):\n                            continue\n\n                        full_url = r[\"url\"] if r[\"url\"].startswith(\"http\") else f\"{self.BASE_URL}{r['url']}\"\n                        links.append(full_url)\n                    except Exception: pass\n\n        if not links:\n            self.logger.warning(\"No metadata found\", context=\"Timeform Index Parsing\", url=index_url)\n            return None\n\n        async def fetch_single_html(url_path: str):\n            async with self._semaphore:\n                await asyncio.sleep(0.5)\n                response = await self.make_request(\"GET\", url_path, headers=self._get_headers())\n                return (url_path, response.text) if response else (url_path, \"\")\n\n        self.logger.info(f\"Found {len(links)} race links on Timeform\")\n        tasks = [fetch_single_html(link) for link in links]\n        results = await asyncio.gather(*tasks)\n        return {\"pages\": [r for r in results if r[1]], \"date\": date}\n\n    def _parse_races(self, raw_data: Any) -> List[Race]:\n        \"\"\"Parses a list of raw HTML strings into Race objects.\"\"\"\n        if not raw_data or not raw_data.get(\"pages\"):\n            return []\n\n        try:\n            race_date = datetime.strptime(raw_data[\"date\"], \"%Y-%m-%d\").date()\n        except ValueError:\n            self.logger.error(\"Invalid date format\", date=raw_data.get(\"date\"))\n            return []\n\n        all_races = []\n        for url_path, html_content in raw_data[\"pages\"]:\n            if not html_content:\n                continue\n            try:\n                parser = HTMLParser(html_content)\n\n                # Extract via JSON-LD if possible\n                venue = \"\"\n                start_time = None\n                scripts = self._parse_all_jsons_from_scripts(parser, 'script[type=\"application/ld+json\"]', context=\"Betfair Index\")\n                for data in scripts:\n                    if data.get(\"@type\") == \"Event\":\n                        venue = normalize_venue_name(data.get(\"location\", {}).get(\"name\", \"\"))\n                        if sd := data.get(\"startDate\"):\n                            # 2026-01-28T14:32:00\n                            start_time = datetime.fromisoformat(sd.split('+')[0])\n                        break\n\n                if not venue:\n                    # Fallback to title\n                    title = parser.css_first(\"title\")\n                    if title:\n                        # 14:32 DUNDALK | Races 28 January 2026 ...\n                        match = re.search(r'(\\d{1,2}:\\d{2})\\s+([^|]+)', title.text())\n                        if match:\n                            time_str = match.group(1)\n                            venue = normalize_venue_name(match.group(2).strip())\n                            start_time = datetime.combine(race_date, datetime.strptime(time_str, \"%H:%M\").time())\n\n                if not venue or not start_time:\n                    continue\n\n                # Betting Forecast Parsing\n                forecast_map = {}\n                verdict_section = parser.css_first(\"section.rp-verdict\")\n                if verdict_section:\n                    forecast_text = clean_text(verdict_section.text())\n                    if \"Betting Forecast :\" in forecast_text:\n                        # \"Betting Forecast : 15/8 2.87 Spring Is Here, 3/1 4 This Guy, ...\"\n                        after_forecast = forecast_text.split(\"Betting Forecast :\")[1]\n                        # Split by comma\n                        parts = after_forecast.split(',')\n                        for part in parts:\n                            # Match odds and then name\n                            # Odds can be fractional space decimal\n                            m = re.search(r'(\\d+/\\d+|EVENS)\\s+([\\d\\.]+)?\\s*(.+)', part.strip())\n                            if m:\n                                odds_str = m.group(1)\n                                name = clean_text(m.group(3))\n                                forecast_map[name.lower()] = odds_str\n\n                # Runners\n                runners = []\n                # Use tbody as the main container for each runner\n                for row in parser.css('tbody.rp-horse-row'):\n                    if runner := self._parse_runner(row, forecast_map):\n                        runners.append(runner)\n\n                if not runners:\n                    continue\n\n                # Race number from URL or sequence\n                race_number = 0\n                num_match = re.search(r'/(\\d+)/([^/]+)$', url_path)\n                # .../1432/207/1/view... -> the '1' is the race number\n                url_parts = url_path.split('/')\n                if len(url_parts) >= 10:\n                    try: race_number = int(url_parts[9])\n                    except Exception: pass\n\n                race = Race(\n                    id=f\"tf_{venue.lower().replace(' ', '')}_{start_time:%Y%m%d}_R{race_number}\",\n                    venue=venue,\n                    race_number=race_number,\n                    start_time=start_time,\n                    runners=runners,\n                    source=self.source_name,\n                )\n                all_races.append(race)\n            except Exception as e:\n                self.logger.warning(f\"Error parsing Timeform race: {e}\")\n                continue\n        return all_races\n\n    def _parse_runner(self, row: Node, forecast_map: dict = None) -> Optional[Runner]:\n        \"\"\"Parses a single runner from a table row node.\"\"\"\n        try:\n            name_node = row.css_first(\"a.rp-horse\") or row.css_first(\"a.rp-horseTable_horse-name\")\n            if not name_node:\n                return None\n            name = clean_text(name_node.text())\n\n            number = 0\n            num_attr = row.attributes.get(\"data-entrynumber\")\n            if num_attr:\n                try:\n                    val = int(num_attr)\n                    if val <= 40: number = val\n                except Exception:\n                    pass\n\n            if not number:\n                num_node = row.css_first(\".rp-entry-number\") or row.css_first(\"span.rp-horseTable_horse-number\")\n                if num_node:\n                    num_text = clean_text(num_node.text()).strip(\"()\")\n                    num_match = re.search(r\"\\d+\", num_text)\n                    if num_match:\n                        val = int(num_match.group())\n                        if val <= 40: number = val\n\n            win_odds = None\n            if forecast_map:\n                win_odds = parse_odds_to_decimal(forecast_map.get(name.lower()))\n\n            # Try to find live odds button if available (old selector)\n            if not win_odds:\n                odds_tag = row.css_first(\"button.rp-bet-placer-btn__odds\")\n                if odds_tag:\n                    win_odds = parse_odds_to_decimal(clean_text(odds_tag.text()))\n\n            odds_data = {}\n            if odds_val := create_odds_data(self.source_name, win_odds):\n                odds_data[self.source_name] = odds_val\n\n            return Runner(number=number, name=name, odds=odds_data)\n        except (AttributeError, ValueError, TypeError):\n            return None\n",
      "name": "TimeformAdapter"
    },
    {
      "type": "miscellaneous",
      "content": "\n# python_service/adapters/racingpost_adapter.py\n\n\n\n\n"
    },
    {
      "type": "class",
      "content": "class RacingPostAdapter(BrowserHeadersMixin, DebugMixin, BaseAdapterV3):\n    \"\"\"\n    Adapter for scraping Racing Post racecards, migrated to BaseAdapterV3.\n    \"\"\"\n\n    SOURCE_NAME = \"RacingPost\"\n    BASE_URL = \"https://www.racingpost.com\"\n\n    def __init__(self, config=None):\n        super().__init__(source_name=self.SOURCE_NAME, base_url=self.BASE_URL, config=config)\n\n    def _configure_fetch_strategy(self) -> FetchStrategy:\n        # RacingPost has strong anti-bot measures. Playwright with stealth is usually the best bet.\n        return FetchStrategy(\n            primary_engine=BrowserEngine.PLAYWRIGHT,\n            enable_js=True,\n            stealth_mode=\"camouflage\",\n            timeout=90,\n            block_resources=False,\n            network_idle=True\n        )\n\n    def _get_headers(self) -> dict:\n        return self._get_browser_headers(host=\"www.racingpost.com\")\n\n    async def _fetch_data(self, date: str) -> Any:\n        \"\"\"\n        Fetches the raw HTML content for all races on a given date, including international.\n        \"\"\"\n        index_url = f\"/racecards/{date}\"\n        # RacingPost international URL sometimes varies\n        intl_urls = [\n            f\"/racecards/international/{date}\",\n            f\"/racecards/{date}/international\",\n            \"/racecards/international\"\n        ]\n\n        index_response = await self.make_request(\"GET\", index_url, headers=self._get_headers())\n\n        intl_response = None\n        for url in intl_urls:\n            resp = await self.make_request(\"GET\", url, headers=self._get_headers())\n            if resp and resp.status == 200:\n                intl_response = resp\n                break\n\n        race_card_urls = []\n        try:\n            target_date = datetime.strptime(date, \"%Y-%m-%d\").date()\n        except Exception:\n            target_date = datetime.now(EASTERN).date()\n\n        site_tz = ZoneInfo(\"Europe/London\")\n        now_site = datetime.now(site_tz)\n\n        if index_response and index_response.text:\n            self._save_debug_html(index_response.text, f\"racingpost_index_{date}\")\n            index_parser = HTMLParser(index_response.text)\n\n            # Broaden window to capture multiple races (Memory Directive Fix)\n            meetings = index_parser.css('.rp-raceCourse__panel') or index_parser.css('.RC-meetingItem') or index_parser.css('.rp-meetingItem') or index_parser.css('.RC-courseCards')\n            for meeting in meetings:\n                # Broaden a tag selectors to catch new Racing Post structures\n                for link in meeting.css('a[data-test-selector^=\"RC-meetingItem__link_race\"], a.rp-raceCourse__panel__race__time, a.rp-meetingItem__race__time, a.RC-meetingItem__race__time, a.RC-meetingItem__link, a[href*=\"/racecards/\"]'):\n                    href = link.attributes.get(\"href\", \"\")\n                    if not href or \"/results/\" in href:\n                        continue\n\n                    txt = clean_text(node_text(link))\n                    time_match = re.search(r\"(\\d{1,2}:\\d{2})\", txt)\n                    if time_match:\n                        try:\n                            time_str = time_match.group(1)\n                            tm = datetime.strptime(time_str, \"%H:%M\")\n                            if tm.hour < 9:\n                                tm = tm.replace(hour=tm.hour + 12)\n\n                            rt = tm.replace(\n                                year=target_date.year, month=target_date.month, day=target_date.day, tzinfo=site_tz\n                            )\n                            diff = (rt - now_site).total_seconds() / 60\n                            if not (-45 < diff <= 1080):\n                                continue\n                        except Exception: pass\n\n                    race_card_urls.append(href)\n\n        elif index_response:\n            self.logger.warning(\"Unexpected status\", status=index_response.status, url=index_url)\n\n        if intl_response and intl_response.text:\n            self._save_debug_html(intl_response.text, f\"racingpost_intl_index_{date}\")\n            intl_parser = HTMLParser(intl_response.text)\n\n            meetings = intl_parser.css('.rp-raceCourse__panel') or intl_parser.css('.RC-meetingItem') or intl_parser.css('.rp-meetingItem') or intl_parser.css('.RC-courseCards')\n            for meeting in meetings:\n                for link in meeting.css('a[data-test-selector^=\"RC-meetingItem__link_race\"], a.rp-raceCourse__panel__race__time, a.rp-meetingItem__race__time, a.RC-meetingItem__race__time, a.RC-meetingItem__link, a[href*=\"/racecards/\"]'):\n                    href = link.attributes.get(\"href\", \"\")\n                    if not href or \"/results/\" in href:\n                        continue\n\n                    txt = clean_text(node_text(link))\n                    time_match = re.search(r\"(\\d{1,2}:\\d{2})\", txt)\n                    if time_match:\n                        try:\n                            time_str = time_match.group(1)\n                            tm = datetime.strptime(time_str, \"%H:%M\")\n                            if tm.hour < 9:\n                                tm = tm.replace(hour=tm.hour + 12)\n\n                            rt = tm.replace(\n                                year=target_date.year, month=target_date.month, day=target_date.day, tzinfo=site_tz\n                            )\n                            diff = (rt - now_site).total_seconds() / 60\n                            if not (-45 < diff <= 1080):\n                                continue\n                        except Exception: pass\n\n                    race_card_urls.append(href)\n        elif intl_response:\n            self.logger.warning(\"Unexpected status\", status=intl_response.status, url=intl_url)\n\n        if not race_card_urls:\n            self.logger.warning(\"Standard RacingPost link discovery failed, trying aggressive fallback\", date=date)\n            if index_response and index_response.text:\n                index_parser = HTMLParser(index_response.text)\n                for a in index_parser.css('a[href*=\"/racecards/\"]'):\n                    href = a.attributes.get(\"href\", \"\")\n                    if re.search(r\"/\\d+/.*/\\d{4}-\\d{2}-\\d{2}/\\d+\", href):\n                        race_card_urls.append(href)\n\n            if intl_response and intl_response.text:\n                intl_parser = HTMLParser(intl_response.text)\n                for a in intl_parser.css('a[href*=\"/racecards/\"]'):\n                    href = a.attributes.get(\"href\", \"\")\n                    if re.search(r\"/\\d+/.*/\\d{4}-\\d{2}-\\d{2}/\\d+\", href):\n                        race_card_urls.append(href)\n\n        if not race_card_urls:\n            self.logger.warning(\"Failed to fetch RacingPost racecard links\", date=date)\n            return None\n\n        async def fetch_single_html(url: str):\n            response = await self.make_request(\"GET\", url, headers=self._get_headers())\n            return response.text if response else \"\"\n\n        tasks = [fetch_single_html(url) for url in race_card_urls]\n        html_contents = await asyncio.gather(*tasks)\n        return {\"date\": date, \"html_contents\": html_contents}\n\n    def _parse_races(self, raw_data: Any) -> List[Race]:\n        \"\"\"Parses a list of raw HTML strings into Race objects.\"\"\"\n        if not raw_data or not raw_data.get(\"html_contents\"):\n            return []\n\n        date = raw_data[\"date\"]\n        html_contents = raw_data[\"html_contents\"]\n        all_races: List[Race] = []\n\n        for html in html_contents:\n            if not html:\n                continue\n            try:\n                parser = HTMLParser(html)\n\n                venue_node = parser.css_first('a[data-test-selector=\"RC-course__name\"]')\n                if not venue_node:\n                    continue\n                venue_raw = venue_node.text(strip=True)\n                venue = normalize_venue_name(venue_raw)\n\n                race_time_node = parser.css_first('span[data-test-selector=\"RC-course__time\"]')\n                if not race_time_node:\n                    continue\n                race_time_str = race_time_node.text(strip=True)\n\n                race_datetime_str = f\"{date} {race_time_str}\"\n                start_time = datetime.strptime(race_datetime_str, \"%Y-%m-%d %H:%M\")\n\n                runners = self._parse_runners(parser)\n\n                if venue and runners:\n                    race_number = self._get_race_number(parser, start_time)\n                    race = Race(\n                        id=f\"rp_{venue.lower().replace(' ', '')}_{date}_{race_number}\",\n                        venue=venue,\n                        race_number=race_number,\n                        start_time=start_time,\n                        runners=runners,\n                        source=self.source_name,\n                    )\n                    all_races.append(race)\n            except (AttributeError, ValueError):\n                self.logger.error(\"Failed to parse RacingPost race from HTML content.\", exc_info=True)\n                continue\n        return all_races\n\n    def _get_race_number(self, parser: HTMLParser, start_time: datetime) -> int:\n        \"\"\"Derives the race number by finding the active time in the nav bar.\"\"\"\n        time_str_to_find = start_time.strftime(\"%H:%M\")\n        time_links = parser.css('a[data-test-selector=\"RC-raceTime\"]')\n        for i, link in enumerate(time_links):\n            if link.text(strip=True) == time_str_to_find:\n                return i + 1\n        return 1\n\n    def _parse_runners(self, parser: HTMLParser) -> list[Runner]:\n        \"\"\"Parses all runners from a single race card page.\"\"\"\n        runners = []\n        runner_nodes = parser.css('div[data-test-selector=\"RC-runnerCard\"]')\n        for node in runner_nodes:\n            if runner := self._parse_runner(node):\n                runners.append(runner)\n        return runners\n\n    def _parse_runner(self, node: Node) -> Optional[Runner]:\n        try:\n            number_node = node.css_first('span[data-test-selector=\"RC-runnerNumber\"]')\n            name_node = node.css_first('a[data-test-selector=\"RC-runnerName\"]')\n            odds_node = node.css_first('span[data-test-selector=\"RC-runnerPrice\"]')\n\n            if not all([number_node, name_node, odds_node]):\n                return None\n\n            number_str = clean_text(number_node.text())\n            number = 0\n            if number_str:\n                num_txt = \"\".join(filter(str.isdigit, number_str))\n                if num_txt:\n                    val = int(num_txt)\n                    if val <= 40: number = val\n            name = clean_text(name_node.text())\n            odds_str = clean_text(odds_node.text())\n            scratched = \"NR\" in odds_str.upper() or not odds_str\n\n            odds = {}\n            if not scratched:\n                win_odds = parse_odds_to_decimal(odds_str)\n                if odds_data := create_odds_data(self.source_name, win_odds):\n                    odds[self.source_name] = odds_data\n\n            return Runner(number=number, name=name, odds=odds, scratched=scratched)\n        except (ValueError, AttributeError):\n            self.logger.warning(\"Could not parse RacingPost runner, skipping.\", exc_info=True)\n            return None\n",
      "name": "RacingPostAdapter"
    },
    {
      "type": "miscellaneous",
      "content": "\n\n"
    },
    {
      "type": "class",
      "content": "class RacingPostToteAdapter(BrowserHeadersMixin, DebugMixin, BaseAdapterV3):\n    \"\"\"\n    Adapter for fetching Tote dividends and results from Racing Post.\n    \"\"\"\n    ADAPTER_TYPE = \"results\"\n    SOURCE_NAME = \"RacingPostTote\"\n    BASE_URL = \"https://www.racingpost.com\"\n\n    def __init__(self, config: Optional[Dict[str, Any]] = None):\n        super().__init__(source_name=self.SOURCE_NAME, base_url=self.BASE_URL, config=config)\n\n    def _configure_fetch_strategy(self) -> FetchStrategy:\n        return FetchStrategy(\n            primary_engine=BrowserEngine.CURL_CFFI,\n            enable_js=True,\n            stealth_mode=StealthMode.CAMOUFLAGE,\n            timeout=45\n        )\n\n    def _get_headers(self) -> dict:\n        return self._get_browser_headers(host=\"www.racingpost.com\")\n\n    async def _fetch_data(self, date: str) -> Optional[Dict[str, Any]]:\n        url = f\"/results/{date}\"\n        resp = await self.make_request(\"GET\", url, headers=self._get_headers())\n        if not resp or not resp.text:\n            return None\n\n        self._save_debug_snapshot(resp.text, f\"rp_tote_results_{date}\")\n        parser = HTMLParser(resp.text)\n\n        # Extract links to individual race results\n        links = set()\n        selectors = [\n            'a[data-test-selector=\"RC-meetingItem__link_race\"]',\n            'a[href*=\"/results/\"]',\n            '.ui-link.rp-raceCourse__panel__race__time',\n            'a.rp-raceCourse__panel__race__time'\n        ]\n        target_venues = getattr(self, \"target_venues\", None)\n        for s in selectors:\n            for a in parser.css(s):\n                href = a.attributes.get(\"href\")\n                if href:\n                    # Filter by venue\n                    if target_venues:\n                        match_found = False\n                        for v in target_venues:\n                            if v in href.lower().replace(\"-\", \"\"):\n                                match_found = True\n                                break\n                        if not match_found:\n                            v_text = get_canonical_venue(node_text(a))\n                            if v_text in target_venues:\n                                match_found = True\n                        if not match_found:\n                            continue\n\n                    # Broaden regex to match various RP result link patterns (Memory Directive Fix)\n                    if re.search(r\"/results/.*?\\d{5,}\", href) or \\\n                       re.search(r\"/results/\\d+/\", href) or \\\n                       re.search(r\"/\\d{4}-\\d{2}-\\d{2}/\", href) or \\\n                       len(href.split(\"/\")) >= 4:\n                        links.add(href if href.startswith(\"http\") else f\"{self.BASE_URL}{href}\")\n\n        async def fetch_result_page(link):\n            r = await self.make_request(\"GET\", link, headers=self._get_headers())\n            return (link, r.text if r else \"\")\n\n        tasks = [fetch_result_page(link) for link in links]\n        pages = await asyncio.gather(*tasks)\n        return {\"pages\": pages, \"date\": date}\n\n    def _parse_races(self, raw_data: Any) -> List[Race]:\n        if not raw_data or not raw_data.get(\"pages\"):\n            return []\n\n        races = []\n        date_str = raw_data[\"date\"]\n\n        for link, html_content in raw_data[\"pages\"]:\n            if not html_content:\n                continue\n            try:\n                parser = HTMLParser(html_content)\n                race = self._parse_result_page(parser, date_str, link)\n                if race:\n                    races.append(race)\n            except Exception as e:\n                self.logger.warning(\"Failed to parse RP result page\", link=link, error=str(e))\n\n        return races\n\n    def _parse_result_page(self, parser: HTMLParser, date_str: str, url: str) -> Optional[Race]:\n        venue_node = parser.css_first('a[data-test-selector=\"RC-course__name\"]')\n        if not venue_node: return None\n        venue = normalize_venue_name(venue_node.text(strip=True))\n\n        time_node = parser.css_first('span[data-test-selector=\"RC-course__time\"]')\n        if not time_node: return None\n        time_str = time_node.text(strip=True)\n\n        try:\n            start_time = datetime.strptime(f\"{date_str} {time_str}\", \"%Y-%m-%d %H:%M\").replace(tzinfo=EASTERN)\n        except Exception:\n            return None\n\n        # Extract dividends\n        dividends = {}\n        tote_container = parser.css_first('div[data-test-selector=\"RC-toteReturns\"]')\n        if not tote_container:\n             # Try alternate selector\n             tote_container = parser.css_first('.rp-toteReturns')\n\n        if tote_container:\n            for row in (tote_container.css('div.rp-toteReturns__row') or tote_container.css('.rp-toteReturns__row')):\n                try:\n                    label_node = row.css_first('div.rp-toteReturns__label') or row.css_first('.rp-toteReturns__label')\n                    val_node = row.css_first('div.rp-toteReturns__value') or row.css_first('.rp-toteReturns__value')\n                    if label_node and val_node:\n                        label = clean_text(label_node.text())\n                        value = clean_text(val_node.text())\n                        if label and value:\n                            dividends[label] = value\n                except Exception as e:\n                    self.logger.debug(\"Failed parsing RP tote row\", error=str(e))\n\n\n\n        # Extract runners (finishers)\n        runners = []\n        for row in parser.css('div[data-test-selector=\"RC-resultRunner\"]'):\n            name_node = row.css_first('a[data-test-selector=\"RC-resultRunnerName\"]')\n            if not name_node: continue\n            name = clean_text(name_node.text())\n            pos_node = row.css_first('span.rp-resultRunner__position')\n            pos = clean_text(pos_node.text()) if pos_node else \"?\"\n\n            # Try to find saddle number\n            number = 0\n            num_node = row.css_first(\".rp-resultRunner__saddleClothNo\")\n            if num_node:\n                try: number = int(clean_text(num_node.text()))\n                except Exception: pass\n\n            runners.append(Runner(\n                name=name,\n                number=number,\n                metadata={\"position\": pos}\n            ))\n\n        # Derive race number from header or navigation\n        race_num = 1\n        # Priority 1: Navigation bar active time (most reliable on RP)\n        time_links = parser.css('a[data-test-selector=\"RC-raceTime\"]')\n        found_in_nav = False\n        for i, link in enumerate(time_links):\n            cls = link.attributes.get(\"class\", \"\")\n            if \"active\" in cls or \"rp-raceTimeCourseName__time\" in cls:\n                race_num = i + 1\n                found_in_nav = True\n                break\n\n        if not found_in_nav:\n            # Priority 2: Text search for \"Race X\"\n            race_num_match = re.search(r'Race\\s+(\\d+)', parser.text())\n            if race_num_match:\n                race_num = int(race_num_match.group(1))\n\n        race = Race(\n            id=f\"rp_tote_{get_canonical_venue(venue)}_{date_str.replace('-', '')}_R{race_num}\",\n            venue=venue,\n            race_number=race_num,\n            start_time=start_time,\n            runners=runners,\n            source=self.source_name,\n            metadata={\"dividends\": dividends, \"url\": url}\n        )\n        return race\n",
      "name": "RacingPostToteAdapter"
    },
    {
      "type": "miscellaneous",
      "content": "\n# ----------------------------------------\n# MASTER ORCHESTRATOR\n# ----------------------------------------\n\n"
    },
    {
      "type": "async_function",
      "content": "async def run_discovery(\n    target_dates: List[str],\n    window_hours: Optional[int] = 8,\n    loaded_races: Optional[List[Race]] = None,\n    adapter_names: Optional[List[str]] = None,\n    save_path: Optional[str] = None,\n    fetch_only: bool = False,\n    live_dashboard: bool = False,\n    track_odds: bool = False,\n    region: Optional[str] = None,\n    config: Optional[Dict[str, Any]] = None\n):\n    logger = structlog.get_logger(\"run_discovery\")\n    logger.info(\"Running Discovery\", dates=target_dates, window_hours=window_hours)\n\n    try:\n        now = datetime.now(EASTERN)\n        cutoff = now + timedelta(hours=window_hours) if window_hours else None\n\n        all_races_raw = []\n        harvest_summary = {}\n\n        # Pre-populate harvest_summary based on region/filter for visibility\n        target_region = region or DEFAULT_REGION\n        target_set = USA_DISCOVERY_ADAPTERS if target_region == \"USA\" else INT_DISCOVERY_ADAPTERS\n\n        # Determine which adapters should be visible in the harvest summary\n        if adapter_names:\n            visible_adapters = [n for n in adapter_names if n in target_set]\n        else:\n            visible_adapters = list(target_set)\n\n        for adapter_name in visible_adapters:\n            harvest_summary[adapter_name] = {\"count\": 0, \"max_odds\": 0.0, \"trust_ratio\": 0.0}\n\n        if loaded_races is not None:\n            logger.info(\"Using loaded races\", count=len(loaded_races))\n            all_races_raw = loaded_races\n            adapters = []\n            # Ensure harvest files exist even for loaded runs (Memory Directive Fix)\n            try:\n                if not os.path.exists(\"discovery_harvest.json\"):\n                    with open(\"discovery_harvest.json\", \"w\") as f:\n                        json.dump(harvest_summary, f)\n            except Exception: pass\n        else:\n            # Auto-discover discovery adapter classes\n            adapter_classes = get_discovery_adapter_classes()\n\n            if adapter_names:\n                adapter_classes = [c for c in adapter_classes if c.__name__ in adapter_names or getattr(c, \"SOURCE_NAME\", \"\") in adapter_names]\n\n            # Load historical performance scores to prioritize adapters\n            db = FortunaDB()\n            adapter_scores = await db.get_adapter_scores(days=30)\n\n            # Prioritize adapters by score (descending)\n            adapter_classes = sorted(\n                adapter_classes,\n                key=lambda c: adapter_scores.get(getattr(c, \"SOURCE_NAME\", c.__name__), 0),\n                reverse=True\n            )\n\n            adapters = []\n            for cls in adapter_classes:\n                try:\n                    adapters.append(cls(config={\"region\": region}))\n                except Exception as e:\n                    logger.error(\"Failed to initialize adapter\", adapter=cls.__name__, error=str(e))\n\n            try:\n                async def fetch_one(a, date_str):\n                    try:\n                        races = await a.get_races(date_str)\n                        return a.source_name, races\n                    except Exception as e:\n                        logger.error(\"Error fetching from adapter\", adapter=a.source_name, date=date_str, error=str(e))\n                        return a.source_name, []\n\n                fetch_tasks = []\n                for d in target_dates:\n                    for a in adapters:\n                        fetch_tasks.append(fetch_one(a, d))\n\n                results = await asyncio.gather(*fetch_tasks)\n                for adapter_name, r_list in results:\n                    all_races_raw.extend(r_list)\n\n                    # Track count and MaxOdds (Proxy for successful odds fetching)\n                    m_odds = 0.0\n                    for r in r_list:\n                        for run in r.runners:\n                            if run.win_odds and run.win_odds > m_odds:\n                                m_odds = float(run.win_odds)\n\n                    if adapter_name not in harvest_summary:\n                        harvest_summary[adapter_name] = {\"count\": 0, \"max_odds\": 0.0}\n\n                    harvest_summary[adapter_name][\"count\"] += len(r_list)\n                    if m_odds > harvest_summary[adapter_name][\"max_odds\"]:\n                        harvest_summary[adapter_name][\"max_odds\"] = m_odds\n\n                    # Find the adapter instance to extract its trust_ratio\n                    matching_adapter = next((a for a in adapters if a.source_name == adapter_name), None)\n                    if matching_adapter:\n                        harvest_summary[adapter_name][\"trust_ratio\"] = max(\n                            harvest_summary[adapter_name].get(\"trust_ratio\", 0.0),\n                            getattr(matching_adapter, \"trust_ratio\", 0.0)\n                        )\n\n                logger.info(\"Fetched total races\", count=len(all_races_raw))\n            finally:\n                # Save discovery harvest summary for GHA reporting and DB persistence\n                try:\n                    # Only create if it doesn't exist or we have data\n                    if harvest_summary or not os.path.exists(\"discovery_harvest.json\"):\n                        with open(\"discovery_harvest.json\", \"w\") as f:\n                            json.dump(harvest_summary, f)\n\n                    if harvest_summary:\n                        await db.log_harvest(harvest_summary, region=region)\n                except Exception: pass\n\n                # Shutdown adapters\n                for a in adapters:\n                    try: await a.close()\n                    except Exception: pass\n\n        # Apply time window filter if requested to avoid overloading\n        # Initial time window filtering removed to ensure all unique races are tracked for reporting\n\n        if not all_races_raw:\n            logger.error(\"No races fetched from any adapter. Discovery aborted.\")\n            if save_path:\n                try:\n                    with open(save_path, \"w\") as f:\n                        json.dump([], f)\n                    logger.info(\"Saved empty race list to file\", path=save_path)\n                except Exception as e:\n                    logger.error(\"Failed to save empty race list\", error=str(e))\n            return\n        \n        # Deduplicate\n        race_map = {}\n        for race in all_races_raw:\n            canonical_venue = get_canonical_venue(race.venue)\n            # Use Canonical Venue + Race Number + Date + Discipline as stable key\n            st = race.start_time\n            if isinstance(st, str):\n                try:\n                    st = datetime.fromisoformat(st.replace('Z', '+00:00'))\n                except (ValueError, TypeError):\n                    pass\n\n            date_str = st.strftime('%Y%m%d') if hasattr(st, 'strftime') else \"Unknown\"\n            # Removing discipline from key to allow better merging across adapters\n            key = f\"{canonical_venue}|{race.race_number}|{date_str}\"\n            \n            if key not in race_map:\n                race_map[key] = race\n            else:\n                existing = race_map[key]\n                # Merge runners/odds\n                for nr in race.runners:\n                    # Match by number OR name (if numbers are missing)\n                    er = next((r for r in existing.runners if (r.number != 0 and r.number == nr.number) or (r.name.lower() == nr.name.lower())), None)\n                    if er:\n                        er.odds.update(nr.odds)\n                        if not er.win_odds and nr.win_odds:\n                            er.win_odds = nr.win_odds\n                        if not er.number and nr.number:\n                            er.number = nr.number\n                    else:\n                        existing.runners.append(nr)\n\n                # Update source\n                sources = set((existing.source or \"\").split(\", \"))\n                sources.add(race.source or \"Unknown\")\n                existing.source = \", \".join(sorted(list(filter(None, sources))))\n\n        unique_races = list(race_map.values())\n        logger.info(\"Unique races identified\", count=len(unique_races))\n\n        # GPT5 Improvement: Keep all races within window for analysis, not just one per track.\n        # Window broadened to 18 hours to match grid cutoff (News Mode)\n        timing_window_races = []\n        now = datetime.now(EASTERN)\n        for race in unique_races:\n            st = race.start_time\n            if isinstance(st, str):\n                try:\n                    st = datetime.fromisoformat(st.replace('Z', '+00:00'))\n                except (ValueError, TypeError):\n                    continue\n            if st.tzinfo is None:\n                st = st.replace(tzinfo=EASTERN)\n\n            # Calculate Minutes to Post\n            diff = st - now\n            mtp = diff.total_seconds() / 60\n\n            # Broaden window to 18 hours to ensure yield for \"News\"\n            if -45 < mtp <= 1080: # 18 hours = 1080 mins\n                timing_window_races.append(race)\n                if mtp <= 45:\n                    logger.info(f\"  \ud83d\udcb0 Found Gold Candidate: {race.venue} R{race.race_number} ({mtp:.1f} MTP)\")\n                else:\n                    logger.debug(f\"  \ud83d\udd2d Found Upcoming Candidate: {race.venue} R{race.race_number} ({mtp:.1f} MTP)\")\n\n        golden_zone_races = timing_window_races\n        if not golden_zone_races:\n            logger.warning(\"\ud83d\udd2d No races found in the broadened window (-45m to 18h).\")\n\n        logger.info(\"Total unique races available for analysis\", count=len(unique_races))\n\n        # Save raw fetched/merged races if requested (Save EVERYTHING unique)\n        if save_path:\n            try:\n                with open(save_path, \"w\") as f:\n                    json.dump([r.model_dump(mode='json') for r in unique_races], f, indent=4)\n                logger.info(\"Saved all unique races to file\", path=save_path)\n            except Exception as e:\n                logger.error(\"Failed to save races\", error=str(e))\n\n        if fetch_only:\n            logger.info(\"Fetch-only mode active. Skipping analysis and reporting.\")\n            return\n\n        # Analyze ALL unique races to ensure Grid is populated with Top 5 info (News Mode)\n        analyzer = SimplySuccessAnalyzer(config=config)\n        result = analyzer.qualify_races(unique_races)\n        qualified = result.get(\"races\", [])\n\n        # Generate Grid & Goldmine (Grid uses unique_races for the broader context)\n        grid = generate_summary_grid(qualified, all_races=unique_races)\n        logger.info(\"Summary Grid Generated\")\n\n        # Generate Field Matrix for all unique races\n        field_matrix = generate_field_matrix(unique_races)\n        logger.info(\"Field Matrix Generated\")\n\n        # Log Hot Tips & Fetch recent historical results for the report\n        tracker = HotTipsTracker()\n        await tracker.log_tips(qualified)\n\n        historical_goldmines = await tracker.db.get_recent_audited_goldmines(limit=15)\n        historical_report = generate_historical_goldmine_report(historical_goldmines)\n\n        gm_report = generate_goldmine_report(qualified, all_races=unique_races)\n        if historical_report:\n            gm_report += \"\\n\" + historical_report\n\n        # NEW: Dashboard and Live Tracking\n        goldmines = [r for r in qualified if get_field(r, 'metadata', {}).get('is_goldmine')]\n\n        # Calculate today's stats for dashboard\n        recent_tips = await tracker.db.get_recent_tips(limit=100)\n        today_str = datetime.now(EASTERN).strftime(\"%Y-%m-%d\")\n        today_tips = [t for t in recent_tips if t.get(\"report_date\", \"\").startswith(today_str)]\n\n        cashed = sum(1 for t in today_tips if t.get(\"verdict\") == \"CASHED\")\n        total_tips = len(today_tips)\n        profit = sum((t.get(\"net_profit\") or 0.0) for t in today_tips)\n\n        stats = {\n            \"tips\": total_tips,\n            \"cashed\": cashed,\n            \"profit\": profit\n        }\n\n        # Generate friendly HTML report\n        try:\n            html_content = await generate_friendly_html_report(qualified, stats)\n            html_path = Path(\"fortuna_report.html\")\n            html_path.write_text(html_content, encoding=\"utf-8\")\n            logger.info(\"Friendly HTML report generated\", path=str(html_path))\n\n            # Launch the report if running as a portable app (not in GHA)\n            if not os.getenv(\"GITHUB_ACTIONS\"):\n                try:\n                    # Use absolute path for reliable opening\n                    abs_path = html_path.absolute()\n                    if sys.platform == \"win32\":\n                        os.startfile(abs_path)\n                    else:\n                        webbrowser.open(f\"file://{abs_path}\")\n                except Exception as e:\n                    logger.warning(\"Failed to automatically launch report\", error=str(e))\n        except Exception as e:\n            logger.error(\"Failed to generate HTML report\", error=str(e))\n\n        if live_dashboard:\n            try:\n                from rich.live import Live\n                from rich.console import Console\n                # Check if our custom dashboard exists\n                try:\n                    from dashboard import FortunaDashboard\n                    dash = FortunaDashboard()\n                    dash.update(goldmines, stats)\n\n                    # Start odds tracker if requested\n                    if track_odds:\n                        try:\n                            from odds_tracker import LiveOddsTracker\n                            adapter_classes = get_discovery_adapter_classes()\n                            odds_tracker = LiveOddsTracker(goldmines, adapter_classes)\n                            asyncio.create_task(odds_tracker.start_tracking())\n                        except ImportError:\n                            logger.warning(\"LiveOddsTracker not available\")\n\n                    await dash.run_live()\n                except (ImportError, Exception) as e:\n                    logger.warning(f\"Rich dashboard component missing or failed: {e}\")\n                    # Fallback to simple rich display if possible\n                    console = Console()\n                    console.print(\"\\n\" + grid + \"\\n\")\n            except ImportError:\n                logger.warning(\"Rich library not available, falling back to static display\")\n                print(\"\\n\" + grid + \"\\n\")\n        else:\n            # Fallback to static print\n            try:\n                from dashboard import print_dashboard\n                print_dashboard(goldmines, stats)\n            except Exception as e:\n                # Silently fallback to standard print if dashboard fails\n                pass\n\n            print(\"\\n\" + grid + \"\\n\")\n            if historical_report:\n                print(\"\\n\" + historical_report + \"\\n\")\n\n        # Always save reports to files (GPT5 Improvement: Defensive guards)\n        try:\n            with open(\"summary_grid.txt\", \"w\", encoding='utf-8') as f: f.write(grid)\n            with open(\"field_matrix.txt\", \"w\", encoding='utf-8') as f: f.write(field_matrix)\n            with open(\"goldmine_report.txt\", \"w\", encoding='utf-8') as f: f.write(gm_report)\n        except Exception as e:\n            logger.error(\"failed_saving_text_reports\", error=str(e))\n\n        # Save qualified races to JSON\n        report_data = {\n            \"races\": [r.model_dump(mode='json') for r in qualified],\n            \"analysis_metadata\": result.get(\"criteria\", {}),\n            \"timestamp\": datetime.now(EASTERN).isoformat(),\n        }\n        try:\n            with open(\"qualified_races.json\", \"w\", encoding='utf-8') as f:\n                json.dump(report_data, f, indent=4)\n        except Exception as e:\n            logger.error(\"failed_saving_qualified_races\", error=str(e))\n\n        # NEW: Write GHA Job Summary\n        if 'GITHUB_STEP_SUMMARY' in os.environ:\n            try:\n                predictions_md = format_predictions_section(qualified)\n                # We need a db instance for format_proof_section\n                proof_md = await format_proof_section(tracker.db)\n                harvest_md = build_harvest_table(harvest_summary, \"\ud83d\udef0\ufe0f Discovery Harvest Performance\")\n                artifacts_md = format_artifact_links()\n                write_job_summary(predictions_md, harvest_md, proof_md, artifacts_md)\n                logger.info(\"GHA Job Summary written\")\n            except Exception as e:\n                logger.error(\"Failed to write GHA summary\", error=str(e))\n\n    finally:\n        await GlobalResourceManager.cleanup()\n",
      "name": "run_discovery"
    },
    {
      "type": "async_function",
      "content": "async def start_desktop_app():\n    \"\"\"Starts a FastAPI server and opens a webview window for the Fortuna Dashboard.\"\"\"\n    try:\n        import uvicorn\n        from fastapi import FastAPI\n        from fastapi.responses import HTMLResponse\n        import webview\n        import threading\n        import time\n    except ImportError as e:\n        print(f\"GUI dependencies missing: {e}. Install with 'pip install fastapi uvicorn pywebview'\")\n        return\n\n    app = FastAPI(title=\"Fortuna Desktop Intelligence\")\n\n    @app.get(\"/\", response_class=HTMLResponse)\n    async def get_dashboard():\n        # Retrieve latest Goldmines from the database\n        db = FortunaDB()\n        try:\n            async with db.get_connection() as conn:\n                try:\n                    async with conn.execute(\n                        \"SELECT venue, race_number, selection_number, predicted_2nd_fav_odds, start_time \"\n                        \"FROM tips ORDER BY id DESC LIMIT 50\"\n                    ) as cursor:\n                        tips = await cursor.fetchall()\n                except Exception as e:\n                    print(f\"DB query failed: {e}\")\n                    tips = []\n        except Exception as e:\n            print(f\"Failed to connect to database: {e}\")\n            tips = []\n\n        tips_html = \"\".join([\n            f\"<tr><td>{t[4]}</td><td>{t[0]}</td><td>R{t[1]}</td><td>#{t[2]}</td><td>{t[3]}</td></tr>\"\n            for t in tips\n        ])\n\n        return f\"\"\"\n        <html>\n            <head>\n                <title>Fortuna Intelligence Desktop</title>\n                <style>\n                    body {{ font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif; background: #0f172a; color: #f8fafc; padding: 30px; }}\n                    .container {{ max-width: 1200px; margin: auto; }}\n                    h1 {{ color: #fbbf24; border-bottom: 2px solid #fbbf24; padding-bottom: 10px; text-transform: uppercase; letter-spacing: 2px; }}\n                    table {{ width: 100%; border-collapse: collapse; margin-top: 20px; background: #1e293b; border-radius: 8px; overflow: hidden; }}\n                    th, td {{ padding: 15px; text-align: left; border-bottom: 1px solid #334155; }}\n                    th {{ background: #334155; color: #fbbf24; }}\n                    tr:hover {{ background: #475569; }}\n                    .footer {{ margin-top: 30px; font-size: 0.8em; color: #94a3b8; text-align: center; }}\n                    .btn {{ display: inline-block; background: #fbbf24; color: #0f172a; padding: 10px 20px; border-radius: 5px; text-decoration: none; font-weight: bold; margin-bottom: 20px; }}\n                </style>\n                <script>\n                    setTimeout(() => {{ location.reload(); }}, 30000);\n                </script>\n            </head>\n            <body>\n                <div class=\"container\">\n                    <h1>Fortuna Intelligence Dashboard</h1>\n                    <p>Monitoring global racing markets for Goldmine opportunities...</p>\n                    <a href=\"/\" class=\"btn\">REFRESH NOW</a>\n                    <table>\n                        <thead>\n                            <tr><th>Time Discovered</th><th>Venue</th><th>Race</th><th>Selection</th><th>Odds</th></tr>\n                        </thead>\n                        <tbody>\n                            {tips_html or \"<tr><td colspan='5'>No opportunities found yet. Run discovery to populate the database.</td></tr>\"}\n                        </tbody>\n                    </table>\n                    <div class=\"footer\">Fortuna Intelligence Monolith - Sci-Fi Future Edition - Auto-refreshing every 30s</div>\n                </div>\n            </body>\n        </html>\n        \"\"\"\n\n    def run_server():\n        uvicorn.run(app, host=\"127.0.0.1\", port=8013, log_level=\"error\")\n\n    # Start FastAPI in a background thread\n    server_thread = threading.Thread(target=run_server, daemon=True)\n    server_thread.start()\n\n    # Wait a moment for server to initialize\n    time.sleep(2.0)\n\n    # Create and start the webview window if server is up\n    if server_thread.is_alive():\n        print(\"Launching Fortuna Desktop Window...\")\n        webview.create_window('Fortuna Intelligence Desktop', 'http://127.0.0.1:8013', width=1300, height=900)\n        webview.start()\n    else:\n        print(\"\u26a0\ufe0f Error: GUI Server failed to start.\")\n",
      "name": "start_desktop_app"
    },
    {
      "type": "miscellaneous",
      "content": "\n"
    },
    {
      "type": "async_function",
      "content": "async def ensure_browsers():\n    \"\"\"Ensure browser dependencies are available for scraping.\"\"\"\n\n    # Skip Playwright in frozen apps if binary doesn't exist - use HTTP-only adapters\n    if is_frozen():\n        playwright_path = os.path.expanduser(\"~\\\\AppData\\\\Local\\\\ms-playwright\")\n        if not os.path.exists(playwright_path) and platform.system() == 'Windows':\n            structlog.get_logger().info(\"Running as frozen app - Playwright disabled (binary not found)\")\n            return True\n\n    try:\n        # Check if playwright is installed and has a chromium binary\n        from playwright.async_api import async_playwright\n        async with async_playwright() as p:\n            try:\n                # We try to launch a headless browser to verify installation\n                browser = await p.chromium.launch(headless=True)\n                await browser.close()\n                return True\n            except Exception as e:\n                structlog.get_logger().debug(\"Playwright launch failed during verification\", error=str(e))\n                if is_frozen():\n                    structlog.get_logger().info(\"Frozen app: Playwright launch failed, using HTTP-only fallbacks\")\n                    return True\n    except ImportError:\n        structlog.get_logger().debug(\"Playwright not imported\")\n        if is_frozen(): return True\n\n    if is_frozen():\n        return True\n\n    structlog.get_logger().info(\"Installing browser dependencies (Playwright Chromium)...\")\n    try:\n        # Run installation in a separate process to avoid blocking the loop too much\n        subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"playwright==1.49.1\"], check=True, capture_output=True, text=True)\n        subprocess.run([sys.executable, \"-m\", \"playwright\", \"install\", \"chromium\"], check=True, capture_output=True, text=True)\n        structlog.get_logger().info(\"Browser dependencies installed successfully.\")\n        return True\n    except subprocess.CalledProcessError as e:\n        structlog.get_logger().error(\n            \"Failed to install browsers\",\n            error=str(e),\n            returncode=e.returncode,\n            stdout=e.stdout,\n            stderr=e.stderr\n        )\n        return False\n    except Exception as e:\n        structlog.get_logger().error(\"Unexpected error installing browsers\", error=str(e))\n        return False\n",
      "name": "ensure_browsers"
    },
    {
      "type": "miscellaneous",
      "content": "\n"
    },
    {
      "type": "async_function",
      "content": "async def main_all_in_one():\n    # Configure logging at the start of main\n    structlog.configure(\n        wrapper_class=structlog.make_filtering_bound_logger(logging.INFO)\n    )\n    # Ensure DB path env is set if passed via argument or already in environment\n    # Actually, we should probably add a --db-path arg here too for parity with analytics\n    config = load_config()\n    logger = structlog.get_logger(\"main\")\n    parser = argparse.ArgumentParser(description=\"Fortuna All-In-One - Professional Racing Intelligence\")\n    parser.add_argument(\"--date\", type=str, help=\"Target date (YYYY-MM-DD)\")\n    parser.add_argument(\"--hours\", type=int, default=8, help=\"Discovery time window in hours (default: 8)\")\n    parser.add_argument(\"--monitor\", action=\"store_true\", help=\"Run in monitor mode\")\n    parser.add_argument(\"--once\", action=\"store_true\", help=\"Run monitor once\")\n    parser.add_argument(\"--region\", type=str, choices=[\"USA\", \"INT\", \"GLOBAL\"], help=\"Filter by region (USA, INT or GLOBAL)\")\n    parser.add_argument(\"--include\", type=str, help=\"Comma-separated adapter names to include\")\n    parser.add_argument(\"--save\", type=str, help=\"Save races to JSON file\")\n    parser.add_argument(\"--load\", type=str, help=\"Load races from JSON file(s), comma-separated\")\n    parser.add_argument(\"--fetch-only\", action=\"store_true\", help=\"Only fetch and save data, skip analysis and reporting\")\n    parser.add_argument(\"--db-path\", type=str, help=\"Path to tip history database\")\n    parser.add_argument(\"--clear-db\", action=\"store_true\", help=\"Clear all tips from the database and exit\")\n    parser.add_argument(\"--gui\", action=\"store_true\", help=\"Start the Fortuna Desktop GUI\")\n    parser.add_argument(\"--live-dashboard\", action=\"store_true\", help=\"Show live updating terminal dashboard\")\n    parser.add_argument(\"--track-odds\", action=\"store_true\", help=\"Monitor live odds and send notifications\")\n    parser.add_argument(\"--status\", action=\"store_true\", help=\"Show application status card and latest metrics\")\n    parser.add_argument(\"--show-log\", action=\"store_true\", help=\"Print recent fetch/audit highlights\")\n    parser.add_argument(\"--quick-help\", action=\"store_true\", help=\"Show friendly onboarding guide\")\n    parser.add_argument(\"--open-dashboard\", action=\"store_true\", help=\"Open the HTML intelligence report in browser\")\n    args = parser.parse_args()\n\n    if args.quick_help:\n        print_quick_help()\n        return\n\n    if args.status:\n        print_status_card(config)\n        return\n\n    if args.show_log:\n        await print_recent_logs()\n        return\n\n    if args.open_dashboard:\n        open_report_in_browser()\n        return\n\n    if args.db_path:\n        os.environ[\"FORTUNA_DB_PATH\"] = args.db_path\n\n    if args.quick_help:\n        print_quick_help()\n        return\n\n    if args.status:\n        print_status_card(config)\n        return\n\n    if args.show_log:\n        await print_recent_logs()\n        return\n\n    if args.open_dashboard:\n        open_report_in_browser()\n        return\n\n    # Print status card for all normal runs\n    print_status_card(config)\n\n    if args.gui:\n        # Start GUI. It runs its own event loop for the webview.\n        await ensure_browsers()\n        await start_desktop_app()\n        return\n\n    if args.clear_db:\n        db = FortunaDB()\n        await db.clear_all_tips()\n        await db.close()\n        print(\"Database cleared successfully.\")\n        return\n\n    adapter_filter = [n.strip() for n in args.include.split(\",\")] if args.include else None\n\n    # Use default region if not specified\n    if not args.region:\n        args.region = config.get(\"region\", {}).get(\"default\", DEFAULT_REGION)\n        structlog.get_logger().info(\"Using default region\", region=args.region)\n\n    # Region-based adapter filtering\n    if args.region:\n        if args.region == \"USA\":\n            target_set = USA_DISCOVERY_ADAPTERS\n        elif args.region == \"INT\":\n            target_set = INT_DISCOVERY_ADAPTERS\n        else:\n            target_set = GLOBAL_DISCOVERY_ADAPTERS\n\n        if adapter_filter:\n            adapter_filter = [n for n in adapter_filter if n in target_set]\n        else:\n            adapter_filter = list(target_set)\n\n        # Special case: TwinSpires needs to know its region internally if it's not filtered out\n        # We can pass the region via config if we were creating adapters manually,\n        # but here we use names.\n        # Actually, I updated TwinSpiresAdapter to check self.config.get(\"region\").\n        # I need to ensure the adapter gets this config.\n\n    loaded_races = None\n    if args.load:\n        loaded_races = []\n        for path in args.load.split(\",\"):\n            path = path.strip()\n            if not os.path.exists(path):\n                print(f\"Warning: File not found: {path}\")\n                logger.warning(\"Race data file not found\", path=path)\n                continue\n            try:\n                with open(path, \"r\") as f:\n                    data = json.load(f)\n                    loaded_races.extend([Race.model_validate(r) for r in data])\n            except Exception as e:\n                print(f\"Error loading {path}: {e}\")\n                logger.error(\"Failed to load race data\", path=path, error=str(e), exc_info=True)\n\n    if args.date:\n        target_dates = [args.date]\n    else:\n        now = datetime.now(EASTERN)\n        future = now + timedelta(hours=args.hours)\n\n        target_dates = [now.strftime(\"%Y-%m-%d\")]\n        if future.date() > now.date():\n            target_dates.append(future.strftime(\"%Y-%m-%d\"))\n\n    if args.monitor:\n        await ensure_browsers()\n        monitor = FavoriteToPlaceMonitor(target_dates=target_dates)\n        # Pass region config to monitor\n        monitor.config[\"region\"] = args.region\n        if args.once:\n            await monitor.run_once(loaded_races=loaded_races, adapter_names=adapter_filter)\n            if config.get(\"ui\", {}).get(\"auto_open_report\", True) and not os.getenv(\"GITHUB_ACTIONS\"):\n                open_report_in_browser()\n        else:\n            await monitor.run_continuous() # Continuous mode doesn't support load/filter yet for simplicity\n    else:\n        await ensure_browsers()\n        await run_discovery(\n            target_dates,\n            window_hours=args.hours,\n            loaded_races=loaded_races,\n            adapter_names=adapter_filter,\n            save_path=args.save,\n            fetch_only=args.fetch_only,\n            live_dashboard=args.live_dashboard,\n            track_odds=args.track_odds,\n            region=args.region, # Pass region to run_discovery\n            config=config\n        )\n        # Post-run UI enhancements (Council of Superbrains Directive)\n        if config.get(\"ui\", {}).get(\"auto_open_report\", True) and not os.getenv(\"GITHUB_ACTIONS\"):\n            open_report_in_browser()\n",
      "name": "main_all_in_one"
    },
    {
      "type": "miscellaneous",
      "content": "\n"
    },
    {
      "type": "unknown",
      "content": "if __name__ == \"__main__\":\n    if os.getenv(\"DEBUG_SNAPSHOTS\"):\n        os.makedirs(\"debug_snapshots\", exist_ok=True)\n    \n    # Windows Event Loop Policy Fix (Project Hardening)\n    if sys.platform == 'win32':\n        try:\n            # We prefer ProactorEventLoopPolicy for subprocess support (Playwright requirement)\n            # This is also set at the top of the file for frozen EXEs.\n            asyncio.set_event_loop_policy(asyncio.WindowsProactorEventLoopPolicy())\n        except AttributeError:\n            # Fallback if Proactor is not available (should be rare on modern Windows)\n            try:\n                asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())\n            except AttributeError:\n                pass\n\n    try:\n        asyncio.run(main_all_in_one())\n    except KeyboardInterrupt:\n        pass\n"
    }
  ]
}