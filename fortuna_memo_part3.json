{
  "memo_type": "monolith_structure",
  "source_file": "fortuna.py",
  "part": 3,
  "total_parts": 3,
  "blocks": [
    {
      "type": "miscellaneous",
      "content": "\n\n"
    },
    {
      "type": "class",
      "content": "class HotTipsTracker:\n    \"\"\"Logs reported opportunities to a SQLite database.\"\"\"\n    def __init__(self, db_path: Optional[str] = None):\n        self.db = FortunaDB(db_path) if db_path else FortunaDB()\n        self.logger = structlog.get_logger(self.__class__.__name__)\n\n    async def log_tips(self, races: List[Race]):\n        if not races:\n            return\n\n        await self.db.initialize()\n        now = datetime.now(EASTERN)\n        report_date = now.isoformat()\n        new_tips = []\n\n        # Strict future cutoff to prevent leakage (Never log more than 20 mins ahead)\n        future_limit = now + timedelta(minutes=45)\n\n        for r in races:\n            # Only store \"Best Bets\" (Goldmine, BET NOW, or You Might Like)\n            # These are marked in metadata by the analyzer.\n            if not r.metadata.get('is_best_bet') and not r.metadata.get('is_goldmine'):\n                continue\n\n            # Trustworthiness Airlock Safeguard (Council of Superbrains Directive)\n            active_runners = [run for run in r.runners if not run.scratched]\n            total_active = len(active_runners)\n\n            # Ensure trustworthy odds exist before logging (Memory Directive Fix)\n            if r.metadata.get('predicted_2nd_fav_odds') is None:\n                continue\n\n            if total_active > 0:\n                trustworthy_count = sum(1 for run in active_runners if run.metadata.get(\"odds_source_trustworthy\"))\n                trust_ratio = trustworthy_count / total_active\n                if trust_ratio < 0.5:\n                    self.logger.warning(\"Rejecting race with low trust_ratio for DB logging\", venue=r.venue, race=r.race_number, trust_ratio=round(trust_ratio, 2))\n                    continue\n\n            st = r.start_time\n            if isinstance(st, str):\n                try: st = datetime.fromisoformat(st.replace('Z', '+00:00'))\n                except Exception: continue\n            if st.tzinfo is None: st = st.replace(tzinfo=EASTERN)\n\n            # Reject races too far in the future\n            if st > future_limit or st < now - timedelta(minutes=10):\n                self.logger.debug(\"Rejecting far-future race\", venue=r.venue, start_time=st)\n                continue\n\n            is_goldmine = r.metadata.get('is_goldmine', False)\n            gap12 = r.metadata.get('1Gap2', 0.0)\n\n            tip_data = {\n                \"report_date\": report_date,\n                \"race_id\": r.id,\n                \"venue\": r.venue,\n                \"race_number\": r.race_number,\n                \"start_time\": r.start_time.isoformat() if isinstance(r.start_time, datetime) else str(r.start_time),\n                \"is_goldmine\": is_goldmine,\n                \"1Gap2\": gap12,\n                \"discipline\": r.discipline,\n                \"top_five\": r.top_five_numbers,\n                \"selection_number\": r.metadata.get('selection_number'),\n                \"selection_name\": r.metadata.get('selection_name'),\n                \"predicted_2nd_fav_odds\": r.metadata.get('predicted_2nd_fav_odds')\n            }\n            new_tips.append(tip_data)\n\n        try:\n            await self.db.log_tips(new_tips)\n            self.logger.info(\"Hot tips processed\", count=len(new_tips))\n        except Exception as e:\n            self.logger.error(\"Failed to log hot tips\", error=str(e))\n",
      "name": "HotTipsTracker"
    },
    {
      "type": "miscellaneous",
      "content": "\n\n# ----------------------------------------\n# MONITOR LOGIC\n# ----------------------------------------\n#!/usr/bin/env python3\n"
    },
    {
      "type": "docstring",
      "content": "\"\"\"\nFortuna Favorite-to-Place Betting Monitor\n=========================================\n\nThis script monitors racing data from multiple adapters and identifies\nbetting opportunities based on:\n1. Second favorite odds >= 4.0 decimal\n2. Races under 120 minutes to post (MTP)\n3. Superfecta availability preferred\n\nUsage:\n    python favorite_to_place_monitor.py [--date YYYY-MM-DD] [--refresh-interval 30]\n\"\"\"\n"
    },
    {
      "type": "miscellaneous",
      "content": "\n@dataclass\n"
    },
    {
      "type": "class",
      "content": "class RaceSummary:\n    \"\"\"Summary of a single race for display.\"\"\"\n    discipline: str  # T/H/G\n    track: str\n    race_number: int\n    field_size: int\n    superfecta_offered: bool\n    adapter: str\n    start_time: datetime\n    mtp: Optional[int] = None  # Minutes to post\n    second_fav_odds: Optional[float] = None\n    second_fav_name: Optional[str] = None\n    selection_number: Optional[int] = None\n    favorite_odds: Optional[float] = None\n    favorite_name: Optional[str] = None\n    top_five_numbers: Optional[str] = None\n    gap12: float = 0.0\n    is_goldmine: bool = False\n    is_best_bet: bool = False\n\n    def to_dict(self) -> dict:\n        \"\"\"Convert to dictionary for JSON serialization.\"\"\"\n        return {\n            \"discipline\": self.discipline,\n            \"track\": self.track,\n            \"race_number\": self.race_number,\n            \"field_size\": self.field_size,\n            \"superfecta_offered\": self.superfecta_offered,\n            \"adapter\": self.adapter,\n            \"start_time\": self.start_time.isoformat(),\n            \"mtp\": self.mtp,\n            \"second_fav_odds\": self.second_fav_odds,\n            \"second_fav_name\": self.second_fav_name,\n            \"selection_number\": self.selection_number,\n            \"favorite_odds\": self.favorite_odds,\n            \"favorite_name\": self.favorite_name,\n            \"top_five_numbers\": self.top_five_numbers,\n            \"gap12\": self.gap12,\n            \"is_goldmine\": self.is_goldmine,\n            \"is_best_bet\": self.is_best_bet,\n        }\n",
      "name": "RaceSummary"
    },
    {
      "type": "miscellaneous",
      "content": "\n\n"
    },
    {
      "type": "function",
      "content": "def get_discovery_adapter_classes() -> List[Type[BaseAdapterV3]]:\n    \"\"\"Returns all non-abstract discovery adapter classes.\"\"\"\n    def get_all_subclasses(cls):\n        return set(cls.__subclasses__()).union(\n            [s for c in cls.__subclasses__() for s in get_all_subclasses(c)]\n        )\n\n    return [\n        c for c in get_all_subclasses(BaseAdapterV3)\n        if not getattr(c, \"__abstractmethods__\", None)\n        and getattr(c, \"ADAPTER_TYPE\", \"discovery\") == \"discovery\"\n    ]\n",
      "name": "get_discovery_adapter_classes"
    },
    {
      "type": "miscellaneous",
      "content": "\n\n"
    },
    {
      "type": "class",
      "content": "class FavoriteToPlaceMonitor:\n    \"\"\"Monitor for favorite-to-place betting opportunities.\"\"\"\n\n    def __init__(self, target_dates: Optional[List[str]] = None, refresh_interval: int = 30, config: Optional[Dict] = None):\n        \"\"\"\n        Initialize monitor.\n\n        Args:\n            target_dates: Dates to fetch races for (YYYY-MM-DD), defaults to today + tomorrow\n            refresh_interval: Seconds between refreshes for BET NOW list\n        \"\"\"\n        if target_dates:\n            self.target_dates = target_dates\n        else:\n            today = datetime.now(EASTERN)\n            tomorrow = today + timedelta(days=1)\n            self.target_dates = [today.strftime(\"%Y-%m-%d\"), tomorrow.strftime(\"%Y-%m-%d\")]\n\n        self.refresh_interval = refresh_interval\n        self.config = config or {}\n        self.all_races: List[RaceSummary] = []\n        self.adapters: List = []\n        self.logger = structlog.get_logger(self.__class__.__name__)\n        self.tracker = HotTipsTracker()\n\n    async def initialize_adapters(self, adapter_names: Optional[List[str]] = None):\n        \"\"\"Initialize all adapters, optionally filtered by name.\"\"\"\n        all_discovery_classes = get_discovery_adapter_classes()\n\n        classes_to_init = all_discovery_classes\n        if adapter_names:\n            classes_to_init = [c for c in all_discovery_classes if c.__name__ in adapter_names or getattr(c, \"SOURCE_NAME\", \"\") in adapter_names]\n\n        self.logger.info(\"Initializing adapters\", count=len(classes_to_init))\n\n        for adapter_class in classes_to_init:\n            try:\n                adapter = adapter_class(config={\"region\": self.config.get(\"region\")})\n                self.adapters.append(adapter)\n                self.logger.debug(\"Adapter initialized\", adapter=adapter_class.__name__)\n            except Exception as e:\n                self.logger.error(\"Adapter initialization failed\", adapter=adapter_class.__name__, error=str(e))\n\n        self.logger.info(\"Adapters initialization complete\", initialized=len(self.adapters))\n\n    async def fetch_all_races(self) -> List[Tuple[Race, str]]:\n        \"\"\"Fetch races from all adapters.\"\"\"\n        self.logger.info(\"Fetching races\", dates=self.target_dates)\n\n        all_races_with_adapters = []\n\n        # Run fetches in parallel for speed\n        async def fetch_one(adapter, date_str):\n            name = adapter.__class__.__name__\n            try:\n                races = await adapter.get_races(date_str)\n                self.logger.info(\"Fetch complete\", adapter=name, date=date_str, count=len(races))\n                return [(r, name) for r in races]\n            except Exception as e:\n                self.logger.error(\"Fetch failed\", adapter=name, date=date_str, error=str(e))\n                return []\n\n        fetch_tasks = []\n        for d in self.target_dates:\n            for a in self.adapters:\n                fetch_tasks.append(fetch_one(a, d))\n\n        results = await asyncio.gather(*fetch_tasks)\n        for r_list in results:\n            all_races_with_adapters.extend(r_list)\n\n        self.logger.info(\"Total races fetched\", total=len(all_races_with_adapters))\n        return all_races_with_adapters\n\n    def _get_discipline_code(self, race: Race) -> str:\n        \"\"\"Get discipline code (T/H/G).\"\"\"\n        if not race.discipline:\n            return \"T\"\n\n        d = race.discipline.lower()\n        if \"harness\" in d or \"standardbred\" in d: return \"H\"\n        if \"greyhound\" in d or \"dog\" in d: return \"G\"\n        return \"T\"\n\n    def _calculate_field_size(self, race: Race) -> int:\n        \"\"\"Calculate active field size.\"\"\"\n        return len([r for r in race.runners if not r.scratched])\n\n    def _has_superfecta(self, race: Race) -> bool:\n        \"\"\"Check if race offers Superfecta.\"\"\"\n        ab = race.available_bets or []\n        # Support metadata fallback if field not populated\n        if not ab and hasattr(race, 'metadata'):\n            ab = race.metadata.get('available_bets', [])\n        return \"Superfecta\" in ab\n\n    def _get_top_runners(self, race: Race, limit: int = 5) -> List[Runner]:\n        \"\"\"Get top runners by odds, sorted lowest first.\"\"\"\n        # Get active runners with valid odds\n        r_with_odds = []\n        for r in race.runners:\n            if r.scratched:\n                continue\n            # Refresh odds to avoid stale metadata in continuous monitor mode\n            wo = _get_best_win_odds(r)\n            if wo is not None and wo > 1.0:\n                # Update runner object with fresh odds for downstream summaries\n                r.win_odds = float(wo)\n                # Store the Decimal odds directly for sorting to avoid conversion\n                r_with_odds.append((r, wo))\n\n        if not r_with_odds:\n            return []\n\n        # Sort by odds (lowest first)\n        sorted_r = sorted(r_with_odds, key=lambda x: x[1])\n        return [x[0] for x in sorted_r[:limit]]\n\n    def _calculate_mtp(self, start_time: Optional[datetime]) -> int:\n        \"\"\"Calculate minutes to post. Returns -9999 if start_time is None.\"\"\"\n        if not start_time: return -9999\n        now = now_eastern()\n        # Use ensure_eastern to handle naive or other timezones correctly\n        st = ensure_eastern(start_time)\n        delta = st - now\n        return int(delta.total_seconds() / 60)\n\n    def _get_top_n_runners(self, race: Race, n: int = 5) -> str:\n        \"\"\"Get top N runners by win odds.\"\"\"\n        top_runners = self._get_top_runners(race, limit=n)\n        return \", \".join([str(r.number) if r.number is not None else \"?\" for r in top_runners])\n\n    def _create_race_summary(self, race: Race, adapter_name: str) -> RaceSummary:\n        \"\"\"Create a RaceSummary from a Race object.\"\"\"\n        top_runners = self._get_top_runners(race, limit=5)\n        favorite = top_runners[0] if len(top_runners) >= 1 else None\n        second_fav = top_runners[1] if len(top_runners) >= 2 else None\n\n        gap12 = 0.0\n        if favorite and second_fav and favorite.win_odds and second_fav.win_odds:\n            gap12 = round(second_fav.win_odds - favorite.win_odds, 2)\n\n        return RaceSummary(\n            discipline=self._get_discipline_code(race),\n            track=normalize_venue_name(race.venue),\n            race_number=race.race_number,\n            field_size=self._calculate_field_size(race),\n            superfecta_offered=self._has_superfecta(race),\n            adapter=adapter_name,\n            start_time=race.start_time,\n            mtp=self._calculate_mtp(race.start_time),\n            second_fav_odds=second_fav.win_odds if second_fav else None,\n            second_fav_name=second_fav.name if second_fav else None,\n            selection_number=second_fav.number if second_fav else None,\n            favorite_odds=favorite.win_odds if favorite else None,\n            favorite_name=favorite.name if favorite else None,\n            top_five_numbers=self._get_top_n_runners(race, 5),\n            gap12=gap12,\n            is_goldmine=race.metadata.get('is_goldmine', False),\n            is_best_bet=race.metadata.get('is_best_bet', False)\n        )\n\n    async def build_race_summaries(self, races_with_adapters: List[Tuple[Race, str]], window_hours: Optional[int] = 12):\n        \"\"\"Build and deduplicate summary list, with optional time window filtering.\"\"\"\n        race_map = {}\n        now = datetime.now(EASTERN)\n        cutoff = now + timedelta(hours=window_hours) if window_hours else None\n\n        for race, adapter_name in races_with_adapters:\n            try:\n                # Time window filtering\n                st = race.start_time\n                if st.tzinfo is None: st = st.replace(tzinfo=EASTERN)\n\n                # Time window filtering removed to ensure all unique races are counted\n\n                summary = self._create_race_summary(race, adapter_name)\n                # Stable key: Canonical Venue + Race Number + Date\n                canonical_venue = get_canonical_venue(summary.track)\n                date_str = summary.start_time.strftime('%Y%m%d') if summary.start_time else \"Unknown\"\n                key = f\"{canonical_venue}|{summary.race_number}|{date_str}\"\n\n                if key not in race_map:\n                    race_map[key] = summary\n                else:\n                    existing = race_map[key]\n                    # Prefer the one with valid second favorite odds\n                    if summary.second_fav_odds and not existing.second_fav_odds:\n                        race_map[key] = summary\n                    # Or prefer more detailed available bets\n                    elif summary.superfecta_offered and not existing.superfecta_offered:\n                        race_map[key] = summary\n            except Exception: pass\n\n        unique_summaries = list(race_map.values())\n        self.all_races = sorted(unique_summaries, key=lambda x: x.start_time)\n\n        # GPT5 Improvement: Keep all races within window for analysis, not just one per track.\n        # Window broadened to 18 hours (News Mode)\n        timing_window_summaries = []\n        now = datetime.now(EASTERN)\n        for summary in unique_summaries:\n            st = summary.start_time\n            if st.tzinfo is None: st = st.replace(tzinfo=EASTERN)\n\n            # Calculate Minutes to Post\n            diff = st - now\n            mtp = diff.total_seconds() / 60\n\n            # Broaden window to 18 hours to ensure yield for \"News\"\n            if -45 < mtp <= 1080: # 18 hours\n                timing_window_summaries.append(summary)\n\n        self.golden_zone_races = timing_window_summaries\n        if not self.golden_zone_races:\n            self.logger.warning(\"\ud83d\udd2d Monitor found 0 races in the Broadened Window (-45m to 18h)\", total_unique=len(unique_summaries))\n\n    def print_full_list(self):\n        \"\"\"Log all fetched races.\"\"\"\n        lines = [\n            \"=\" * 120,\n            \"FULL RACE LIST\".center(120),\n            \"=\" * 120,\n            f\"{'DISC':<5} {'TRACK':<25} {'R#':<4} {'FIELD':<6} {'SUPER':<6} {'ADAPTER':<25} {'START TIME':<20}\",\n            \"-\" * 120\n        ]\n        for r in sorted(self.all_races, key=lambda x: (x.discipline, x.track, x.race_number)):\n            superfecta = \"Yes\" if r.superfecta_offered else \"No\"\n            # Display time in Eastern with ET suffix\n            st = r.start_time.strftime(\"%Y-%m-%d %H:%M ET\") if r.start_time else \"Unknown\"\n            lines.append(f\"{r.discipline:<5} {r.track[:24]:<25} {r.race_number:<4} {r.field_size:<6} {superfecta:<6} {r.adapter[:24]:<25} {st:<20}\")\n        lines.append(\"-\" * 120)\n        lines.append(f\"Total races: {len(self.all_races)}\")\n        self.logger.info(\"\\n\".join(lines))\n\n    def get_bet_now_races(self) -> List[RaceSummary]:\n        \"\"\"Get races meeting BET NOW criteria.\"\"\"\n        # 1. MTP <= 120 (Broadened for yield)\n        # 2. 2nd Fav Odds >= 4.0\n        # 3. Field size <= 11 (User Directive)\n        # 4. Gap > 0.25 (User Directive)\n        bet_now = [\n            r for r in self.golden_zone_races\n            if r.mtp is not None and -10 < r.mtp <= 120\n            and r.second_fav_odds is not None and r.second_fav_odds >= 4.0\n            and r.field_size <= 11\n            and r.gap12 > 0.25\n        ]\n        # Sort by Superfecta desc, then MTP asc\n        bet_now.sort(key=lambda r: (not r.superfecta_offered, r.mtp))\n        return bet_now\n\n    def get_you_might_like_races(self) -> List[RaceSummary]:\n        \"\"\"Get 'You Might Like' races with relaxed criteria.\"\"\"\n        # Criteria: Not in BET NOW, but -10 < MTP <= 240 (4h) and 2nd Fav Odds >= 3.0\n        # and field size <= 11 and Gap > 0.25\n        bet_now_keys = {(r.track, r.race_number) for r in self.get_bet_now_races()}\n        yml = [\n            r for r in self.golden_zone_races\n            if r.mtp is not None and -10 < r.mtp <= 240\n            and r.second_fav_odds is not None and r.second_fav_odds >= 3.0\n            and r.field_size <= 11\n            and r.gap12 > 0.25\n            and (r.track, r.race_number) not in bet_now_keys\n        ]\n        # Sort by MTP asc\n        yml.sort(key=lambda r: r.mtp)\n        return yml[:5]  # Limit to top 5 recommendations\n\n    async def print_bet_now_list(self):\n        \"\"\"Log filtered BET NOW list and recent audited goldmine results.\"\"\"\n        bet_now = self.get_bet_now_races()\n        lines = [\n            \"=\" * 140,\n            \"\ud83c\udfaf BET NOW - FAVORITE TO PLACE OPPORTUNITIES\".center(140),\n            \"=\" * 140,\n            f\"Updated: {datetime.now(EASTERN).strftime('%Y-%m-%d %H:%M:%S')} ET\",\n            \"Criteria: -10 < MTP <= 120 minutes AND 2nd Favorite Odds >= 4.0\",\n            \"-\" * 140\n        ]\n        if not bet_now:\n            lines.append(\"\u23f3 No races currently meet BET NOW criteria.\")\n            yml = self.get_you_might_like_races()\n            if yml:\n                lines.extend([\n                    \"=\" * 160,\n                    \"\ud83c\udf1f YOU MIGHT LIKE - NEAR-MISS OPPORTUNITIES\".center(160),\n                    \"=\" * 160,\n                    f\"{'SUPER':<6} {'MTP':<5} {'DISC':<5} {'TRACK':<20} {'R#':<4} {'FIELD':<6} {'ODDS':<20} {'TOP 5':<20}\",\n                    \"-\" * 160\n                ])\n                for r in yml:\n                    sup = \"\u2705\" if r.superfecta_offered else \"\u274c\"\n                    fo = f\"{r.favorite_odds:.2f}\" if r.favorite_odds else \"N/A\"\n                    so = f\"{r.second_fav_odds:.2f}\" if r.second_fav_odds else \"N/A\"\n                    top5 = r.top_five_numbers or \"N/A\"\n                    # Leading zero alignment (Memory Directive Fix)\n                    m_str = f\"{r.mtp:02d}\" if 0 <= r.mtp < 10 else str(r.mtp)\n                    lines.append(f\"{sup:<6} {m_str:<5} {r.discipline:<5} {r.track[:19]:<20} {r.race_number:<4} {r.field_size:<6}  ~ {fo}, {so:<15} [{top5}]\")\n                lines.append(\"-\" * 160)\n            self.logger.info(\"\\n\".join(lines))\n            return\n\n        lines.extend([\n            f\"{'SUPER':<6} {'MTP':<5} {'DISC':<5} {'TRACK':<20} {'R#':<4} {'FIELD':<6} {'ODDS':<20} {'TOP 5':<20}\",\n            \"-\" * 160\n        ])\n        for r in bet_now:\n            sup = \"\u2705\" if r.superfecta_offered else \"\u274c\"\n            fo = f\"{r.favorite_odds:.2f}\" if r.favorite_odds else \"N/A\"\n            so = f\"{r.second_fav_odds:.2f}\" if r.second_fav_odds else \"N/A\"\n            top5 = r.top_five_numbers or \"N/A\"\n            m_str = f\"{r.mtp:02d}\" if 0 <= r.mtp < 10 else str(r.mtp)\n            lines.append(f\"{sup:<6} {m_str:<5} {r.discipline:<5} {r.track[:19]:<20} {r.race_number:<4} {r.field_size:<6}  ~ {fo}, {so:<15} [{top5}]\")\n        lines.extend([\"-\" * 160, f\"Total opportunities: {len(bet_now)}\"])\n        self.logger.info(\"\\n\".join(lines))\n\n        # Include recent audited results to provide proof of system performance\n        history = await self.tracker.db.get_recent_audited_goldmines(limit=10)\n        if history:\n            historical_report = generate_historical_goldmine_report(history)\n            self.logger.info(historical_report)\n\n    def save_to_json(self, filename: str = \"race_data.json\"):\n        \"\"\"Export to JSON.\"\"\"\n        bn = self.get_bet_now_races()\n        yml = self.get_you_might_like_races()\n\n        if not bn:\n            self.logger.warning(\"\ud83d\udd2d Monitor found 0 BET NOW opportunities\", total_checked=len(self.golden_zone_races))\n            # Structured telemetry for monitoring\n            structlog.get_logger(\"FortunaTelemetry\").warning(\"empty_bet_now_list\", golden_zone_count=len(self.golden_zone_races))\n            # Create an indicator file for downstream monitoring (GPT5 Improvement)\n            try:\n                Path(\"monitor_empty.alert\").write_text(datetime.now(EASTERN).isoformat())\n            except Exception: pass\n        else:\n            # Clear alert if it exists\n            try:\n                alert_file = Path(\"monitor_empty.alert\")\n                if alert_file.exists(): alert_file.unlink()\n            except Exception: pass\n\n        data = {\n            \"generated_at\": datetime.now(EASTERN).isoformat(),\n            \"target_dates\": self.target_dates,\n            \"total_races\": len(self.all_races),\n            \"bet_now_count\": len(bn),\n            \"you_might_like_count\": len(yml),\n            \"all_races\": [r.to_dict() for r in self.all_races],\n            \"bet_now_races\": [r.to_dict() for r in bn],\n            \"you_might_like_races\": [r.to_dict() for r in yml],\n        }\n        try:\n            # Ensure parent directory exists (GPT5 Improvement)\n            Path(filename).parent.mkdir(parents=True, exist_ok=True)\n            with open(filename, 'w', encoding='utf-8') as f:\n                json.dump(data, f, indent=2)\n        except Exception as e:\n            self.logger.error(\"failed_saving_race_data\", path=filename, error=str(e))\n\n        # Persistent history log\n        self._append_to_history(bn + yml)\n\n    def _append_to_history(self, races: List[RaceSummary]):\n        \"\"\"Append races to persistent history for future result matching.\"\"\"\n        if not races: return\n        history_file = \"prediction_history.jsonl\"\n        timestamp = datetime.now(EASTERN).isoformat()\n        try:\n            with open(history_file, 'a') as f:\n                for r in races:\n                    record = r.to_dict()\n                    record[\"logged_at\"] = timestamp\n                    f.write(json.dumps(record) + \"\\n\")\n        except Exception as e:\n            self.logger.error(\"History logging failed\", error=str(e))\n\n    async def run_once(self, loaded_races: Optional[List[Race]] = None, adapter_names: Optional[List[str]] = None):\n        try:\n            if loaded_races is not None:\n                self.logger.info(\"Using loaded races\", count=len(loaded_races))\n                # Map to (Race, AdapterName) tuple expected by build_race_summaries\n                raw = [(r, r.source) for r in loaded_races]\n            else:\n                await self.initialize_adapters(adapter_names=adapter_names)\n                raw = await self.fetch_all_races()\n\n            await self.build_race_summaries(raw, window_hours=12) # Use 12h window for monitor\n            self.print_full_list()\n            await self.print_bet_now_list()\n            self.save_to_json()\n        finally:\n            for a in self.adapters: await a.shutdown()\n            await GlobalResourceManager.cleanup()\n\n    async def run_continuous(self):\n        await self.initialize_adapters()\n        raw = await self.fetch_all_races()\n        await self.build_race_summaries(raw, window_hours=12)\n        self.print_full_list()\n        try:\n            for _ in range(1000): # Iteration limit to prevent potential hangs\n                for r in self.all_races: r.mtp = self._calculate_mtp(r.start_time)\n                await self.print_bet_now_list()\n                self.save_to_json()\n                await asyncio.sleep(self.refresh_interval)\n        except KeyboardInterrupt:\n            self.logger.info(\"Stopped by user\")\n        finally:\n            for a in self.adapters: await a.shutdown()\n            await GlobalResourceManager.cleanup()\n",
      "name": "FavoriteToPlaceMonitor"
    },
    {
      "type": "miscellaneous",
      "content": "\n\n\n\n\n# ----------------------------------------\n# EXPANDED ADAPTERS\n# ----------------------------------------\n# python_service/adapters/oddschecker_adapter.py\n\n\n\n\n\n"
    },
    {
      "type": "class",
      "content": "class OddscheckerAdapter(BrowserHeadersMixin, DebugMixin, BaseAdapterV3):\n    \"\"\"Adapter for scraping horse racing odds from Oddschecker, migrated to BaseAdapterV3.\"\"\"\n\n    SOURCE_NAME = \"Oddschecker\"\n    BASE_URL = \"https://www.oddschecker.com\"\n\n    def __init__(self, config=None):\n        super().__init__(source_name=self.SOURCE_NAME, base_url=self.BASE_URL, config=config)\n\n    def _configure_fetch_strategy(self) -> FetchStrategy:\n        # Oddschecker is heavily protected by Cloudflare; Playwright with high timeout and network idle\n        return FetchStrategy(\n            primary_engine=BrowserEngine.PLAYWRIGHT,\n            enable_js=True,\n            stealth_mode=\"camouflage\",\n            timeout=120,\n            network_idle=True\n        )\n\n    async def make_request(self, method: str, url: str, **kwargs: Any) -> Any:\n        # Playwright doesn't use impersonate but SmartFetcher handles it now\n        return await super().make_request(method, url, **kwargs)\n\n    def _get_headers(self) -> dict:\n        return self._get_browser_headers(host=\"www.oddschecker.com\")\n\n    async def _fetch_data(self, date: str) -> Optional[dict]:\n        \"\"\"\n        Fetches the raw HTML for all race pages for a given date. This involves a multi-level fetch.\n        \"\"\"\n        index_url = f\"/horse-racing/{date}\"\n        index_response = await self.make_request(\"GET\", index_url, headers=self._get_headers())\n        if not index_response or not index_response.text:\n            self.logger.warning(\"Failed to fetch Oddschecker index page\", url=index_url)\n            return None\n\n        self._save_debug_html(index_response.text, f\"oddschecker_index_{date}\")\n\n        parser = HTMLParser(index_response.text)\n        # Find all links to individual race pages\n        metadata = []\n\n        try:\n            target_date = datetime.strptime(date, \"%Y-%m-%d\").date()\n        except Exception:\n            target_date = datetime.now(EASTERN).date()\n\n        site_tz = ZoneInfo(\"Europe/London\")\n        now_site = datetime.now(site_tz)\n\n        # Group by track to pick \"next\" race\n        track_map = defaultdict(list)\n\n        # Broaden selectors for race links\n        for selector in [\"a.race-time-link[href]\", \"a[href*='/horse-racing/'][href*='/20']\", \".rf__link\"]:\n            for a in parser.css(selector):\n                href = a.attributes.get(\"href\")\n                if href and not href.endswith(\"/horse-racing\"):\n                    # Ensure absolute URL\n                    full_url = href if href.startswith(\"http\") else f\"{self.BASE_URL}{href}\"\n\n                    # Extract track from URL if possible, or use parent\n                    # URL usually /horse-racing/venue/date/time\n                    parts = full_url.split(\"/\")\n                    if len(parts) >= 6:\n                        track = parts[4]\n                        txt = node_text(a) # Time is often in text\n                        track_map[track].append({\"url\": full_url, \"time_txt\": txt})\n\n        for track, races in track_map.items():\n            for r in races:\n                if re.match(r\"\\d{1,2}:\\d{2}\", r[\"time_txt\"]):\n                    try:\n                        rt = datetime.strptime(r[\"time_txt\"], \"%H:%M\").replace(\n                            year=target_date.year, month=target_date.month, day=target_date.day, tzinfo=site_tz\n                        )\n                        # Broaden window to capture multiple races (Memory Directive Fix)\n                        diff = (rt - now_site).total_seconds() / 60\n                        if not (-45 < diff <= 1080):\n                            continue\n\n                        metadata.append(r[\"url\"])\n                    except Exception: pass\n\n        if not metadata:\n            self.logger.warning(\"No metadata found\", context=\"Oddschecker Index Parsing\", url=index_url)\n            return None\n\n        async def fetch_single_html(url_path: str):\n            response = await self.make_request(\"GET\", url_path, headers=self._get_headers())\n            return response.text if response else \"\"\n\n        tasks = [fetch_single_html(link) for link in metadata]\n        html_pages = await asyncio.gather(*tasks)\n        return {\"pages\": html_pages, \"date\": date}\n\n    def _parse_races(self, raw_data: Any) -> List[Race]:\n        \"\"\"Parses a list of raw HTML strings from different races into Race objects.\"\"\"\n        if not raw_data or not raw_data.get(\"pages\"):\n            return []\n\n        try:\n            race_date = datetime.strptime(raw_data[\"date\"], \"%Y-%m-%d\").date()\n        except ValueError:\n            self.logger.error(\n                \"Invalid date format provided to OddscheckerAdapter\",\n                date=raw_data.get(\"date\"),\n            )\n            return []\n\n        all_races = []\n        for html in raw_data[\"pages\"]:\n            if not html:\n                continue\n            try:\n                parser = HTMLParser(html)\n                race = self._parse_race_page(parser, race_date)\n                if race:\n                    all_races.append(race)\n            except (AttributeError, IndexError, ValueError):\n                self.logger.warning(\n                    \"Error parsing a race from Oddschecker, skipping race.\",\n                    exc_info=True,\n                )\n                continue\n        return all_races\n\n    def _parse_race_page(self, parser: HTMLParser, race_date) -> Optional[Race]:\n        track_name_node = parser.css_first(\"h1.meeting-name\")\n        if not track_name_node:\n            return None\n        track_name = track_name_node.text(strip=True)\n\n        race_time_node = parser.css_first(\"span.race-time\")\n        if not race_time_node:\n            return None\n        race_time_str = race_time_node.text(strip=True)\n\n        # Heuristic to find race number from navigation\n        active_link = parser.css_first(\"a.race-time-link.active\")\n        race_number = 0\n        if active_link:\n            all_links = parser.css(\"a.race-time-link\")\n            try:\n                for i, link in enumerate(all_links):\n                    if link.html == active_link.html:\n                        race_number = i + 1\n                        break\n            except Exception:\n                pass\n\n        start_time = datetime.combine(race_date, datetime.strptime(race_time_str, \"%H:%M\").time())\n        runners = [runner for row in parser.css(\"tr.race-card-row\") if (runner := self._parse_runner_row(row))]\n\n        if not runners:\n            return None\n\n        return Race(\n            id=f\"oc_{track_name.lower().replace(' ', '')}_{start_time.strftime('%Y%m%d')}_r{race_number}\",\n            venue=track_name,\n            race_number=race_number,\n            start_time=start_time,\n            runners=runners,\n            source=self.source_name,\n        )\n\n    def _parse_runner_row(self, row: Node) -> Optional[Runner]:\n        try:\n            name_node = row.css_first(\"span.selection-name\")\n            if not name_node:\n                return None\n            name = name_node.text(strip=True)\n\n            odds_node = row.css_first(\"span.bet-button-odds-desktop, span.best-price\")\n            if not odds_node:\n                return None\n            odds_str = odds_node.text(strip=True)\n\n            number_node = row.css_first(\"td.runner-number\")\n            number = 0\n            if number_node:\n                num_txt = \"\".join(filter(str.isdigit, number_node.text(strip=True)))\n                if num_txt:\n                    number = int(num_txt)\n\n            if not name or not odds_str:\n                return None\n\n            win_odds = parse_odds_to_decimal(odds_str)\n            odds_dict = {}\n            if odds_data := create_odds_data(self.source_name, win_odds):\n                odds_dict[self.source_name] = odds_data\n\n            return Runner(number=number, name=name, odds=odds_dict)\n        except (AttributeError, ValueError):\n            self.logger.warning(\"Failed to parse a runner on Oddschecker, skipping runner.\")\n            return None\n",
      "name": "OddscheckerAdapter"
    },
    {
      "type": "miscellaneous",
      "content": "\n# python_service/adapters/timeform_adapter.py\n\n\n\n\n\n"
    },
    {
      "type": "class",
      "content": "class TimeformAdapter(JSONParsingMixin, BrowserHeadersMixin, DebugMixin, BaseAdapterV3):\n    \"\"\"\n    Adapter for timeform.com, migrated to BaseAdapterV3 and standardized on selectolax.\n    \"\"\"\n\n    SOURCE_NAME = \"Timeform\"\n    BASE_URL = \"https://www.timeform.com\"\n\n    def __init__(self, config=None):\n        super().__init__(source_name=self.SOURCE_NAME, base_url=self.BASE_URL, config=config)\n        self._semaphore = asyncio.Semaphore(5)\n\n    def _configure_fetch_strategy(self) -> FetchStrategy:\n        # Timeform often blocks basic requests; Playwright is robust\n        return FetchStrategy(\n            primary_engine=BrowserEngine.PLAYWRIGHT,\n            enable_js=True,\n            stealth_mode=\"camouflage\",\n            timeout=90,\n            network_idle=True\n        )\n\n    def _get_headers(self) -> dict:\n        headers = self._get_browser_headers(host=\"www.timeform.com\")\n        headers.update({\n            \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8\",\n            \"Accept-Language\": \"en-US,en;q=0.9\",\n        })\n        return headers\n\n    async def _fetch_data(self, date: str) -> Optional[dict]:\n        \"\"\"\n        Fetches the raw HTML for all race pages for a given date.\n        \"\"\"\n        index_url = f\"/horse-racing/racecards/{date}\"\n        index_response = await self.make_request(\"GET\", index_url, headers=self._get_headers())\n        if not index_response or not index_response.text:\n            self.logger.warning(\"Failed to fetch Timeform index page\", url=index_url)\n            return None\n\n        self._save_debug_snapshot(index_response.text, f\"timeform_index_{date}\")\n\n        parser = HTMLParser(index_response.text)\n        # Updated selector for race links\n        try:\n            target_date = datetime.strptime(date, \"%Y-%m-%d\").date()\n        except Exception:\n            target_date = datetime.now(EASTERN).date()\n\n        site_tz = ZoneInfo(\"Europe/London\")\n        now_site = datetime.now(site_tz)\n\n        track_map = defaultdict(list)\n        # Broaden selectors for Timeform race links\n        for selector in [\"a[href*='/racecards/']\", \".rf__link\", \"a.rf-meeting-race__time\", \".rp-meetingItem__race__time\"]:\n            for a in parser.css(selector):\n                href = a.attributes.get(\"href\")\n                if href and \"/racecards/\" in href and not href.endswith(\"/racecards\"):\n                    # URL usually: /horse-racing/racecards/venue/date/time/...\n                    # or: /racecards/venue/date/time\n                    parts = href.split(\"/\")\n                    # Handle both relative and absolute-ish paths\n                    track = \"unknown\"\n                    for i, p in enumerate(parts):\n                        if p == \"racecards\" and i + 1 < len(parts):\n                            track = parts[i+1]\n                            break\n\n                    txt = node_text(a)\n                    track_map[track].append({\"url\": href, \"time_txt\": txt})\n\n        links = []\n        for track, races in track_map.items():\n            for r in races:\n                # Timeform often uses HH:MM in text\n                time_match = re.search(r\"(\\d{1,2}:\\d{2})\", r[\"time_txt\"])\n                if time_match:\n                    try:\n                        rt = datetime.strptime(time_match.group(1), \"%H:%M\").replace(\n                            year=target_date.year, month=target_date.month, day=target_date.day, tzinfo=site_tz\n                        )\n                        # Broaden window to capture multiple races (Memory Directive Fix)\n                        diff = (rt - now_site).total_seconds() / 60\n                        if not (-45 < diff <= 1080):\n                            continue\n\n                        full_url = r[\"url\"] if r[\"url\"].startswith(\"http\") else f\"{self.BASE_URL}{r['url']}\"\n                        links.append(full_url)\n                    except Exception: pass\n\n        if not links:\n            self.logger.warning(\"No metadata found\", context=\"Timeform Index Parsing\", url=index_url)\n            return None\n\n        async def fetch_single_html(url_path: str):\n            async with self._semaphore:\n                await asyncio.sleep(0.5)\n                response = await self.make_request(\"GET\", url_path, headers=self._get_headers())\n                return (url_path, response.text) if response else (url_path, \"\")\n\n        self.logger.info(f\"Found {len(links)} race links on Timeform\")\n        tasks = [fetch_single_html(link) for link in links]\n        results = await asyncio.gather(*tasks)\n        return {\"pages\": [r for r in results if r[1]], \"date\": date}\n\n    def _parse_races(self, raw_data: Any) -> List[Race]:\n        \"\"\"Parses a list of raw HTML strings into Race objects.\"\"\"\n        if not raw_data or not raw_data.get(\"pages\"):\n            return []\n\n        try:\n            race_date = datetime.strptime(raw_data[\"date\"], \"%Y-%m-%d\").date()\n        except ValueError:\n            self.logger.error(\"Invalid date format\", date=raw_data.get(\"date\"))\n            return []\n\n        all_races = []\n        for url_path, html_content in raw_data[\"pages\"]:\n            if not html_content:\n                continue\n            try:\n                parser = HTMLParser(html_content)\n\n                # Extract via JSON-LD if possible\n                venue = \"\"\n                start_time = None\n                scripts = self._parse_all_jsons_from_scripts(parser, 'script[type=\"application/ld+json\"]', context=\"Betfair Index\")\n                for data in scripts:\n                    if data.get(\"@type\") == \"Event\":\n                        venue = normalize_venue_name(data.get(\"location\", {}).get(\"name\", \"\"))\n                        if sd := data.get(\"startDate\"):\n                            # 2026-01-28T14:32:00\n                            start_time = datetime.fromisoformat(sd.split('+')[0])\n                        break\n\n                if not venue:\n                    # Fallback to title\n                    title = parser.css_first(\"title\")\n                    if title:\n                        # 14:32 DUNDALK | Races 28 January 2026 ...\n                        match = re.search(r'(\\d{1,2}:\\d{2})\\s+([^|]+)', title.text())\n                        if match:\n                            time_str = match.group(1)\n                            venue = normalize_venue_name(match.group(2).strip())\n                            start_time = datetime.combine(race_date, datetime.strptime(time_str, \"%H:%M\").time())\n\n                if not venue or not start_time:\n                    continue\n\n                # Betting Forecast Parsing\n                forecast_map = {}\n                verdict_section = parser.css_first(\"section.rp-verdict\")\n                if verdict_section:\n                    forecast_text = clean_text(verdict_section.text())\n                    if \"Betting Forecast :\" in forecast_text:\n                        # \"Betting Forecast : 15/8 2.87 Spring Is Here, 3/1 4 This Guy, ...\"\n                        after_forecast = forecast_text.split(\"Betting Forecast :\")[1]\n                        # Split by comma\n                        parts = after_forecast.split(',')\n                        for part in parts:\n                            # Match odds and then name\n                            # Odds can be fractional space decimal\n                            m = re.search(r'(\\d+/\\d+|EVENS)\\s+([\\d\\.]+)?\\s*(.+)', part.strip())\n                            if m:\n                                odds_str = m.group(1)\n                                name = clean_text(m.group(3))\n                                forecast_map[name.lower()] = odds_str\n\n                # Runners\n                runners = []\n                # Use tbody as the main container for each runner\n                for row in parser.css('tbody.rp-horse-row'):\n                    if runner := self._parse_runner(row, forecast_map):\n                        runners.append(runner)\n\n                if not runners:\n                    continue\n\n                # Race number from URL or sequence\n                race_number = 0\n                num_match = re.search(r'/(\\d+)/([^/]+)$', url_path)\n                # .../1432/207/1/view... -> the '1' is the race number\n                url_parts = url_path.split('/')\n                if len(url_parts) >= 10:\n                    try: race_number = int(url_parts[9])\n                    except Exception: pass\n\n                race = Race(\n                    id=f\"tf_{venue.lower().replace(' ', '')}_{start_time:%Y%m%d}_R{race_number}\",\n                    venue=venue,\n                    race_number=race_number,\n                    start_time=start_time,\n                    runners=runners,\n                    source=self.source_name,\n                )\n                all_races.append(race)\n            except Exception as e:\n                self.logger.warning(f\"Error parsing Timeform race: {e}\")\n                continue\n        return all_races\n\n    def _parse_runner(self, row: Node, forecast_map: dict = None) -> Optional[Runner]:\n        \"\"\"Parses a single runner from a table row node.\"\"\"\n        try:\n            name_node = row.css_first(\"a.rp-horse\") or row.css_first(\"a.rp-horseTable_horse-name\")\n            if not name_node:\n                return None\n            name = clean_text(name_node.text())\n\n            number = 0\n            num_attr = row.attributes.get(\"data-entrynumber\")\n            if num_attr:\n                try:\n                    val = int(num_attr)\n                    if val <= 40: number = val\n                except Exception:\n                    pass\n\n            if not number:\n                num_node = row.css_first(\".rp-entry-number\") or row.css_first(\"span.rp-horseTable_horse-number\")\n                if num_node:\n                    num_text = clean_text(num_node.text()).strip(\"()\")\n                    num_match = re.search(r\"\\d+\", num_text)\n                    if num_match:\n                        val = int(num_match.group())\n                        if val <= 40: number = val\n\n            win_odds = None\n            if forecast_map:\n                win_odds = parse_odds_to_decimal(forecast_map.get(name.lower()))\n\n            # Try to find live odds button if available (old selector)\n            if not win_odds:\n                odds_tag = row.css_first(\"button.rp-bet-placer-btn__odds\")\n                if odds_tag:\n                    win_odds = parse_odds_to_decimal(clean_text(odds_tag.text()))\n\n            odds_data = {}\n            if odds_val := create_odds_data(self.source_name, win_odds):\n                odds_data[self.source_name] = odds_val\n\n            return Runner(number=number, name=name, odds=odds_data)\n        except (AttributeError, ValueError, TypeError):\n            return None\n",
      "name": "TimeformAdapter"
    },
    {
      "type": "miscellaneous",
      "content": "\n# python_service/adapters/racingpost_adapter.py\n\n\n\n\n"
    },
    {
      "type": "class",
      "content": "class RacingPostAdapter(BrowserHeadersMixin, DebugMixin, BaseAdapterV3):\n    \"\"\"\n    Adapter for scraping Racing Post racecards, migrated to BaseAdapterV3.\n    \"\"\"\n\n    SOURCE_NAME = \"RacingPost\"\n    BASE_URL = \"https://www.racingpost.com\"\n\n    def __init__(self, config=None):\n        super().__init__(source_name=self.SOURCE_NAME, base_url=self.BASE_URL, config=config)\n\n    def _configure_fetch_strategy(self) -> FetchStrategy:\n        # RacingPost has strong anti-bot measures. Playwright with stealth is usually the best bet.\n        return FetchStrategy(\n            primary_engine=BrowserEngine.PLAYWRIGHT,\n            enable_js=True,\n            stealth_mode=\"camouflage\",\n            timeout=90,\n            block_resources=False,\n            network_idle=True\n        )\n\n    def _get_headers(self) -> dict:\n        return self._get_browser_headers(host=\"www.racingpost.com\")\n\n    async def _fetch_data(self, date: str) -> Any:\n        \"\"\"\n        Fetches the raw HTML content for all races on a given date, including international.\n        \"\"\"\n        index_url = f\"/racecards/{date}\"\n        # RacingPost international URL sometimes varies\n        intl_urls = [\n            f\"/racecards/international/{date}\",\n            f\"/racecards/{date}/international\",\n            \"/racecards/international\"\n        ]\n\n        index_response = await self.make_request(\"GET\", index_url, headers=self._get_headers())\n\n        intl_response = None\n        for url in intl_urls:\n            resp = await self.make_request(\"GET\", url, headers=self._get_headers())\n            if resp and resp.status == 200:\n                intl_response = resp\n                break\n\n        race_card_urls = []\n        try:\n            target_date = datetime.strptime(date, \"%Y-%m-%d\").date()\n        except Exception:\n            target_date = datetime.now(EASTERN).date()\n\n        site_tz = ZoneInfo(\"Europe/London\")\n        now_site = datetime.now(site_tz)\n\n        if index_response and index_response.text:\n            self._save_debug_html(index_response.text, f\"racingpost_index_{date}\")\n            index_parser = HTMLParser(index_response.text)\n\n            # Broaden window to capture multiple races (Memory Directive Fix)\n            meetings = index_parser.css('.rp-raceCourse__panel') or index_parser.css('.RC-meetingItem') or index_parser.css('.rp-meetingItem') or index_parser.css('.RC-courseCards')\n            for meeting in meetings:\n                # Broaden a tag selectors to catch new Racing Post structures\n                for link in meeting.css('a[data-test-selector^=\"RC-meetingItem__link_race\"], a.rp-raceCourse__panel__race__time, a.rp-meetingItem__race__time, a.RC-meetingItem__race__time, a.RC-meetingItem__link, a[href*=\"/racecards/\"]'):\n                    href = link.attributes.get(\"href\", \"\")\n                    if not href or \"/results/\" in href:\n                        continue\n\n                    txt = clean_text(node_text(link))\n                    time_match = re.search(r\"(\\d{1,2}:\\d{2})\", txt)\n                    if time_match:\n                        try:\n                            time_str = time_match.group(1)\n                            tm = datetime.strptime(time_str, \"%H:%M\")\n                            if tm.hour < 9:\n                                tm = tm.replace(hour=tm.hour + 12)\n\n                            rt = tm.replace(\n                                year=target_date.year, month=target_date.month, day=target_date.day, tzinfo=site_tz\n                            )\n                            diff = (rt - now_site).total_seconds() / 60\n                            if not (-45 < diff <= 1080):\n                                continue\n                        except Exception: pass\n\n                    race_card_urls.append(href)\n\n        elif index_response:\n            self.logger.warning(\"Unexpected status\", status=index_response.status, url=index_url)\n\n        if intl_response and intl_response.text:\n            self._save_debug_html(intl_response.text, f\"racingpost_intl_index_{date}\")\n            intl_parser = HTMLParser(intl_response.text)\n\n            meetings = intl_parser.css('.rp-raceCourse__panel') or intl_parser.css('.RC-meetingItem') or intl_parser.css('.rp-meetingItem') or intl_parser.css('.RC-courseCards')\n            for meeting in meetings:\n                for link in meeting.css('a[data-test-selector^=\"RC-meetingItem__link_race\"], a.rp-raceCourse__panel__race__time, a.rp-meetingItem__race__time, a.RC-meetingItem__race__time, a.RC-meetingItem__link, a[href*=\"/racecards/\"]'):\n                    href = link.attributes.get(\"href\", \"\")\n                    if not href or \"/results/\" in href:\n                        continue\n\n                    txt = clean_text(node_text(link))\n                    time_match = re.search(r\"(\\d{1,2}:\\d{2})\", txt)\n                    if time_match:\n                        try:\n                            time_str = time_match.group(1)\n                            tm = datetime.strptime(time_str, \"%H:%M\")\n                            if tm.hour < 9:\n                                tm = tm.replace(hour=tm.hour + 12)\n\n                            rt = tm.replace(\n                                year=target_date.year, month=target_date.month, day=target_date.day, tzinfo=site_tz\n                            )\n                            diff = (rt - now_site).total_seconds() / 60\n                            if not (-45 < diff <= 1080):\n                                continue\n                        except Exception: pass\n\n                    race_card_urls.append(href)\n        elif intl_response:\n            self.logger.warning(\"Unexpected status\", status=intl_response.status, url=intl_url)\n\n        if not race_card_urls:\n            self.logger.warning(\"Standard RacingPost link discovery failed, trying aggressive fallback\", date=date)\n            if index_response and index_response.text:\n                index_parser = HTMLParser(index_response.text)\n                for a in index_parser.css('a[href*=\"/racecards/\"]'):\n                    href = a.attributes.get(\"href\", \"\")\n                    if re.search(r\"/\\d+/.*/\\d{4}-\\d{2}-\\d{2}/\\d+\", href):\n                        race_card_urls.append(href)\n\n            if intl_response and intl_response.text:\n                intl_parser = HTMLParser(intl_response.text)\n                for a in intl_parser.css('a[href*=\"/racecards/\"]'):\n                    href = a.attributes.get(\"href\", \"\")\n                    if re.search(r\"/\\d+/.*/\\d{4}-\\d{2}-\\d{2}/\\d+\", href):\n                        race_card_urls.append(href)\n\n        if not race_card_urls:\n            self.logger.warning(\"Failed to fetch RacingPost racecard links\", date=date)\n            return None\n\n        async def fetch_single_html(url: str):\n            response = await self.make_request(\"GET\", url, headers=self._get_headers())\n            return response.text if response else \"\"\n\n        tasks = [fetch_single_html(url) for url in race_card_urls]\n        html_contents = await asyncio.gather(*tasks)\n        return {\"date\": date, \"html_contents\": html_contents}\n\n    def _parse_races(self, raw_data: Any) -> List[Race]:\n        \"\"\"Parses a list of raw HTML strings into Race objects.\"\"\"\n        if not raw_data or not raw_data.get(\"html_contents\"):\n            return []\n\n        date = raw_data[\"date\"]\n        html_contents = raw_data[\"html_contents\"]\n        all_races: List[Race] = []\n\n        for html in html_contents:\n            if not html:\n                continue\n            try:\n                parser = HTMLParser(html)\n\n                venue_node = parser.css_first('a[data-test-selector=\"RC-course__name\"]')\n                if not venue_node:\n                    continue\n                venue_raw = venue_node.text(strip=True)\n                venue = normalize_venue_name(venue_raw)\n\n                race_time_node = parser.css_first('span[data-test-selector=\"RC-course__time\"]')\n                if not race_time_node:\n                    continue\n                race_time_str = race_time_node.text(strip=True)\n\n                race_datetime_str = f\"{date} {race_time_str}\"\n                start_time = datetime.strptime(race_datetime_str, \"%Y-%m-%d %H:%M\")\n\n                runners = self._parse_runners(parser)\n\n                if venue and runners:\n                    race_number = self._get_race_number(parser, start_time)\n                    race = Race(\n                        id=f\"rp_{venue.lower().replace(' ', '')}_{date}_{race_number}\",\n                        venue=venue,\n                        race_number=race_number,\n                        start_time=start_time,\n                        runners=runners,\n                        source=self.source_name,\n                    )\n                    all_races.append(race)\n            except (AttributeError, ValueError):\n                self.logger.error(\"Failed to parse RacingPost race from HTML content.\", exc_info=True)\n                continue\n        return all_races\n\n    def _get_race_number(self, parser: HTMLParser, start_time: datetime) -> int:\n        \"\"\"Derives the race number by finding the active time in the nav bar.\"\"\"\n        time_str_to_find = start_time.strftime(\"%H:%M\")\n        time_links = parser.css('a[data-test-selector=\"RC-raceTime\"]')\n        for i, link in enumerate(time_links):\n            if link.text(strip=True) == time_str_to_find:\n                return i + 1\n        return 1\n\n    def _parse_runners(self, parser: HTMLParser) -> list[Runner]:\n        \"\"\"Parses all runners from a single race card page.\"\"\"\n        runners = []\n        runner_nodes = parser.css('div[data-test-selector=\"RC-runnerCard\"]')\n        for node in runner_nodes:\n            if runner := self._parse_runner(node):\n                runners.append(runner)\n        return runners\n\n    def _parse_runner(self, node: Node) -> Optional[Runner]:\n        try:\n            number_node = node.css_first('span[data-test-selector=\"RC-runnerNumber\"]')\n            name_node = node.css_first('a[data-test-selector=\"RC-runnerName\"]')\n            odds_node = node.css_first('span[data-test-selector=\"RC-runnerPrice\"]')\n\n            if not all([number_node, name_node, odds_node]):\n                return None\n\n            number_str = clean_text(number_node.text())\n            number = 0\n            if number_str:\n                num_txt = \"\".join(filter(str.isdigit, number_str))\n                if num_txt:\n                    val = int(num_txt)\n                    if val <= 40: number = val\n            name = clean_text(name_node.text())\n            odds_str = clean_text(odds_node.text())\n            scratched = \"NR\" in odds_str.upper() or not odds_str\n\n            odds = {}\n            if not scratched:\n                win_odds = parse_odds_to_decimal(odds_str)\n                if odds_data := create_odds_data(self.source_name, win_odds):\n                    odds[self.source_name] = odds_data\n\n            return Runner(number=number, name=name, odds=odds, scratched=scratched)\n        except (ValueError, AttributeError):\n            self.logger.warning(\"Could not parse RacingPost runner, skipping.\", exc_info=True)\n            return None\n",
      "name": "RacingPostAdapter"
    },
    {
      "type": "miscellaneous",
      "content": "\n\n"
    },
    {
      "type": "class",
      "content": "class RacingPostToteAdapter(BrowserHeadersMixin, DebugMixin, BaseAdapterV3):\n    \"\"\"\n    Adapter for fetching Tote dividends and results from Racing Post.\n    \"\"\"\n    ADAPTER_TYPE = \"results\"\n    SOURCE_NAME = \"RacingPostTote\"\n    BASE_URL = \"https://www.racingpost.com\"\n\n    def __init__(self, config: Optional[Dict[str, Any]] = None):\n        super().__init__(source_name=self.SOURCE_NAME, base_url=self.BASE_URL, config=config)\n\n    def _configure_fetch_strategy(self) -> FetchStrategy:\n        return FetchStrategy(\n            primary_engine=BrowserEngine.CURL_CFFI,\n            enable_js=True,\n            stealth_mode=StealthMode.CAMOUFLAGE,\n            timeout=45\n        )\n\n    def _get_headers(self) -> dict:\n        return self._get_browser_headers(host=\"www.racingpost.com\")\n\n    async def _fetch_data(self, date: str) -> Optional[Dict[str, Any]]:\n        url = f\"/results/{date}\"\n        resp = await self.make_request(\"GET\", url, headers=self._get_headers())\n        if not resp or not resp.text:\n            return None\n\n        self._save_debug_snapshot(resp.text, f\"rp_tote_results_{date}\")\n        parser = HTMLParser(resp.text)\n\n        # Extract links to individual race results\n        links = set()\n        selectors = [\n            'a[data-test-selector=\"RC-meetingItem__link_race\"]',\n            'a[href*=\"/results/\"]',\n            '.ui-link.rp-raceCourse__panel__race__time',\n            'a.rp-raceCourse__panel__race__time'\n        ]\n        target_venues = getattr(self, \"target_venues\", None)\n        for s in selectors:\n            for a in parser.css(s):\n                href = a.attributes.get(\"href\")\n                if href:\n                    # Filter by venue\n                    if target_venues:\n                        match_found = False\n                        for v in target_venues:\n                            if v in href.lower().replace(\"-\", \"\"):\n                                match_found = True\n                                break\n                        if not match_found:\n                            v_text = get_canonical_venue(node_text(a))\n                            if v_text in target_venues:\n                                match_found = True\n                        if not match_found:\n                            continue\n\n                    # Broaden regex to match various RP result link patterns (Memory Directive Fix)\n                    if re.search(r\"/results/.*?\\d{5,}\", href) or \\\n                       re.search(r\"/results/\\d+/\", href) or \\\n                       re.search(r\"/\\d{4}-\\d{2}-\\d{2}/\", href) or \\\n                       len(href.split(\"/\")) >= 4:\n                        links.add(href if href.startswith(\"http\") else f\"{self.BASE_URL}{href}\")\n\n        async def fetch_result_page(link):\n            r = await self.make_request(\"GET\", link, headers=self._get_headers())\n            return (link, r.text if r else \"\")\n\n        tasks = [fetch_result_page(link) for link in links]\n        pages = await asyncio.gather(*tasks)\n        return {\"pages\": pages, \"date\": date}\n\n    def _parse_races(self, raw_data: Any) -> List[Race]:\n        if not raw_data or not raw_data.get(\"pages\"):\n            return []\n\n        races = []\n        date_str = raw_data[\"date\"]\n\n        for link, html_content in raw_data[\"pages\"]:\n            if not html_content:\n                continue\n            try:\n                parser = HTMLParser(html_content)\n                race = self._parse_result_page(parser, date_str, link)\n                if race:\n                    races.append(race)\n            except Exception as e:\n                self.logger.warning(\"Failed to parse RP result page\", link=link, error=str(e))\n\n        return races\n\n    def _parse_result_page(self, parser: HTMLParser, date_str: str, url: str) -> Optional[Race]:\n        venue_node = parser.css_first('a[data-test-selector=\"RC-course__name\"]')\n        if not venue_node: return None\n        venue = normalize_venue_name(venue_node.text(strip=True))\n\n        time_node = parser.css_first('span[data-test-selector=\"RC-course__time\"]')\n        if not time_node: return None\n        time_str = time_node.text(strip=True)\n\n        try:\n            start_time = datetime.strptime(f\"{date_str} {time_str}\", \"%Y-%m-%d %H:%M\").replace(tzinfo=EASTERN)\n        except Exception:\n            return None\n\n        # Extract dividends\n        dividends = {}\n        tote_container = parser.css_first('div[data-test-selector=\"RC-toteReturns\"]')\n        if not tote_container:\n             # Try alternate selector\n             tote_container = parser.css_first('.rp-toteReturns')\n\n        if tote_container:\n            for row in (tote_container.css('div.rp-toteReturns__row') or tote_container.css('.rp-toteReturns__row')):\n                try:\n                    label_node = row.css_first('div.rp-toteReturns__label') or row.css_first('.rp-toteReturns__label')\n                    val_node = row.css_first('div.rp-toteReturns__value') or row.css_first('.rp-toteReturns__value')\n                    if label_node and val_node:\n                        label = clean_text(label_node.text())\n                        value = clean_text(val_node.text())\n                        if label and value:\n                            dividends[label] = value\n                except Exception as e:\n                    self.logger.debug(\"Failed parsing RP tote row\", error=str(e))\n\n\n\n        # Extract runners (finishers)\n        runners = []\n        for row in parser.css('div[data-test-selector=\"RC-resultRunner\"]'):\n            name_node = row.css_first('a[data-test-selector=\"RC-resultRunnerName\"]')\n            if not name_node: continue\n            name = clean_text(name_node.text())\n            pos_node = row.css_first('span.rp-resultRunner__position')\n            pos = clean_text(pos_node.text()) if pos_node else \"?\"\n\n            # Try to find saddle number\n            number = 0\n            num_node = row.css_first(\".rp-resultRunner__saddleClothNo\")\n            if num_node:\n                try: number = int(clean_text(num_node.text()))\n                except Exception: pass\n\n            runners.append(Runner(\n                name=name,\n                number=number,\n                metadata={\"position\": pos}\n            ))\n\n        # Derive race number from header or navigation\n        race_num = 1\n        # Priority 1: Navigation bar active time (most reliable on RP)\n        time_links = parser.css('a[data-test-selector=\"RC-raceTime\"]')\n        found_in_nav = False\n        for i, link in enumerate(time_links):\n            cls = link.attributes.get(\"class\", \"\")\n            if \"active\" in cls or \"rp-raceTimeCourseName__time\" in cls:\n                race_num = i + 1\n                found_in_nav = True\n                break\n\n        if not found_in_nav:\n            # Priority 2: Text search for \"Race X\"\n            race_num_match = re.search(r'Race\\s+(\\d+)', parser.text())\n            if race_num_match:\n                race_num = int(race_num_match.group(1))\n\n        race = Race(\n            id=f\"rp_tote_{get_canonical_venue(venue)}_{date_str.replace('-', '')}_R{race_num}\",\n            venue=venue,\n            race_number=race_num,\n            start_time=start_time,\n            runners=runners,\n            source=self.source_name,\n            metadata={\"dividends\": dividends, \"url\": url}\n        )\n        return race\n",
      "name": "RacingPostToteAdapter"
    },
    {
      "type": "miscellaneous",
      "content": "\n# ----------------------------------------\n# MASTER ORCHESTRATOR\n# ----------------------------------------\n\n"
    },
    {
      "type": "async_function",
      "content": "async def run_discovery(\n    target_dates: List[str],\n    window_hours: Optional[int] = 8,\n    loaded_races: Optional[List[Race]] = None,\n    adapter_names: Optional[List[str]] = None,\n    save_path: Optional[str] = None,\n    fetch_only: bool = False,\n    live_dashboard: bool = False,\n    track_odds: bool = False,\n    region: Optional[str] = None,\n    config: Optional[Dict[str, Any]] = None\n):\n    logger = structlog.get_logger(\"run_discovery\")\n    logger.info(\"Running Discovery\", dates=target_dates, window_hours=window_hours)\n\n    try:\n        now = datetime.now(EASTERN)\n        cutoff = now + timedelta(hours=window_hours) if window_hours else None\n\n        all_races_raw = []\n        harvest_summary = {}\n\n        # Pre-populate harvest_summary based on region/filter for visibility\n        target_region = region or DEFAULT_REGION\n        target_set = USA_DISCOVERY_ADAPTERS if target_region == \"USA\" else INT_DISCOVERY_ADAPTERS\n\n        # Determine which adapters should be visible in the harvest summary\n        if adapter_names:\n            visible_adapters = [n for n in adapter_names if n in target_set]\n        else:\n            visible_adapters = list(target_set)\n\n        for adapter_name in visible_adapters:\n            harvest_summary[adapter_name] = {\"count\": 0, \"max_odds\": 0.0, \"trust_ratio\": 0.0}\n\n        if loaded_races is not None:\n            logger.info(\"Using loaded races\", count=len(loaded_races))\n            all_races_raw = loaded_races\n            adapters = []\n            # Ensure harvest files exist even for loaded runs (Memory Directive Fix)\n            try:\n                if not os.path.exists(\"discovery_harvest.json\"):\n                    with open(\"discovery_harvest.json\", \"w\") as f:\n                        json.dump(harvest_summary, f)\n            except Exception: pass\n        else:\n            # Auto-discover discovery adapter classes\n            adapter_classes = get_discovery_adapter_classes()\n\n            if adapter_names:\n                adapter_classes = [c for c in adapter_classes if c.__name__ in adapter_names or getattr(c, \"SOURCE_NAME\", \"\") in adapter_names]\n\n            # Load historical performance scores to prioritize adapters\n            db = FortunaDB()\n            adapter_scores = await db.get_adapter_scores(days=30)\n\n            # Prioritize adapters by score (descending)\n            adapter_classes = sorted(\n                adapter_classes,\n                key=lambda c: adapter_scores.get(getattr(c, \"SOURCE_NAME\", c.__name__), 0),\n                reverse=True\n            )\n\n            adapters = []\n            for cls in adapter_classes:\n                try:\n                    adapters.append(cls(config={\"region\": region}))\n                except Exception as e:\n                    logger.error(\"Failed to initialize adapter\", adapter=cls.__name__, error=str(e))\n\n            try:\n                async def fetch_one(a, date_str):\n                    try:\n                        races = await a.get_races(date_str)\n                        return a.source_name, races\n                    except Exception as e:\n                        logger.error(\"Error fetching from adapter\", adapter=a.source_name, date=date_str, error=str(e))\n                        return a.source_name, []\n\n                fetch_tasks = []\n                for d in target_dates:\n                    for a in adapters:\n                        fetch_tasks.append(fetch_one(a, d))\n\n                results = await asyncio.gather(*fetch_tasks)\n                for adapter_name, r_list in results:\n                    all_races_raw.extend(r_list)\n\n                    # Track count and MaxOdds (Proxy for successful odds fetching)\n                    m_odds = 0.0\n                    for r in r_list:\n                        for run in r.runners:\n                            if run.win_odds and run.win_odds > m_odds:\n                                m_odds = float(run.win_odds)\n\n                    if adapter_name not in harvest_summary:\n                        harvest_summary[adapter_name] = {\"count\": 0, \"max_odds\": 0.0}\n\n                    harvest_summary[adapter_name][\"count\"] += len(r_list)\n                    if m_odds > harvest_summary[adapter_name][\"max_odds\"]:\n                        harvest_summary[adapter_name][\"max_odds\"] = m_odds\n\n                    # Find the adapter instance to extract its trust_ratio\n                    matching_adapter = next((a for a in adapters if a.source_name == adapter_name), None)\n                    if matching_adapter:\n                        harvest_summary[adapter_name][\"trust_ratio\"] = max(\n                            harvest_summary[adapter_name].get(\"trust_ratio\", 0.0),\n                            getattr(matching_adapter, \"trust_ratio\", 0.0)\n                        )\n\n                logger.info(\"Fetched total races\", count=len(all_races_raw))\n            finally:\n                # Save discovery harvest summary for GHA reporting and DB persistence\n                try:\n                    # Only create if it doesn't exist or we have data\n                    if harvest_summary or not os.path.exists(\"discovery_harvest.json\"):\n                        with open(\"discovery_harvest.json\", \"w\") as f:\n                            json.dump(harvest_summary, f)\n\n                    if harvest_summary:\n                        await db.log_harvest(harvest_summary, region=region)\n                except Exception: pass\n\n                # Shutdown adapters\n                for a in adapters:\n                    try: await a.close()\n                    except Exception: pass\n\n        # Apply time window filter if requested to avoid overloading\n        # Initial time window filtering removed to ensure all unique races are tracked for reporting\n\n        if not all_races_raw:\n            logger.error(\"No races fetched from any adapter. Discovery aborted.\")\n            if save_path:\n                try:\n                    with open(save_path, \"w\") as f:\n                        json.dump([], f)\n                    logger.info(\"Saved empty race list to file\", path=save_path)\n                except Exception as e:\n                    logger.error(\"Failed to save empty race list\", error=str(e))\n            return\n        \n        # Deduplicate\n        race_map = {}\n        for race in all_races_raw:\n            canonical_venue = get_canonical_venue(race.venue)\n            # Use Canonical Venue + Race Number + Date + Discipline as stable key\n            st = race.start_time\n            if isinstance(st, str):\n                try:\n                    st = datetime.fromisoformat(st.replace('Z', '+00:00'))\n                except (ValueError, TypeError):\n                    pass\n\n            date_str = st.strftime('%Y%m%d') if hasattr(st, 'strftime') else \"Unknown\"\n            # Removing discipline from key to allow better merging across adapters\n            key = f\"{canonical_venue}|{race.race_number}|{date_str}\"\n            \n            if key not in race_map:\n                race_map[key] = race\n            else:\n                existing = race_map[key]\n                # Merge runners/odds\n                for nr in race.runners:\n                    # Match by number OR name (if numbers are missing)\n                    er = next((r for r in existing.runners if (r.number != 0 and r.number == nr.number) or (r.name.lower() == nr.name.lower())), None)\n                    if er:\n                        er.odds.update(nr.odds)\n                        if not er.win_odds and nr.win_odds:\n                            er.win_odds = nr.win_odds\n                        if not er.number and nr.number:\n                            er.number = nr.number\n                    else:\n                        existing.runners.append(nr)\n\n                # Update source\n                sources = set((existing.source or \"\").split(\", \"))\n                sources.add(race.source or \"Unknown\")\n                existing.source = \", \".join(sorted(list(filter(None, sources))))\n\n        unique_races = list(race_map.values())\n        logger.info(\"Unique races identified\", count=len(unique_races))\n\n        # GPT5 Improvement: Keep all races within window for analysis, not just one per track.\n        # Window broadened to 18 hours to match grid cutoff (News Mode)\n        timing_window_races = []\n        now = datetime.now(EASTERN)\n        for race in unique_races:\n            st = race.start_time\n            if isinstance(st, str):\n                try:\n                    st = datetime.fromisoformat(st.replace('Z', '+00:00'))\n                except (ValueError, TypeError):\n                    continue\n            if st.tzinfo is None:\n                st = st.replace(tzinfo=EASTERN)\n\n            # Calculate Minutes to Post\n            diff = st - now\n            mtp = diff.total_seconds() / 60\n\n            # Broaden window to 18 hours to ensure yield for \"News\"\n            if -45 < mtp <= 1080: # 18 hours = 1080 mins\n                timing_window_races.append(race)\n                if mtp <= 45:\n                    logger.info(f\"  \ud83d\udcb0 Found Gold Candidate: {race.venue} R{race.race_number} ({mtp:.1f} MTP)\")\n                else:\n                    logger.debug(f\"  \ud83d\udd2d Found Upcoming Candidate: {race.venue} R{race.race_number} ({mtp:.1f} MTP)\")\n\n        golden_zone_races = timing_window_races\n        if not golden_zone_races:\n            logger.warning(\"\ud83d\udd2d No races found in the broadened window (-45m to 18h).\")\n\n        logger.info(\"Total unique races available for analysis\", count=len(unique_races))\n\n        # Save raw fetched/merged races if requested (Save EVERYTHING unique)\n        if save_path:\n            try:\n                with open(save_path, \"w\") as f:\n                    json.dump([r.model_dump(mode='json') for r in unique_races], f, indent=4)\n                logger.info(\"Saved all unique races to file\", path=save_path)\n            except Exception as e:\n                logger.error(\"Failed to save races\", error=str(e))\n\n        if fetch_only:\n            logger.info(\"Fetch-only mode active. Skipping analysis and reporting.\")\n            return\n\n        # Analyze ALL unique races to ensure Grid is populated with Top 5 info (News Mode)\n        analyzer = SimplySuccessAnalyzer(config=config)\n        result = analyzer.qualify_races(unique_races)\n        qualified = result.get(\"races\", [])\n\n        # Generate Grid & Goldmine (Grid uses unique_races for the broader context)\n        grid = generate_summary_grid(qualified, all_races=unique_races)\n        logger.info(\"Summary Grid Generated\")\n\n        # Generate Field Matrix for all unique races\n        field_matrix = generate_field_matrix(unique_races)\n        logger.info(\"Field Matrix Generated\")\n\n        # Log Hot Tips & Fetch recent historical results for the report\n        tracker = HotTipsTracker()\n        await tracker.log_tips(qualified)\n\n        historical_goldmines = await tracker.db.get_recent_audited_goldmines(limit=15)\n        historical_report = generate_historical_goldmine_report(historical_goldmines)\n\n        gm_report = generate_goldmine_report(qualified, all_races=unique_races)\n        if historical_report:\n            gm_report += \"\\n\" + historical_report\n\n        # NEW: Dashboard and Live Tracking\n        goldmines = [r for r in qualified if get_field(r, 'metadata', {}).get('is_goldmine')]\n\n        # Calculate today's stats for dashboard\n        recent_tips = await tracker.db.get_recent_tips(limit=100)\n        today_str = datetime.now(EASTERN).strftime(\"%Y-%m-%d\")\n        today_tips = [t for t in recent_tips if t.get(\"report_date\", \"\").startswith(today_str)]\n\n        cashed = sum(1 for t in today_tips if t.get(\"verdict\") == \"CASHED\")\n        total_tips = len(today_tips)\n        profit = sum((t.get(\"net_profit\") or 0.0) for t in today_tips)\n\n        stats = {\n            \"tips\": total_tips,\n            \"cashed\": cashed,\n            \"profit\": profit\n        }\n\n        # Generate friendly HTML report\n        try:\n            html_content = await generate_friendly_html_report(qualified, stats)\n            html_path = Path(\"fortuna_report.html\")\n            html_path.write_text(html_content, encoding=\"utf-8\")\n            logger.info(\"Friendly HTML report generated\", path=str(html_path))\n\n            # Launch the report if running as a portable app (not in GHA)\n            if not os.getenv(\"GITHUB_ACTIONS\"):\n                try:\n                    # Use absolute path for reliable opening\n                    abs_path = html_path.absolute()\n                    if sys.platform == \"win32\":\n                        os.startfile(abs_path)\n                    else:\n                        webbrowser.open(f\"file://{abs_path}\")\n                except Exception as e:\n                    logger.warning(\"Failed to automatically launch report\", error=str(e))\n        except Exception as e:\n            logger.error(\"Failed to generate HTML report\", error=str(e))\n\n        if live_dashboard:\n            try:\n                from rich.live import Live\n                from rich.console import Console\n                # Check if our custom dashboard exists\n                try:\n                    from dashboard import FortunaDashboard\n                    dash = FortunaDashboard()\n                    dash.update(goldmines, stats)\n\n                    # Start odds tracker if requested\n                    if track_odds:\n                        try:\n                            from odds_tracker import LiveOddsTracker\n                            adapter_classes = get_discovery_adapter_classes()\n                            odds_tracker = LiveOddsTracker(goldmines, adapter_classes)\n                            asyncio.create_task(odds_tracker.start_tracking())\n                        except ImportError:\n                            logger.warning(\"LiveOddsTracker not available\")\n\n                    await dash.run_live()\n                except (ImportError, Exception) as e:\n                    logger.warning(f\"Rich dashboard component missing or failed: {e}\")\n                    # Fallback to simple rich display if possible\n                    console = Console()\n                    console.print(\"\\n\" + grid + \"\\n\")\n            except ImportError:\n                logger.warning(\"Rich library not available, falling back to static display\")\n                print(\"\\n\" + grid + \"\\n\")\n        else:\n            # Fallback to static print\n            try:\n                from dashboard import print_dashboard\n                print_dashboard(goldmines, stats)\n            except Exception as e:\n                # Silently fallback to standard print if dashboard fails\n                pass\n\n            print(\"\\n\" + grid + \"\\n\")\n            if historical_report:\n                print(\"\\n\" + historical_report + \"\\n\")\n\n        # Always save reports to files (GPT5 Improvement: Defensive guards)\n        try:\n            with open(\"summary_grid.txt\", \"w\", encoding='utf-8') as f: f.write(grid)\n            with open(\"field_matrix.txt\", \"w\", encoding='utf-8') as f: f.write(field_matrix)\n            with open(\"goldmine_report.txt\", \"w\", encoding='utf-8') as f: f.write(gm_report)\n        except Exception as e:\n            logger.error(\"failed_saving_text_reports\", error=str(e))\n\n        # Save qualified races to JSON\n        report_data = {\n            \"races\": [r.model_dump(mode='json') for r in qualified],\n            \"analysis_metadata\": result.get(\"criteria\", {}),\n            \"timestamp\": datetime.now(EASTERN).isoformat(),\n        }\n        try:\n            with open(\"qualified_races.json\", \"w\", encoding='utf-8') as f:\n                json.dump(report_data, f, indent=4)\n        except Exception as e:\n            logger.error(\"failed_saving_qualified_races\", error=str(e))\n\n        # NEW: Write GHA Job Summary\n        if 'GITHUB_STEP_SUMMARY' in os.environ:\n            try:\n                predictions_md = format_predictions_section(qualified)\n                # We need a db instance for format_proof_section\n                proof_md = await format_proof_section(tracker.db)\n                harvest_md = build_harvest_table(harvest_summary, \"\ud83d\udef0\ufe0f Discovery Harvest Performance\")\n                artifacts_md = format_artifact_links()\n                write_job_summary(predictions_md, harvest_md, proof_md, artifacts_md)\n                logger.info(\"GHA Job Summary written\")\n            except Exception as e:\n                logger.error(\"Failed to write GHA summary\", error=str(e))\n\n    finally:\n        await GlobalResourceManager.cleanup()\n",
      "name": "run_discovery"
    },
    {
      "type": "async_function",
      "content": "async def start_desktop_app():\n    \"\"\"Starts a FastAPI server and opens a webview window for the Fortuna Dashboard.\"\"\"\n    try:\n        import uvicorn\n        from fastapi import FastAPI\n        from fastapi.responses import HTMLResponse\n        import webview\n        import threading\n        import time\n    except ImportError as e:\n        print(f\"GUI dependencies missing: {e}. Install with 'pip install fastapi uvicorn pywebview'\")\n        return\n\n    app = FastAPI(title=\"Fortuna Desktop Intelligence\")\n\n    @app.get(\"/\", response_class=HTMLResponse)\n    async def get_dashboard():\n        # Retrieve latest Goldmines from the database\n        db = FortunaDB()\n        try:\n            async with db.get_connection() as conn:\n                try:\n                    async with conn.execute(\n                        \"SELECT venue, race_number, selection_number, predicted_2nd_fav_odds, start_time \"\n                        \"FROM tips ORDER BY id DESC LIMIT 50\"\n                    ) as cursor:\n                        tips = await cursor.fetchall()\n                except Exception as e:\n                    print(f\"DB query failed: {e}\")\n                    tips = []\n        except Exception as e:\n            print(f\"Failed to connect to database: {e}\")\n            tips = []\n\n        tips_html = \"\".join([\n            f\"<tr><td>{t[4]}</td><td>{t[0]}</td><td>R{t[1]}</td><td>#{t[2]}</td><td>{t[3]}</td></tr>\"\n            for t in tips\n        ])\n\n        return f\"\"\"\n        <html>\n            <head>\n                <title>Fortuna Intelligence Desktop</title>\n                <style>\n                    body {{ font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif; background: #0f172a; color: #f8fafc; padding: 30px; }}\n                    .container {{ max-width: 1200px; margin: auto; }}\n                    h1 {{ color: #fbbf24; border-bottom: 2px solid #fbbf24; padding-bottom: 10px; text-transform: uppercase; letter-spacing: 2px; }}\n                    table {{ width: 100%; border-collapse: collapse; margin-top: 20px; background: #1e293b; border-radius: 8px; overflow: hidden; }}\n                    th, td {{ padding: 15px; text-align: left; border-bottom: 1px solid #334155; }}\n                    th {{ background: #334155; color: #fbbf24; }}\n                    tr:hover {{ background: #475569; }}\n                    .footer {{ margin-top: 30px; font-size: 0.8em; color: #94a3b8; text-align: center; }}\n                    .btn {{ display: inline-block; background: #fbbf24; color: #0f172a; padding: 10px 20px; border-radius: 5px; text-decoration: none; font-weight: bold; margin-bottom: 20px; }}\n                </style>\n                <script>\n                    setTimeout(() => {{ location.reload(); }}, 30000);\n                </script>\n            </head>\n            <body>\n                <div class=\"container\">\n                    <h1>Fortuna Intelligence Dashboard</h1>\n                    <p>Monitoring global racing markets for Goldmine opportunities...</p>\n                    <a href=\"/\" class=\"btn\">REFRESH NOW</a>\n                    <table>\n                        <thead>\n                            <tr><th>Time Discovered</th><th>Venue</th><th>Race</th><th>Selection</th><th>Odds</th></tr>\n                        </thead>\n                        <tbody>\n                            {tips_html or \"<tr><td colspan='5'>No opportunities found yet. Run discovery to populate the database.</td></tr>\"}\n                        </tbody>\n                    </table>\n                    <div class=\"footer\">Fortuna Intelligence Monolith - Sci-Fi Future Edition - Auto-refreshing every 30s</div>\n                </div>\n            </body>\n        </html>\n        \"\"\"\n\n    def run_server():\n        uvicorn.run(app, host=\"127.0.0.1\", port=8013, log_level=\"error\")\n\n    # Start FastAPI in a background thread\n    server_thread = threading.Thread(target=run_server, daemon=True)\n    server_thread.start()\n\n    # Wait a moment for server to initialize\n    time.sleep(2.0)\n\n    # Create and start the webview window if server is up\n    if server_thread.is_alive():\n        print(\"Launching Fortuna Desktop Window...\")\n        webview.create_window('Fortuna Intelligence Desktop', 'http://127.0.0.1:8013', width=1300, height=900)\n        webview.start()\n    else:\n        print(\"\u26a0\ufe0f Error: GUI Server failed to start.\")\n",
      "name": "start_desktop_app"
    },
    {
      "type": "miscellaneous",
      "content": "\n"
    },
    {
      "type": "async_function",
      "content": "async def ensure_browsers():\n    \"\"\"Ensure browser dependencies are available for scraping.\"\"\"\n\n    # Skip Playwright in frozen apps if binary doesn't exist - use HTTP-only adapters\n    if is_frozen():\n        playwright_path = os.path.expanduser(\"~\\\\AppData\\\\Local\\\\ms-playwright\")\n        if not os.path.exists(playwright_path) and platform.system() == 'Windows':\n            structlog.get_logger().info(\"Running as frozen app - Playwright disabled (binary not found)\")\n            return True\n\n    try:\n        # Check if playwright is installed and has a chromium binary\n        from playwright.async_api import async_playwright\n        async with async_playwright() as p:\n            try:\n                # We try to launch a headless browser to verify installation\n                browser = await p.chromium.launch(headless=True)\n                await browser.close()\n                return True\n            except Exception as e:\n                structlog.get_logger().debug(\"Playwright launch failed during verification\", error=str(e))\n                if is_frozen():\n                    structlog.get_logger().info(\"Frozen app: Playwright launch failed, using HTTP-only fallbacks\")\n                    return True\n    except ImportError:\n        structlog.get_logger().debug(\"Playwright not imported\")\n        if is_frozen(): return True\n\n    if is_frozen():\n        return True\n\n    structlog.get_logger().info(\"Installing browser dependencies (Playwright Chromium)...\")\n    try:\n        # Run installation in a separate process to avoid blocking the loop too much\n        subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"playwright==1.49.1\"], check=True, capture_output=True, text=True)\n        subprocess.run([sys.executable, \"-m\", \"playwright\", \"install\", \"chromium\"], check=True, capture_output=True, text=True)\n        structlog.get_logger().info(\"Browser dependencies installed successfully.\")\n        return True\n    except subprocess.CalledProcessError as e:\n        structlog.get_logger().error(\n            \"Failed to install browsers\",\n            error=str(e),\n            returncode=e.returncode,\n            stdout=e.stdout,\n            stderr=e.stderr\n        )\n        return False\n    except Exception as e:\n        structlog.get_logger().error(\"Unexpected error installing browsers\", error=str(e))\n        return False\n",
      "name": "ensure_browsers"
    },
    {
      "type": "miscellaneous",
      "content": "\n"
    },
    {
      "type": "async_function",
      "content": "async def main_all_in_one():\n    # Configure logging at the start of main\n    structlog.configure(\n        wrapper_class=structlog.make_filtering_bound_logger(logging.INFO)\n    )\n    # Ensure DB path env is set if passed via argument or already in environment\n    # Actually, we should probably add a --db-path arg here too for parity with analytics\n    config = load_config()\n    logger = structlog.get_logger(\"main\")\n    parser = argparse.ArgumentParser(description=\"Fortuna All-In-One - Professional Racing Intelligence\")\n    parser.add_argument(\"--date\", type=str, help=\"Target date (YYYY-MM-DD)\")\n    parser.add_argument(\"--hours\", type=int, default=8, help=\"Discovery time window in hours (default: 8)\")\n    parser.add_argument(\"--monitor\", action=\"store_true\", help=\"Run in monitor mode\")\n    parser.add_argument(\"--once\", action=\"store_true\", help=\"Run monitor once\")\n    parser.add_argument(\"--region\", type=str, choices=[\"USA\", \"INT\", \"GLOBAL\"], help=\"Filter by region (USA, INT or GLOBAL)\")\n    parser.add_argument(\"--include\", type=str, help=\"Comma-separated adapter names to include\")\n    parser.add_argument(\"--save\", type=str, help=\"Save races to JSON file\")\n    parser.add_argument(\"--load\", type=str, help=\"Load races from JSON file(s), comma-separated\")\n    parser.add_argument(\"--fetch-only\", action=\"store_true\", help=\"Only fetch and save data, skip analysis and reporting\")\n    parser.add_argument(\"--db-path\", type=str, help=\"Path to tip history database\")\n    parser.add_argument(\"--clear-db\", action=\"store_true\", help=\"Clear all tips from the database and exit\")\n    parser.add_argument(\"--gui\", action=\"store_true\", help=\"Start the Fortuna Desktop GUI\")\n    parser.add_argument(\"--live-dashboard\", action=\"store_true\", help=\"Show live updating terminal dashboard\")\n    parser.add_argument(\"--track-odds\", action=\"store_true\", help=\"Monitor live odds and send notifications\")\n    parser.add_argument(\"--status\", action=\"store_true\", help=\"Show application status card and latest metrics\")\n    parser.add_argument(\"--show-log\", action=\"store_true\", help=\"Print recent fetch/audit highlights\")\n    parser.add_argument(\"--quick-help\", action=\"store_true\", help=\"Show friendly onboarding guide\")\n    parser.add_argument(\"--open-dashboard\", action=\"store_true\", help=\"Open the HTML intelligence report in browser\")\n    args = parser.parse_args()\n\n    if args.quick_help:\n        print_quick_help()\n        return\n\n    if args.status:\n        print_status_card(config)\n        return\n\n    if args.show_log:\n        await print_recent_logs()\n        return\n\n    if args.open_dashboard:\n        open_report_in_browser()\n        return\n\n    if args.db_path:\n        os.environ[\"FORTUNA_DB_PATH\"] = args.db_path\n\n    if args.quick_help:\n        print_quick_help()\n        return\n\n    if args.status:\n        print_status_card(config)\n        return\n\n    if args.show_log:\n        await print_recent_logs()\n        return\n\n    if args.open_dashboard:\n        open_report_in_browser()\n        return\n\n    # Print status card for all normal runs\n    print_status_card(config)\n\n    if args.gui:\n        # Start GUI. It runs its own event loop for the webview.\n        await ensure_browsers()\n        await start_desktop_app()\n        return\n\n    if args.clear_db:\n        db = FortunaDB()\n        await db.clear_all_tips()\n        await db.close()\n        print(\"Database cleared successfully.\")\n        return\n\n    adapter_filter = [n.strip() for n in args.include.split(\",\")] if args.include else None\n\n    # Use default region if not specified\n    if not args.region:\n        args.region = config.get(\"region\", {}).get(\"default\", DEFAULT_REGION)\n        structlog.get_logger().info(\"Using default region\", region=args.region)\n\n    # Region-based adapter filtering\n    if args.region:\n        if args.region == \"USA\":\n            target_set = USA_DISCOVERY_ADAPTERS\n        elif args.region == \"INT\":\n            target_set = INT_DISCOVERY_ADAPTERS\n        else:\n            target_set = GLOBAL_DISCOVERY_ADAPTERS\n\n        if adapter_filter:\n            adapter_filter = [n for n in adapter_filter if n in target_set]\n        else:\n            adapter_filter = list(target_set)\n\n        # Special case: TwinSpires needs to know its region internally if it's not filtered out\n        # We can pass the region via config if we were creating adapters manually,\n        # but here we use names.\n        # Actually, I updated TwinSpiresAdapter to check self.config.get(\"region\").\n        # I need to ensure the adapter gets this config.\n\n    loaded_races = None\n    if args.load:\n        loaded_races = []\n        for path in args.load.split(\",\"):\n            path = path.strip()\n            if not os.path.exists(path):\n                print(f\"Warning: File not found: {path}\")\n                logger.warning(\"Race data file not found\", path=path)\n                continue\n            try:\n                with open(path, \"r\") as f:\n                    data = json.load(f)\n                    loaded_races.extend([Race.model_validate(r) for r in data])\n            except Exception as e:\n                print(f\"Error loading {path}: {e}\")\n                logger.error(\"Failed to load race data\", path=path, error=str(e), exc_info=True)\n\n    if args.date:\n        target_dates = [args.date]\n    else:\n        now = datetime.now(EASTERN)\n        future = now + timedelta(hours=args.hours)\n\n        target_dates = [now.strftime(\"%Y-%m-%d\")]\n        if future.date() > now.date():\n            target_dates.append(future.strftime(\"%Y-%m-%d\"))\n\n    if args.monitor:\n        await ensure_browsers()\n        monitor = FavoriteToPlaceMonitor(target_dates=target_dates)\n        # Pass region config to monitor\n        monitor.config[\"region\"] = args.region\n        if args.once:\n            await monitor.run_once(loaded_races=loaded_races, adapter_names=adapter_filter)\n            if config.get(\"ui\", {}).get(\"auto_open_report\", True) and not os.getenv(\"GITHUB_ACTIONS\"):\n                open_report_in_browser()\n        else:\n            await monitor.run_continuous() # Continuous mode doesn't support load/filter yet for simplicity\n    else:\n        await ensure_browsers()\n        await run_discovery(\n            target_dates,\n            window_hours=args.hours,\n            loaded_races=loaded_races,\n            adapter_names=adapter_filter,\n            save_path=args.save,\n            fetch_only=args.fetch_only,\n            live_dashboard=args.live_dashboard,\n            track_odds=args.track_odds,\n            region=args.region, # Pass region to run_discovery\n            config=config\n        )\n        # Post-run UI enhancements (Council of Superbrains Directive)\n        if config.get(\"ui\", {}).get(\"auto_open_report\", True) and not os.getenv(\"GITHUB_ACTIONS\"):\n            open_report_in_browser()\n",
      "name": "main_all_in_one"
    },
    {
      "type": "miscellaneous",
      "content": "\n"
    },
    {
      "type": "unknown",
      "content": "if __name__ == \"__main__\":\n    if os.getenv(\"DEBUG_SNAPSHOTS\"):\n        os.makedirs(\"debug_snapshots\", exist_ok=True)\n    \n    # Windows Event Loop Policy Fix (Project Hardening)\n    if sys.platform == 'win32':\n        try:\n            # We prefer ProactorEventLoopPolicy for subprocess support (Playwright requirement)\n            # This is also set at the top of the file for frozen EXEs.\n            asyncio.set_event_loop_policy(asyncio.WindowsProactorEventLoopPolicy())\n        except AttributeError:\n            # Fallback if Proactor is not available (should be rare on modern Windows)\n            try:\n                asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())\n            except AttributeError:\n                pass\n\n    try:\n        asyncio.run(main_all_in_one())\n    except KeyboardInterrupt:\n        pass\n"
    }
  ]
}